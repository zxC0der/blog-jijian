[{"title":"Git基本操作备忘","permalink":"http://127.0.0.1:8080/detail/git-fundamental-memorandum","content":"\n## 基本概念\n\n### Git\n\nGit是一个分布式的版本控制系统。\n\n### 工作区，暂存区和版本库\n\n工作区(Workspace): 本地目录。\n暂存区(Index): 指`.git/index`文件，修改会先添加到暂存区，再提交到版本库。\n版本库(Repository): 可以认为整个`.git`就是版本库，Git所管理的文件数据都是保存在该文件夹里。\n\n## 基本操作\n\n### 初始化仓库\n\n`git init`\n\n### 添加到暂存区\n\n`git add a.txt`\n\n### 提交到版本库\n\n`git commit -m \"add a.txt\"`\n\n### 查看提交历史\n\n`git log`包括该版本之前的版本提交信息。`git log --pretty=oneline`可以以单行简化输出。\n\n`git reflog`包括所有分支的操作记录。\n\n### 查看仓库状态\n\n`git status`\n\n查看整个版本库的状态，显示有变更的文件。\n\n### 对比工作区和暂存区\n\n`git diff a.txt`\n\n`git diff`\n\n### 对比暂存区和版本库\n\n`git diff --cached a.txt`\n\n`git diff --cached`\n\n### 版本回退\n\n对于已经提交到版本库的文件信息，通过`git reset --hard HEAD^`来回退到上一版本，其中版本指针`HEAD^`表示上一版本，上上版本是`HEAD^^`，上上上n个版本是`HEAD~n`，也可以用commit id来表示指针，即`git log`中的16进制字符串。\n\n### 从暂存区回退\n\n`git checkout -- a.txt`\n\n该命令可以撤回工作区文件的修改(删除同理)，如果文件还没提交到暂存区，将回退到当前版本库的版本。如果文件已经提交到暂存区，将回退到上一次添加到暂存区后的版本。也就是让文件回到最近一次commit或者add的状态。\n\n如果修改已经添加到暂存区，可以通过`git reset HEAD a.txt`将修改从暂存区撤销，回到工作区，然后再根据上一段的命令从工作区撤销(也可以用该命令同时执行暂存区和工作区的撤销)。\n\n如果修改已经提交到版本库，就需要通过上一节的命令进行版本回退。\n\n### 删除文件\n\n`git rm a.txt`是同时删除版本库和工作区的文件，该命令执行后要`git commit`，而`git rm a.txt --cached`只删除版本库中的文件，让该文件不再收版本控制，但本地仍保留该文件。\n\n## 远程仓库\n\n### 关联远程仓库\n\n首先在远程的Git托管平台上创建一个远程Git仓库，`git remote add origin git@github.com:zxc/learngit.git`命令将远程仓库和本地仓库相关联，这里使用的是默认的`git://`(ssh协议)，其中origin是远程仓库名字，通常都命名为origin，这里托管平台是github，用户名是zxc，仓库名是learngit。\n\n### 推送到远程仓库\n\n`git push -u origin master`将本地仓库推送到远程仓库origin，其中master表示将本地的master分支推送到远程的master分支，如果本地和远程分支名不同，则需要指定{本地分支名}:{远程分支名}，而-u是设置当前本地分支的上游分支，这样下次push的时候，在当前分支下，可以不指定远程分支。\n\n### 克隆远程仓库\n\n`git clone git@github.com:zxc/learngit.git`\n\nTODO pull呢？\n\n## 分支管理\n\nGit里的分支都是直接通过修改指针的指向来管理，所以速度更快。Git的HEAD是指当前分支，默认是master，实际上所有提交版本构成了一个(多个)版本链，而master分支是一个指针，指向分支的最新版本，而HEAD也是一个指针，指向了当前分支。\n\n### 列出分支\n\n`git branch`\n\n其中带*号的是当前分支。\n\n### 创建并切换分支\n\n`git checkout -b dev`\n\n在新版本Git中，等同于`git switch -c dev`。\n\n该命令表示创建并切换到dev分支，等同于先`git branch dev`创建分支，再`git checkout dev`(在新版本Git中，等同于`git switch dev`)切换分支。\n\n\n### 合并分支\n\n`git merge dev`\n\n该命令将指定分支dev合并到当前分支，如果没有其他冲突，直接将master指针移动到dev指针处，速度很快。\n\n### 删除分支\n\n`git branch -d dev`\n\n### 手动解决冲突\n\n合并时如果文件发生冲突需要手动解决，此时文件会显示如下的内容，表示每个分支的不同修改，需要手动合并之后再提交。\n\n```txt\n<<<<<<< HEAD\nCreating a new branch is quick & simple.\n=======\nCreating a new branch is quick AND simple.\n>>>>>>> feature1\n```\n\n### 查看分支合并图\n\n`git log --graph --pretty=oneline --abbrev-commit`\n\n### 分支合并策略\n\n默认的分支合并策略是Fast forward，比如`git merge dev`是直接将master指针指向dev，来达到合并的效果，这时候如果删除dev分支，就会直接丢失这个分支的所有提交动作，可以通过添加`--no-ff`参数，即`git merge --no-ff -m \"merge\" dev`，将合并作为一个新的提交，将两个提交合并起来，这样即使删除了dev分支，原先dev分支的提交动作已经合并在master分支上而不会丢失了。\n\n### 临时保存进度\n\n当一个分支的工作还没完成提交，这时候又有一个紧急bug需要处理，因此需要新建一个新的分支来操作，但是为了不影响这个分支的commit日志，这时候的修改还不能提交，如果直接切到bug分支进行修改，那么这个分支的修改就会丢失(即使添加到了暂存区，也是多个分支共享的)。\n\n这时候可以使用`git stash`直接将整个工作区入栈保存起来。\n\n然后切回master分支，再新建一个bug分支进行处理，之后回到dev分支。\n\n`git stash list`可以查看所有暂存的工作区。\n\n`git stash pop`恢复栈顶工作区并弹出。等同于`git stash apply stash@{0}`之后`git stash drop stash@{0}`，后面参数是list查看到的序号。\n\n这时候发现其实修复的bug在dev分支也存在，因为dev分支是在修复bug之前新建的，所以这时候一种方法就是在dev分支重新执行一次bug修复，另一种方法就是使用`git cherry-pick xxxx`命令，其中xxxx是commit id，该命令可以将提交在该分支上重新执行一遍，当然如果存在冲突也需要手动处理。\n\n### 多人协作\n\n多人协作的工作模式通常是这样：\n\n- `git push origin {branch}`尝试推送自己的修改。\n- 如果推送失败，因为远程分支领先本地分支(别人先推送上去了)，需要先用`git pull origin {branch}`拉下来并尝试合并(设定上游之后可以简写为`git push`和`git pull`)。\n- 如果合并有冲突，需要手动解决冲突，并提交。\n- 如果没有冲突或者冲突已解决，再用`git push origin {branch}`推送。\n- 如果提示`no tracking information`，说明本地分支和远程分支还没有链接关系，通过命令`git branch --set-upstream-to {branch} origin/{origi_branch}`设置上游分支即可。\n\n### Rebase\n\n`git rebase`\n\nrebase操作可以把**本地未推送**的分叉提交历史整理成直线。\n\n## 其他\n\n### 标签\n\n标签用来标记某一个版本，比起commit id更好记忆，本质上也是一个指向某个commit的不可变的指针。\n\n`git tag v1.0 {commit id}`指给指定的commit打上v1.0的标签，如果忽略commit id，表示给最近的一个commit打上标签。\n\n`git tag -a v1.0 -m \"comment\"`可以指定标签名和说明文字。\n\n`git tag`查看所有标签。\n\n`git show {tag name}`查看标签详细信息。\n\n`git tag -d {tag name}`删除标签。\n\n`git push origin {tag name} || --tags`可以将某个标签或者所有标签推送到远程origin分支上，默认不推送。\n\n`git push origin :refs/tags/{tag name}`在删除本地标签之后删除远程标签。","date":"Feb 21, 2021","category":{"name":"基础知识","permalink":"http://127.0.0.1:8080/category/基础知识"},"tags":[{"name":"git","permalink":"http://127.0.0.1:8080/tags/git"},{"name":"开发","permalink":"http://127.0.0.1:8080/tags/开发"}]},{"title":"JWT基础原理","permalink":"http://127.0.0.1:8080/detail/jwt-session-summary","content":"\n> 整理自[https://www.v2ex.com/t/656457](https://www.v2ex.com/t/656457)和[http://www.ruanyifeng.com/blog/2018/07/json_web_token-tutorial.html](http://www.ruanyifeng.com/blog/2018/07/json_web_token-tutorial.html)\n\n## 狭义的Session和Cookie\n\n普通的用户认证流程通常如下：\n\n- 客户端向服务端发送用户名和密码。\n- 服务端验证通过后，在当前对话(session)里面保存相关数据(session data)，比如用户角色、登录时间等。\n- 服务端向用户返回一个session_id，写入用户本地的Cookie。\n- 客户端随后的每一次请求，都会自动将Cookie加到请求头中，其中的session_id被传回到服务端。\n- 服务端收到session_id后，检索session data找到用户相关信息。\n\n这种模式的扩展性不好，对于服务器集群，或者是跨域的服务架构等要求session数据共享的情况，一种解决方案是session数据持久化，写入数据库，这种方案的优点是架构清晰，缺点是工程量比较大，而且要性能有一定的损耗。另一种方案是服务器不保存session数据，采用Client Side Session的模式，所有数据都保存在客户端，每次请求再发回服务器。JWT就是这种方案的一个代表。\n\n## JWT\n\n### JWT原理\n\nJWT全称是Json Web Token，它的原理是，在服务器认证以后，生成一个JSON对象，返回给用户，例如\n\n```json\n{\n  \"username\": \"zhangsan\",\n  \"role\": \"admin\",\n  \"expiredTime\": \"20180701000000\"\n}\n```\n\n之后，用户与服务端通信的时候，都要带上这个JSON对象。而服务器就不保存任何session数据了，也就是说，服务器变成无状态了，从而比较容易实现扩展。\n\n服务器只靠这个对象来认定用户身份。因此为了防止用户篡改数据，服务器在生成这个对象的时候，需要加上签名。\n\n### JWT 的数据结构\n\nJWT是一个很长的字符串，中间用点分隔成三个部分，分别是Header，Payload和Signature。\n\n其中Header是一个 JSON 对象，描述JWT的元数据，例如\n\n```json\n{\n  \"alg\": \"HS256\",\n  \"typ\": \"JWT\"\n}\n```\n\n其中alg属性表示签名算法，默认是HMAC SHA256。typ属性表示令牌类型，JWT令牌统一写为JWT。\n\nPayload也是一个 SON 对象，用来存放实际传递的数据。JWT规定了7个官方字段。\n\n- iss (issuer)：签发人\n- exp (expiration time)：过期时间\n- sub (subject)：主题\n- aud (audience)：受众\n- nbf (Not Before)：生效时间\n- iat (Issued At)：签发时间\n- jti (JWT ID)：编号\n\n除了官方字段，也可以定义私有字段\n\n**注意，JWT默认是不加密的(只是签名)，任何人都可以读到，不要把私密信息放在这个部分**。\n\n这两部分都通过Base64URL算法转成字符串。\n\nSignature部分是通过服务端的私钥，根据Header中指定的签名算法，对前两部分进行签名，防止数据篡改。\n\n然后把Header，Payload，Signature三个部分拼成一个字符串，每个部分之间用\"点\"（.）分隔，返回给用户。\n\n### JWT的使用方式\n\n客户端收到服务器返回的Token，可以储存在Cookie里，也可以储存在localStorage里。\n\n此后，客户端每次与服务器通信，都要带上这个Token。为了方便跨域，通常更好的做法是放在HTTP请求头的Authorization字段里面，或者放在POST请求的数据体里面。\n\n### JWT的特点\n\n- JWT默认不加密，但也是可以加密的。在生成原始 Token 以后，可以用密钥再加密一次。\n- JWT的最大缺点是，由于服务器不保存session状态，因此无法在使用过程中使某个token失效，除非服务器部署额外的逻辑。\n\n## 广义的Session和Cookie\n\n广义的Session指的就是从用户登录到登出的过程中，维持的这样一种状态和相关信息。\n\n### Sever Side Session\n\nSever Side Session是最常见的 Session实现，但它并不是唯一的一种Session实现。\n\n优点:\n\n- 数据存储在服务端，相对安全性更强\n- 请求时只需要传递Session-ID，减小流量开销\n- 可以方便的管理和吊销Session\n\n缺点:\n\n- Session-Data集中管理，不利于分布式架构，需要专门解决Session共享问题\n- Session-Data需要占用服务端内存/存储，对服务端存在压力\n\n### Client Side Session\n\nClient Side Session顾名思义是在客户端存储Session。\n\n优点:\n\n- 将存储压力转移到了客户端，可以减小服务端的资源消耗\n- 分布式架构下，不需要考虑Session共享问题\n\n劣势:\n\n- Cookie默认有4KB限制，不能存储太多内容\n- 存在重放攻击的风险，客户端可能会将数据替换为合法的旧数据\n- 实现Session数据的拉黑、强制失效等功能时比较复杂\n- 部分实现没有对数据进行加密，客户端可以直接查看到数据内容，存在安全风险\n\n## 相关问题\n\n### Token是什么？\n\nToken实际上泛指用于认证鉴权的凭据，广义上来说，如果使用Session-ID或Session-Data进行用户认证，也可以称之为Token。\n\n### Session和JWT的关系是什么？\n\nJWT是特定的一种Token生成方式，它有特定的格式，通过签名保证了信息的不可篡改，可以作为Client Side Session的数据处理方式。\n","date":"Mar 6, 2021","category":{"name":"基础知识","permalink":"http://127.0.0.1:8080/category/基础知识"},"tags":[{"name":"默认","permalink":"http://127.0.0.1:8080/tags/默认"}]},{"title":"数据库事务故障恢复","permalink":"http://127.0.0.1:8080/detail/database-transaction-recovery","content":"\n## 事务管理\n\n### 事务的定义\n\n事务是一系列的数据库操作，作为一个不可分割的执行**单元(unit)**，要么全做要么全不做。\n\n在这一部分单纯讨论事务的并发时，我们通常定义一个简单的事务模型，假设事务的写操作是直接写到磁盘(实际上会有页面的缓冲池)的原子操作(因此另一个并发事务可以读取到最新数据)。而事务并非总能成功执行，但事务出错终止(aborted)时，为了保证原子性，事务终止前对数据库做的操作都需要撤销，需要对事务进行回滚(rolled back)，数据库系统通常是基于日志(log)进行故障恢复。\n\n### 事务的四个特性(ACID)：\n\n- 原子性(Atomicity): 事务是最小的执行单位，要么全做，要么全不做。\n- 一致性(Consistency): 执行事务前后，数据都是处于一致性状态，即该状态满足预定的约束(比如转账模型中，事务执行**前后**，两个账户余额总数应该是不变的)。\n- 隔离性(Isolation): 多事务并发执行时，各个并发事务之间都是独立的，感觉不到其他事务的存在。\n- 持久性(Durability): 一个事务成功完成后，对数据库的改变是持久的，即使发生系统故障。\n\n\n## 故障恢复\n\n### 故障分类\n\n- 事务故障: 事务逻辑错误或者并发事务出现死锁。\n- 系统故障: 硬件或者软件的错误或漏洞导致数据库系统发生故障。导致内存数据丢失(已提交事务可能数据还在内存没刷回磁盘，需要重做，未提交事务)。\n- 磁盘故障: 非执行时错误，而是写入磁盘时因为磁盘故障导致磁盘数据丢失。\n\n### 日志记录\n\n日志是包括一系列的日志记录(log record)，基本的日志记录类型包括：\n\n- 更新日志记录，<T,X,V1,V2>，其中T表示事务标识，X是数据项标识，V1是旧值，V2是新值。\n- 事务开始记录，<T,start>\n- 事务提交记录，<T,commit>\n- 事务终止记录，<T,abort>\n\n每次事务执行写操作，需要先写日志，再执行操作。\n\n### Redo和Undo\n\n发生系统故障时，可以利用日志对事务进行重做(Redo)或者撤销(Undo)。\n\n- 如果一个事务包括start，但不包括commit和abort，说明这个事务在系统故障前开始，但是没有完成或回滚，因此此时数据更新有可能在内存，也可能已经刷盘，为了保证数据的一致性，必须进行Undo操作。\n- 如果一个事务包括start，以及包括commit或者abort，说明这个事务的所有操作都已经完成，都记录在日志里，但是也不知道修改是否已经刷盘，所以必须根据日志记录，进行Redo操作。(已经commit或者abort的事务必须Redo而不是Undo，因为修改有可能已经写入磁盘？)\n\n- Undo操作除了把数据恢复成旧值，还会把恢复操作作为一个更新操作写入为**一种只读的特殊日志**，不需要记录恢复操作的旧值和新值。最后写入一个abort日志，表示事务撤销完成。\n- Redo操作按顺序将数据设置成新值。\n\n### 检查点(checkpoint)\n\n系统会定时将缓存中的脏页刷回磁盘，并用另外的日志记录下当前记录点的时刻，系统故障恢复时不需要考虑最后一个记录点之前成功提交的事务。\n\n### 事务回滚\n\n正常的事务回滚，从后往前扫描日志，对于该事务的每一个日志记录<T,X,V1,V2>，将数据项X写入旧值V1，然后往日志末尾添加一个特殊的只读日志记录<T,X,V2>，注意这种特殊记录不需要旧值，不会被Undo。\n\n当遇到该事务的<T,start>记录，停止扫描，并往日志末尾写入<T,abort>表示回滚完成。\n\n### 系统故障的恢复\n\n利用日志记录进行系统故障的恢复(Recovery after a system crash)，书上所描述的完整算法如下：\n\n- 重做(redo)阶段：把该redo的事务进行redo，并构造该undo的undo-list(包括系统奔溃前未完成的事务，即没有提交也没有回滚)。\n\n    - 初始化undo-list为最后一个检查点(checkpoint)的活跃事务(+检查点之前开始的事务)。\n    - 从最后一个检查点开始正向扫描日志记录，遇到一个普通日志记录<T,X,V1,V2>或者read-only特殊日志记录<T,X,V2>(表示重做回滚(比如自定义的某些条件下的事务回滚而不是意外的事务回滚))，执行redo操作，将数据项X写入V2值。\n    - 遇到<T,start>，将该事务加入undo-list(+检查点之后开始的事务)。\n    - 遇到<T,commit>或者T<T,abort>，将该事务从undo-list中删除(-故障发生点之前提交或回滚的事务(第2步的redo操作中就对这些事务进行了重做))。\n\n- 撤销(undo)阶段：\n\n    - 反向扫描日志，遇到属于undo-list的事务日志，执行undo操作，将数据项写回旧值。\n    - 遇到属于undo-list的事务的<T,start>，把该事务从undo-list删除，写入一个<T,abort>日志记录(该事务已撤销完成)。\n    - 当undo-list为空，则所有事务撤销完成。\n\n为了方便处理，undo-list的事务在重做阶段也会被重做，然后在撤销阶段再被撤销。\n\n## MySQL中的事务恢复\n\n## PostgreSQL中的事务恢复","date":"Feb 22, 2021","category":{"name":"基础知识","permalink":"http://127.0.0.1:8080/category/基础知识"},"tags":[{"name":"数据库","permalink":"http://127.0.0.1:8080/tags/数据库"},{"name":"事务","permalink":"http://127.0.0.1:8080/tags/事务"}]},{"title":"CMU15-445(19-fall)-Project1-Buffer-Pool-Manager","permalink":"http://127.0.0.1:8080/detail/cmu15445-19fall-project1","content":"\n> 由于课程禁止公开项目代码，所以如果有同样在做这个项目的同学对我的实现感兴趣，或者发现其中存在什么问题欢迎联系我，邮箱：1129142694@qq.com\n\n## 理论基础\n\n### Introduction of DBMS \n\n广义的数据库应该是指存储数据的地方，比如一个文件其实也是一个数据库，而数据库管理系统(DBMS)才是我们对数据库进行操作，管理的一个软件，也是我们这个project要实现的。\n\n一个最基本的DBMS会由查询解析/编译器，执行引擎，索引管理器，缓冲区管理器，磁盘管理器，物理存储等几部分组成，另外还应该包括事务管理，日志恢复，并发控制等功能。在这个project中，我们主要实现的是后端部分，也就是关注以下几个部分。\n\n- Query Planning\n- Operator Execution\n- Access Methods\n- Buffer Pool Manager\n- Disk Manager\n\n### Database Storage Part #1\n\n#### File Storage\n\n为了实现对内存更好的控制，项目没有采用操作系统的缓存/虚拟内存技术，而是需要我们手动实现一个页面缓冲池管理器，...\n\nDBMS通过包含页(Page，一块数据)的一个或多个文件来组织数据库(**通俗来讲，就是最少按一个页来操作数据的，比如一次从文件里读取一个页(16KB)的数据，这些数据里可能存储了多个元祖，或者存储了索引等**)，实际的硬件存储也有页的概念，一个磁盘页通常是4KB，所以一个4KB的页面的读写是自带有原子性的，但是一个数据库的页通常不止4KB，所以读取时需要采取额外的措施来保证并发安全。\n\nDBMS通常是通过heap file的形式来组织文件中的这些Page(**通俗来讲，就是这些page(本质就是一块数据)是以什么顺序存放在一个文件里的，所以需要维护一些元数据来跟踪每个Page的位置，以及当前的空闲位置**)，通常有两种实现方式，链表或者页面文件(Page Directory)的形式，简单来说一个是链表存储，每次要找一个Page都需要从Header page开始找，一个是类似索引，Directory page里维护了每个Page在文件中的位置以及空闲的slot(一个Page有多个slot可以放数据)，所以DBMS必须保证Directory page的同步。\n\n#### Page Layout\n\n每个Page有一个Header来维护一些元数据，比如存储的元组(tuple)个数(假设存储的都是元组)，校验和，版本号，事务相关，压缩信息等等。具体每个Page存储元组的方式有slotted-page和log-structured两种。slotted page就是Page的Header部分维护一个slot表，指向当前存储的元组，而实际元组存储是从后往前，从Page的最后一个内存位置开始存储。类似于栈和堆的相向增长，直到两者相遇。另外一种方式log-structured则是指Page不存储具体的元组，而是存储数据库更新的记录，包括Insert，Update和Delete等，通过逆序扫描Page的日志记录，就可以查询所需的元组，为了提高扫描效率，可以添加日志的索引方便在记录中跳跃查找，以及对日志进行压缩。\n\n#### Tuple Layout\n\n最后一部分是元组的存储结构，同样有Header和其他Attribute Data，每个元组会有一个独一无二的identifier，通常是Page id+offset/slot。\n\n### Database Storage Part #2\n\n#### Data Representation\n\n文件中存储的一个元组其实就是一些字节序列，所以DBMS需要能够将这些字节序列解析表示数据库里实际的类型(Type)和值(Value)。\n\n#### System Catalogs\n\n系统目录(System Catalogs)是DBMS内部存储的关于数据库的元数据，包括表，列，索引，视图，用户权限，内部统计等信息，其实也就是一个保存元数据的表。\n\n#### Storage Models\n\nOLTP(On-Line Transaction Processing)：联机事务处理，关系型数据库基本日常的简单查询，事务处理。\n\nOLAP(On-Line Analytical Processing)：联机分析处理，应用在大型数据仓库，支持复杂的分析查询操作。\n\nDMBS可以以不同的方式来组织元组的存储，使得可以满足不同数据量，不同场景的应用(OLTP or OLAP)。\n\n在这个project中，我们将使用n-ary storage model(n-ary存储模型)，也叫row storage(行存储)，也就是将连续元组的所有属性保存在一个Page里，这种存储模型适用于OLTP。另外一种存储模型是decomposition storage model，也叫column storage(列存储)，也就是一个Page保存的是连续元组的同一个属性，这种存储模型适用于OLAP。\n\n### Buffer Pool\n\n#### Buffer Pool Manager\n\n数据库的存储第一个问题是\"How the DBMS represents the database in files on disk\"，即DBMS如何用磁盘里的文件来表示数据库，第二个问题则是\"How the DBMS manages its memory and move data back-and-forth from disk\"，即DBMS如何管理内存，在磁盘和内存之间来回移动数据。\n\n和磁盘文件分成多个Page(不是hardware page，而是DBMS组织的抽象Page)一样，DMBS将一块内存空间分为多个大小和Page相同的Frame，当DBMS请求一个页面，就从磁盘里读取并放到对应的空Frame上，然后用一个表(Page Table)来维护每个Page在缓冲区(Frame Table)的位置，同时也要维护两个标记：dirty flag和pin counter。其中dirty flag就表示这个页面的数据有没有被写过，是否需要刷会磁盘，而pin counter顾名思义，表示有多少个操作/线程pin(钉)住了这个页面，在对这个页面进行操作，所以这时候不能将这个页面淘汰出内存。\n\n我们可以对缓冲池进行一些优化，包括以下几种操作。\n\n- Multiple Buffer Pool，多个缓冲池，可以通过对page id进行哈希来判断使用哪一个缓冲池，或者可以每个表使用一个缓冲池。好处就是可以对每一个缓冲池采用单独的页面替换策略。\n- Pre-Fetching，预先将可能需要用到的磁盘page读取到缓冲池中，而不是等到用到才去一页一页读取。这些page在磁盘中可能不连续，但可以通过索引进行跳转。\n- Scan Sharing，多个查询可以共享一些从磁盘查到的数据，查询并不需要完全相同，可能是共享一些中间结果。也就是一个线程查询的cursor可以先attach在另一个已经开始的查询线程中，先查两者剩下的查询中共有的，然后再回过头去查剩下的，避免缓冲池页面的频繁进出替换。\n- Buffer Pool ByPass，为了查询一些临时的数据，不想污染缓冲池，可以直接将磁盘读取到的page放到一块另外的内存里，当查询完成后就释放掉这块内存。\n\n#### Replacement Policies\n\n缓冲池的另外的一个重要部分就是页面替换策略，即当缓冲池空间不足时将某个页面淘汰的策略。最常见的替换策略是LRU(Least Recently Used)，即最近最少使用，通过一个链表和一个维护位置的哈希表可以实现。Clock，时钟算法是LRU算法的近似算法，简单来说就是通过模拟时钟的指针不断循环扫描整个page table，扫到的slot对应的页面就应该被淘汰，但如果这个页面当前正在被使用(pin)，可以有一次拯救的机会，将pin标记置为0，下一次再扫到这个slot，如果标记为0，就直接淘汰。\n\n这两种策略是最基本的替换策略，实际上还有很多优化的版本。\n\n## 具体实现\n\nProject \\#1目标是实现一个Buffer Pool Manager(缓冲池管理器)用于管理内存中的缓冲池，以及缓冲区必须要有的页面置换算法，包括LRU和Clock算法。\n\n缓冲池就是指内存里一块特定区域，用于缓存一些从磁盘读取的页面，对应到代码中，就是一个Page数组，每个Page元素就保存有数据和一些元数据。Page数组的下标其实就是frame id，所以这个数组其实可以看成frame id对page的映射，也就是哪一个frame放了哪个page，而一般情况下我们是需要通过page id来查找page的位置，所以显然需要另外一个映射关系表示从page id到frame id的映射，理论上也是一个数值数组就行，不过为了逻辑上更加合理，可以采用哈希表，也就是上面提到的page table。\n\n然后管理器里还可以再维护一个free list，保存空闲的frame id，初始化整个缓冲池是空的，所以free list就是满的。\n\nReplacer是管理器的另外一个重要组件，也就是替换策略的实现，其实就是维护一些已被使用的page所对应的frame id，当缓冲池需要放一个新页面，但是free list已经为空，也就是缓冲池已满，那就需要通过调用Replacer的Victim方法来淘汰掉某一个frame id所对应的page。\n\n并发安全的实现采用std的互斥量mutex类来实现，原始的方法是采用mutex 的lock/trylock方法来加锁，unlock方法来解锁。更常见的是用lock_guard这个模板类，实现在作用域结束后自动调用析构函数，执行unlock方法。\n\n### ClockReplacer\n\n时钟的大小和缓冲池大小一样，对应每个位置(也就是每一个frame id)有两个标记，一个标记记录这个frame是否真正在Replacer中，这是为了方便处理，用静态大小的数组可以无需处理frame动态的进出，另一个标记是Clock算法中的记录最近是否被使用，更准确地说应该是最近是否被使用结束后被加到Replacer中，如果是，则标记为1。\n\nReplacer包括3个主要函数，Pin函数表示这个页面被某个线程引用了，要对这个页面进行读写操作，所以显然不能放在Replacer里，如果在里面就要拿出来，反过来如果一个页面不再被引用了，即pin count减到0，就应该调用Unpin函数，将这个frame加入Replacer，随时可以被淘汰。\n\nVictim函数就是选择淘汰某个页面，也是置换算法的核心，维护一个指针位置clockHand，然后循环扫过每一个\"在\"Replacer里的frame，如果frame的最近访问标志为true，置为false，即给了一次\"自我拯救\"的机会，如果是false，就需要将这个frame对应的page淘汰掉，返回该frame id即可。\n\n### LRUReplacer\n\n这个比较简单，用一个链表放frame id，同时用一个哈希表维护frame id在链表中的位置(迭代器)，同样包括三个主要函数，Pin就是将frame从链表中移除，Unpin就是将frame加入链表尾部，如果已经在链表中就直接过，Victim就是将链表头部的frame淘汰。\n\n这里跟想象中的LRU似乎不太一样，第一次实现Pin函数是把frame从链表中移到尾部，从单元测试来看，这里直接删除似乎更为合理。\n\n### Buffer Pool Manager\n\n缓冲池管理器作用就是管理缓冲池，包括页面的读写存取，页面的替换等。包括几个主要函数：\n\n- FlushPage，给定page id，如果该页面在缓冲池中且脏标记为true，将其刷回磁盘，记得要将脏标记置为false。\n- UnpinPage，给定page id和一个脏标记，表示对这个页面的操作结束以及是否有修改，找到对应页面，将pin count减1，如果pin count减为0，就要调用Replacer的Unpin方法，将这个frame加入Replacer中。\n- DeletePage，从缓冲池中删掉一个页面，其实也就是将内存清空，将一些对应关系清空。\n- NewPage，在缓冲池中新建一个空页面，从free list或者Replacer里取出/淘汰一个frame，然后通过磁盘管理器的AllocatePage函数创建一个新页面，返回对应的page id，添加上frame id和page id的对应关系。这个操作也就是执行写操作，即把数据先写到内存里，等再刷回磁盘，所以脏标记是true。\n- FetchPage，通过page id读取对应页面，如果缓冲池里有直接读取，如果没有，同样是需要从free list里或者Replacer里淘汰一个frame，然后从磁盘里读取对应页面到缓冲池里。\n","date":"Nov 16, 2020","category":{"name":"技术学习","permalink":"http://127.0.0.1:8080/category/技术学习"},"tags":[{"name":"CMU15-445","permalink":"http://127.0.0.1:8080/tags/CMU15-445"},{"name":"数据库","permalink":"http://127.0.0.1:8080/tags/数据库"}]},{"title":"CMU15-445(19-fall)-Project2-Hash-Index","permalink":"http://127.0.0.1:8080/detail/cmu15445-19fall-project2","content":"\n> 由于课程禁止公开项目代码，所以如果有同样在做这个项目的同学对我的实现感兴趣，或者发现其中存在什么问题欢迎联系我，邮箱：1129142694@qq.com\n\n## 理论基础\n\nDBMS内部会使用多种数据结构，具体用途包括表示内部的元数据，存储核心数据，执行查询时的临时数据结构(比如join时用了哈希表)，数据库索引。\n\n对于一个哈希表我们通常会关注两个部分，一个是Hash Function(哈希函数)，另一个是Hashing Scheme(哈希冲突解决方案)。\n\n对于不同的哈希函数，通常我们会关注它的速度和冲突率。\n\n对于哈希冲突时的解决方案，该课程提供了3种静态和3种动态方案：\n\n静态方案：\n\n- Linear Probe Hashing：线性探测，最简单的方案之一，也是我们Project中要实现的，即当发生哈希冲突时，直接尝试下一个slot，直到可以放下为止。\n- Robin Hood Hashing：线性探测的改进，对于普通的线性探测，如果如果冲突的key很多，可能会导致后面的key查询花费非常大，robing hood方案就是对每个slot的key维护一个数字，表示离实际哈希位置的距离，然后线性探测的时候，如果探测到某个slot的key的这个距离小于要插入的这个key当前离哈希位置的距离，为了\"劫富济贫\"，就应该把这个位置让给这个新插入的key，而原来的key就要另寻位置。\n- Cuckoo Hashing：布谷鸟方案，维护多个哈希函数，对应多个哈希表，以2个为例，对于一个key的插入，先计算每个哈希函数的值，然后在多个哈希表中找到有空闲slot的一个，放下，如果多个哈希表都冲突了，选择其中一个，替换原有的key，原有的key重新使用该方案查找可以插入的slot，直到所有key都可以放下。\n\n动态方案：\n\n- Chained Hashing：链地址法，用一个链表来保存冲突的key。\n- Extendible Hashing：可拓展哈希，基于二进制位，重点就是每个block放的key都是有相同长度的二进制前缀，所以需要维护每个block的深度(也就是这个前缀长度)，以及全局的深度(最大的block深度)，而哈希表slot的数量就刚好等于2的深度次幂，当block满时就分裂并更新深度和哈希表slot。\n- Linear Hashing：线性哈希，重点在于不是当block溢出是分裂，而是维护一个split pointer，指向下一个需要分裂的位置，当插入时**任意一个**block溢出，先用链地址法解决(仍然属于溢出)，然后添加一个哈希函数，然后将split pointer指向的block重新哈希并分裂，然后移动split pointer到下一位，这样查找时如果发现哈希后的位置在split pointer之上，说明需要进行第二次哈希。当split pointer移动到最后一个slot之后，直接删除第一个哈希，将split pointer移动到顶端，重新开始。\n\n(后面这几种哈希方案第一次见，有一说一，还是很秀的，最后一个有点看不太懂，或者说有点难描述。\n\n## 具体实现\n\nProject \\#2目标是实现一个哈希表作为数据库中的基础数据结构，使用线性探测来解决哈希冲突。\n\n这里实现的哈希表和之前理解的哈希表没有本质的区别，有一点不同就是后者因为可以完全加载到内存中，所以其实就是一个k-v对的数组，而前者的不行，必须将哈希表必须分成多个部分(block)，一个page就放一个block，按需加载进内存。\n\n### Header Page\n\n维护哈希表的一些元数据，包括bucket(存放k-v对的桶，一个block里有多个bucket)数量，blcok数量以及所有block对应的page id。\n\n### Block Page\n\n存放实际数据的page，一个block包含一个k-v对数组，以及两个标记数组，occupied表示该bucket是否被占用，readable表示该bucket如果被占用是有效k-v对还是已经被懒删除。这里采用atomic_char(8位)来表示8个bucket的状态，同时可以通过cas技术来实现线程安全。\n\n同时包括两个主要函数，Insert和Remove。\n\nInsert首先通过cas判断occupied对应位是否为0，若是，置为1(cas自动实现)并插入数据，同时将readable对应位修改为1，若否，说明已经有k-v对占用，再次通过cas判断readable是否为0，若是，置为1(cas自动实现)并插入数据，若否，插入失败，返回false，由上层的哈希算法去决定如何处理。\n\nRemove采用懒删除的方法，因为这里采用线性探测，如果不采用懒删除，就会导致删除后的查找失败。所以具体操作就是不断轮询cas将readable对应位改为0，表示该位置是无效数据。\n\n### Linear Probing Hash Table\n\n实际的哈希表对象包含了一个header page的page id，所有操作都是从这里发起，先从磁盘中读取header page，再根据header page定位找到各个block page。此外还有一个哈希函数和一个key的比较器，一个用来读取page的缓冲池管理器，以及一个读写锁用于实现并发线程安全。\n\n同时哈希表包括了以下几个核心函数功能：\n\n- 初始化：创建header page和block page，传入的bucket数量不一定能整除每个block的bucket数量，但是问题不大，以传入的为准。注意创建完的page要Unpin，等到需要再加载进来。\n- GetValue()，Insert()，Remove()，GetSize()：这几个函数都具有类似的操作，比如查询操作，首先是读取header page，获取bucket总数，然后就是计算哈希值，取模得到blockIdx(在第几个block)和bucketIdx(在block的第几个bucket)，然后就是循环，动态加载block page，要记得对上一个block page进行Unpin。然后比较key之前要判断Occupied和Readable，即bucket里存放了有效k-v对才进行比较，由于这个project的哈希表是支持相同key的，所以这里查询到一个匹配的key之后并不能停，有两种情况可以退出，一种是扫描到了一个Occupied为0的bucket，因为key的哈希值余数相同(哈希冲突)的k-v对经过线性探测后一定是会处于连续的一段bucket；另外一种就是当所有bucket都被占用了，这时候如果用第一种判断就会死循环，所以需要记录是否走了完整一圈。而对于插入操作，当要插入的k-v对已存在，直接返回false，此外，当index走了一整圈还没找到可以插入的bucket，应该进行Resize(**Resize前要做好退出函数收尾工作**)，然后再进行插入。删除操作也和插入类似，而GetSize操作我的理解是查询实际k-v对数量，所以跟普通查询也没有差别。\n- Resize()：稍微复杂一些， 需要创建一些新的page(header page和block page)，然后遍历原哈希表的所有有效k-v对，重新哈希后插入新的page里，最后记得**更新这个哈希表的header page id**。\n\n并发安全方面，首先要弄清楚在每种操作中每个对象(哈希表对象和各个page对象)应该加读锁还是写锁，这取决于是否对这个对象进行写操作。比如对于哈希表对象，GetValue函数中只会对哈希表对象进行读操作，读取BPM对象，compartor对象等，所以加读锁，Insert和Remove同理，但是Resize就会修改哈希表对象的bucket数量，header page id，block数量，block映射关系等等，所以需要加写锁。对于page对象，包括header page和block page，在GetValue函数中，page只需要加读锁，在Insert和Remove函数中，header page只要加读锁，block page因为要插入，删除，要加写锁，在Resize函数中，新的page需要写入数据，所以要加写锁。\n\n注意在读写锁的实现中，加读锁是可以同时读，加了读锁可以再加写锁，但不能再加读锁，而且会等到所有读者完成才开始写锁，所以加读锁可以防止写。注意**加锁后一定要记得解锁，不然就很容易出现死锁**。\n","date":"Nov 17, 2020","category":{"name":"技术学习","permalink":"http://127.0.0.1:8080/category/技术学习"},"tags":[{"name":"CMU15-445","permalink":"http://127.0.0.1:8080/tags/CMU15-445"},{"name":"数据库","permalink":"http://127.0.0.1:8080/tags/数据库"}]},{"title":"CSharp语言集成查询(LINQ)","permalink":"http://127.0.0.1:8080/detail/csharp-linq-startup","content":"\r\n## 概述\r\n\r\nLINQ(Language Integrated Query，语言集成查询)，是C#的一个统一查询工具，可以用相同的基本查询表达式来支持不同的数据源，比如数据库，XML文档，内存对象等，LINQ查询返回的。\r\n\r\n例子：\r\n\r\n```c#\r\nint[] num = {1, 5, 2, 7, 6, 3, 9, 8, 4, 10};\r\nvar lowNum = from i in num where i > 5 orderby i descending select i;\r\nforeach (var x in lowNum) {\r\n    Console.WriteLine(x);\r\n}\r\n```\r\n\r\n每一个查询运算符(基本上)都有对应的一个**拓展方法**，因此上述的例子可以改写成\r\n\r\n```c#\r\nvar lowNum2 = num.Where(i => i > 5).OrderByDescending(i => i);\r\nforeach (var x in lowNum2) {\r\n    Console.WriteLine(x);\r\n}\r\n```\r\n\r\nLINQ查询返回的基本都是`IEnumerable<\\T>`或其派生接口，仅创建查询变量不会检索到任何数据，默认是延迟执行。\r\n\r\n对于`Count`、`Max`、`Average`和`First`等此查询，默认强制立即执行，返回单个值，`ToList`和`ToArray`方法也可以强制立即执行查询并缓存结果。\r\n\r\n![LINQ查询示意图](https://docs.microsoft.com/zh-cn/dotnet/csharp/programming-guide/concepts/linq/media/introduction-to-linq-queries/linq-query-complete-operation.png)\r\n\r\n\r\n除了上面的简单查询的例子，LINQ支持分组，连接，子查询等复杂操作，详见文档。\r\n","date":"Jan 8, 2021","category":{"name":"技术学习","permalink":"http://127.0.0.1:8080/category/技术学习"},"tags":[{"name":"C#","permalink":"http://127.0.0.1:8080/tags/C#"},{"name":"LINQ","permalink":"http://127.0.0.1:8080/tags/LINQ"}]},{"title":"Vue基础入门——从0搭建一个博客","permalink":"http://127.0.0.1:8080/detail/vue-startup-with-my-blog","content":"\n> 拖了很久的一篇，一直想记录一下Vue开发一个简单的个人前端项目的全过程<del>(免得每写一次前端都要重新查一遍文档和博客)</del>，直到Vue3都出了，我终于写下这篇小短文了<del>(Vue2)</del>。\n\n## 初始化\n\n首先全局安装`vue`和`vue-cli`\n\n```shell\nnpm install -g vue\nnpm install -g @vue/cli\n```\n\n然后通过`vue-cli`创建项目\n\n```shell\nvue init xxx\n```\n\n## 页面\n\n在`src/components`下写一些组件，创建文件夹`src/pages`放页面，在`src`目录下的`App.\nvue`是主组件，也是入口页面，里面通常是放`Header`，`Footer`，`router-view`这些组件，其中`router-view`是可以根据路由的变化切换页面的，后面会提到。\n\n一个Vue组件有一些通用的结构，如下：\n\n```vue\n<template>\n  <div>\n    <!--组件内容-->\n  </div>\n</template>\n<script>  // js部分\nexport default {\n  name:\"Navbar\", // 组件名\n  data() {       // 数据，写成函数的形式，使用数据副本，避免了数据污染\n    return {\n      current: 'home',\n      searchText:\"\",\n    };\n  },\n  methods: {     // 方法\n    search(){\n      // ...\n    }\n  },\n  watch:{        // 监听某个数据的变化 \n    searchText(nVal){   // 函数名就是监听的数据名，或者写成数据名:function(){}\n      //参数就是新值，如果有两个参数，第二个就是旧值\n    },\n  },\n};\n</script>\n<style scoped>  /** css部分，scoped表示只在组件内生效 **/\n</style>\n```\n\n## 父子组件交互\n\n### 父组件传值给子组件\n\n在子组件中定义`props`，包括名字和类型，默认值等等。\n\n```vue\n<script>\nexport default {\n  props: {\n    pageSize: {\n      type: Number\n    },\n  },\n}\n</script>\n```\n\n父组件传值，注意子组件props的命名是camelCase，而父组件传值则使用横线划分。除此之外还有很多其他的命名规范，<del>就不一一遵守了。</del>\n\n```vue\n<Pagination \n    :page-size=\"pageSize\"\n    @change=\"pageChange\">\n</Pagination>\n```\n\n### 子组件传值给父组件\n\n在子组件中通过调用`emit`函数来触发父组件的回调函数，例如以下代码触发了父组件的`change`函数，也就是`@change`所指定的函数。\n\n```js\nthis.$emit(\"change\", para);\n```\n\n## 路由\n\n首先安装`vue-router`，在`main.js`同级文件夹里创建`routes.js`，格式如下\n\n```js\nVue.use(VueRouter)\nconst routes = [\n    {\n        path: \"/about\",\n        name: \"about\",\n        component: AboutPage,\n    },\n    // 路由匹配的优先级问题\n    // 如果这个路由放在第一位，后面的/about会匹配到，把about看成page参数\n    // {\n    //     path: \"/:page?/:tag?/:cat?\",\n    //     name: \"home\",\n    //     component: PostListPage,\n    // },\n\n]\nconst router = new VueRouter({\n    routes\n})\nconst VueRouterPush = VueRouter.prototype.push\nVueRouter.prototype.push = function push (to) {\n    return VueRouterPush.call(this, to).catch(err => err)\n}\nexport default router\n```\n\n在`main.js`中导入，并作为参数构造Vue实例，注意要在`App.vue`中配置好`<router-view>`组件\n\n```js\nimport router from \"./routes\";\nnew Vue({\n    render: h => h(App),\n    router, // 必须是router或者指定router:xxx\n}).$mount('#app')\n```\n\n路由中复杂的参数以及其他配置具体参照文档。\n\n## 获取数据\n\n通常使用`axios`来获取数据，在本项目中，用将`md`文件生成`json`文件的方法实现一个“伪后端”，将文件放到`public`文件夹下，`axios`也同样可以获取到。\n\n```js\nthis.$axios.get(\"/data.json\").then((resp) => {\n    // ...\n})\n```\n\n注意`axios`的使用还需要在`main.js`中将`axios`绑定到`Vue`原型上。\n\n```js\nVue.prototype.$axios = axios\n```\n\n## 其他\n\n博客里用到了`pure.css`样式库，用法很简单，先`npm`安装，然后在`main.js`里`import Purecss from 'purecss'`导入，再用`Vue.use(Purecss)\n`就能用上这个样式库，其他插件/组件库/样式库等也基本同理，看对应文档即可。\n\n博客除了`pure.css`，还用到了`normalize.css`，`awesome-icon`，`mavon-editor`等。\n\n## 运行\n\n在项目目录下先`npm install`安装依赖包，然后`npm run serve`可以在本地跑起来，`npm run build`可以打包成静态文件放服务器上。\n\n这个博客是用了`Vercel`托管，绑定`github`项目后，每次`push`完就会自动构建打包。\n","date":"Jan 12, 2021","category":{"name":"技术学习","permalink":"http://127.0.0.1:8080/category/技术学习"},"tags":[{"name":"博客","permalink":"http://127.0.0.1:8080/tags/博客"},{"name":"Vue","permalink":"http://127.0.0.1:8080/tags/Vue"}]},{"title":"dotnet5-项目快速起步","permalink":"http://127.0.0.1:8080/detail/dotnet5-project-quick-start","content":"\n> 用最简单的方式介绍dotnet中我所用到的一小小小部分，更多具体信息需要查看官方文档。\n\n## Solution和Project\n\n一个Solution可以包括多个Project，比如在我的项目中，为了方便(<del>其实是电脑打开两个IDE太卡</del>)，把MongoDB-PBAC-Server/Client/Common三个Project放在同一个Solution里，其中sln文件就是Solution的相关配置文件，csproj文件就是Project的配置文件。\n\n在这里，我的Project分为两种，\\*-Common是一个Class Library(类库)，可以打包成NuGet包，另外两个是Console Application(控制台应用)，可以引入NuGet包，直接运行或者打包成可执行文件。\n\n## 打包\n\n`dotnet build`命令会将项目及其依赖包生成为一大堆二进制文件(.dll，.pdb等)。\n\n`build`之后，通过`dotnet pack --no-build`命令可以将项目打包为NuGet包，也就是一个.nupkg文件，`--no-build`指明无需再build一次，所以也可以直接`pack`而不用先`buiid`。\n\n也可以在csproj文件中指定`<GeneratePackageOnBuild>true</GeneratePackageOnBuild>`，这样子在`build`的时候会自动`pack`。\n\n默认情况下会在`**/bin/Debug/`下生成一个.nupkg文件，更多选项可以参考文档。\n\n得到的NuGet包可以放到某个文件夹里，然后在本地的NuGet配置文件(`~/.nuget/NuGet/NuGet.Config`)中指定路径，相当于添加本地源，这样子就可以通过`dotnet add`或者直接在IDE中安装NuGet包，对应的在csproj文件中就会有响应的`<PackageReference Include=\"\" Version=\"\"/>`。\n\n## 发布\n\n`dotnet publish -r linux-x64 -p:PublishSingleFile=true -p:PublishTrimmed=true`，该命令默认会在`**/bin/Debug/net5.0/linux-x64/publish`下生成一个可执行文件。\n\n其中`-r`后面接linux-x64是指定运行时(runtime)，而`PublishSingleFile=true`和`PublishTrimmed=true`分别表示生成单个可执行文件以及对生成的文件进行裁剪，减小文件大小。\n\n## build和publish\n\n简单来说，`build`不会把第三方的依赖包打包进入，所以`build`之后的可执行文件可以在本机运行，在运行时依赖本地的依赖包，而`publish`是专门用于发布的，所以会将所有依赖包打包进入，可以直接移植到其他机器运行。\n\n根据官方文档，其实在.Net Core3.0之后的项目，库依赖项会被打包到输出文件夹。 如果没有其他任何特定于发布的逻辑(例如，Web项目具有的逻辑)，也可以直接部署。\n","date":"Dec 14, 2020","category":{"name":"技术学习","permalink":"http://127.0.0.1:8080/category/技术学习"},"tags":[{"name":"C#","permalink":"http://127.0.0.1:8080/tags/C#"},{"name":"dotnet","permalink":"http://127.0.0.1:8080/tags/dotnet"}]},{"title":"how-a-relational-database-works(原文)","permalink":"http://127.0.0.1:8080/detail/how-a-relational-database-work","content":"\n> 原文出处 [http://coding-geek.com/how-databases-work/](http://coding-geek.com/how-databases-work/)\n\n> 网上流传的翻译版看起来不错，费尽心思找到英文原版出处，看起来网站有了一定年头了，这里边读边做个记录保存，防止以后文章丢失。\n\n> 这是一篇2015年发布的文章，对关系型数据库的整个工作流程介绍得比较清楚。\n\n## Preface\n\nWhen it comes to relational databases, I can’t help thinking that something is missing. They’re used everywhere. There are many different databases: from the small and useful SQLite to the powerful Teradata. But, there are only a few articles that explain how a database works. You can google by yourself “how does a relational database work” to see how few results there are. Moreover, those articles are short.  Now, if you look for the last trendy technologies (Big Data, NoSQL or JavaScript), you’ll find more in-depth articles explaining how they work.\n\nAre relational databases too old and too boring to be explained outside of university courses, research papers and books?\n\nAs a developer, I HATE using something I don’t understand. And, if databases have been used for 40 years, there must be a reason. Over the years, I’ve spent hundreds of hours to really understand these weird black boxes I use every day. **Relational Databases are** very interesting because they’re **based on useful and reusable concepts**. If understanding a database interests you but you’ve never had the time or the will to dig into this wide subject, you should like this article.\n\nThough the title of this article is explicit, **the aim of this article is NOT to understand how to use a database**. Therefore, **you should already know how to write a simple join query and basic CRUD queries**; otherwise you might not understand this article. This is the only thing you need to know, I’ll explain everything else.\n\nI’ll start with some computer science stuff like time complexity. I know that some of you hate this concept but, without it, you can’t understand the cleverness inside a database. Since it’s a huge topic, **I’ll focus on** what I think is essential: **the way a database handles an SQL query**. I’ll only present **the basic concepts behind a database** so that at the end of the article you’ll have a good idea of what’s happening under the hood.\n\nSince it’s a long and technical article that involves many algorithms and data structures, take your time to read it. Some concepts are more difficult to understand; you can skip them and still get the overall idea.\n\nFor the more knowledgeable of you, this article is more or less divided into 3 parts:\n\n- An overview of low-level and high-level database components\n- An overview of the query optimization process\n- An overview of the transaction and buffer pool management\n\n## Back to basics\n\nA long time ago (in a galaxy far, far away….), developers had to know exactly the number of operations they were coding. They knew by heart their algorithms and data structures because they couldn’t afford to waste the CPU and memory of their slow computers.\n\nIn this part, I’ll remind you about some of these concepts because they are essential to understand a database. I’ll also introduce the notion of **database index**.\n\n### O(1) vs O($n^2$)\n\nNowadays, many developers don’t care about time complexity … and they’re right!\n\nBut when you deal with a large amount of data (I’m not talking about thousands) or if you’re fighting for milliseconds, it becomes critical to understand this concept. And guess what, databases have to deal with both situations! I won’t bore you a long time, just the time to get the idea. This will help us later to understand the concept of **cost based optimization**.\n\n#### The concept\n\nThe **time complexity is used to see how long an algorithm will take for a given amount of data**. To describe this complexity, computer scientists use the mathematical big O notation. This notation is used with a function that describes how many operations an algorithm needs for a given amount of input data.\n\nFor example, when I say “this algorithm is in O( some_function() )”, it means that for a certain amount of data the algorithm needs some_function(a_certain_amount_of_data) operations to do its job.\n\n**What’s important** is not the amount of data but **the way the number of operations increases when the amount of data increases**. The time complexity doesn’t give the exact number of operations but a good idea.\n\n![TimeComplexity.png](https://i.loli.net/2020/12/09/u8k2U4rOFzaNlgf.png)\n\nIn this figure, you can see the evolution of different types of complexities. I used a logarithmic scale to plot it. In other words, the number of data is quickly increasing from 1 to 1 billion. We can see that:\n\n- The O(1) or constant complexity stays constant (otherwise it wouldn’t be called constant complexity).\n- The O(log(n)) stays low even with billions of data.\n- The worst complexity is the O(n2) where the number of operations quickly explodes.\n- The two other complexities are quickly increasing.\n\n#### Examples\n\nWith a low amount of data, the difference between O(1) and O(n2) is negligible. For example, let’s say you have an algorithm that needs to process 2000 elements.\n\n- An O(1) algorithm will cost you 1 operation\n- An O(log(n)) algorithm will cost you 7 operations\n- An O(n) algorithm will cost you 2 000 operations\n- An O(n*log(n)) algorithm will cost you 14 000 operations\n- An O(n2) algorithm will cost you 4 000 000 operations\n\nThe difference between O(1) and O(n2) seems a lot (4 million) but you’ll lose at max 2 ms, just the time to blink your eyes. Indeed, current processors can handle [hundreds of millions of operations per second](https://en.wikipedia.org/wiki/Instructions_per_second). This is why performance and optimization are not an issue in many IT projects.\n\nAs I said, it’s still important to know this concept when facing a huge number of data. If this time the algorithm needs to process 1 000 000 elements (which is not that big for a database):\n\n- An O(1) algorithm will cost you 1 operation\n- An O(log(n)) algorithm will cost you 14 operations\n- An O(n) algorithm will cost you 1 000 000 operations\n- An O(n*log(n)) algorithm will cost you 14 000 000 operations\n- An O(n2) algorithm will cost you 1 000 000 000 000 operations\n\nI didn’t do the math but I’d say with the O(n2) algorithm you have the time to take a coffee (even a second one!). If you put another 0 on the amount of data, you’ll have the time to take a long nap.\n\n#### Going deeper\n\nTo give you an idea:\n\n- A search in a good hash table gives an element in O(1)\n- A search in a well-balanced tree gives a result in O(log(n))\n- A search in an array gives a result in O(n)\n- The best sorting algorithms have an O(n*log(n)) complexity.\n- A bad sorting algorithm has an O(n2) complexity\n\nNote: In the next parts, we’ll see these algorithms and data structures.\n\nThere are multiple types of time complexity:\n\n- the average case scenario\n- the best case scenario\n- and the worst case scenario\n\nThe time complexity is often the worst case scenario.\n\nI only talked about time complexity but complexity also works for:\n\n- the memory consumption of an algorithm\n- the disk I/O consumption of an algorithm\n\nOf course there are worse complexities than n2, like:\n\n- $n^4$: that sucks! Some of the algorithms I’ll mention have this complexity.\n- $3^n$: that sucks even more! One of the algorithms we’re going to see in the middle of this article has this complexity (and it’s really used in many databases).\n- factorial n : you’ll never get your results, even with a low amount of data.\n- $n^n$: if you end-up with this complexity, you should ask yourself if IT is really your field…\n\nNote: I didn’t give you the real definition of the big O notation but just the idea. You can read this article on [Wikipedia](https://en.wikipedia.org/wiki/Big_O_notation) for the real (asymptotic) definition.\n\n### Merge Sort\n\nWhat do you do when you need to sort a collection? What? You call the sort() function …  ok, good answer… But for a database you have to understand how this sort() function works.\n\nThere are several good sorting algorithms so I’ll focus on the most important one: **the merge sort**. You might not understand right now why sorting data is useful but you should after the part on query optimization. Moreover, understanding the merge sort will help us later to understand a common database join operation called the **merge join**.\n\n#### Merge\n\nLike many useful algorithms, the merge sort is based on a trick: merging 2 sorted arrays of size N/2 into a N-element sorted array only costs N operations. This operation is called a **merge**.\n\nLet’s see what this means with a simple example:\n\n![merge_sort_3.png](https://i.loli.net/2020/12/09/gcbQ4NfiMUX5YKa.png)\n\nYou can see on this figure that to construct the final sorted array of 8 elements, you only need to iterate one time in the 2 4-element arrays. Since both 4-element arrays are already sorted:\n\n- 1) you compare both current elements in the 2 arrays (current=first for the first time)\n- 2) then take the lowest one to put it in the 8-element array\n- 3) and go to the next element in the array you took the lowest element\n- and repeat 1,2,3 until you reach the last element of one of the arrays.\n- Then you take the rest of the elements of the other array to put them in the 8-element array.\n\nThis works because both 4-element arrays are sorted and therefore you don’t need to “go back” in these arrays.\n\nNow that we’ve understood this trick, here is my pseudocode of the merge sort.\n\n```cpp\narray mergeSort(array a)\n   if(length(a)==1)\n      return a[0];\n   end if\n \n   //recursive calls\n   [left_array right_array] := split_into_2_equally_sized_arrays(a);\n   array new_left_array := mergeSort(left_array);\n   array new_right_array := mergeSort(right_array);\n \n   //merging the 2 small ordered arrays into a big one\n   array result := merge(new_left_array,new_right_array);\n   return result;\n```\n\nThe merge sort breaks the problem into smaller problems then finds the results of the smaller problems to get the result of the initial problem (note: this kind of algorithms is called divide and conquer). If you don’t understand this algorithm, don’t worry; I didn’t understand it the first time I saw it. If it can help you, I see this algorithm as a two-phase algorithm:\n\n- The division phase where the array is divided into smaller arrays\n- The sorting phase where the small arrays are put together (using the merge) to form a bigger array.\n\n#### Division phase\n\n![merge_sort_1.png](https://i.loli.net/2020/12/09/U7gyHqDIf1QsicF.png)\n\nDuring the division phase, the array is divided into unitary arrays using 3 steps. The formal number of steps is log(N)  (since N=8, log(N) = 3).\n\nHow do I know that?\n\n<del>I’m a genius!</del> In one word: mathematics. The idea is that each step divides the size of the initial array by 2. The number of steps is the number of times you can divide the initial array by two. This is the exact definition of logarithm (in base 2).\n\n#### Sorting phase\n\n![merge_sort_2.png](https://i.loli.net/2020/12/09/O8vbUkK7LDzIhGn.png)\n\nIn the sorting phase, you start with the unitary arrays. During each step, you apply multiple merges and the overall cost is N=8 operations:\n\n- In the first step you have 4 merges that cost 2 operations each\n- In the second step you have 2 merges that cost 4 operations each\n- In the third step you have 1 merge that costs 8 operations\n\nSince there are log(N) steps, **the overall costs N * log(N) operations**.\n\n#### The power of the merge sort\n\nWhy this algorithm is so powerful?\n\nBecause:\n\n- You can modify it in order to reduce the memory footprint, in a way that you don’t create new arrays but you directly modify the input array.\n\nNote: this kind of algorithms is called [in-place](https://en.wikipedia.org/wiki/In-place_algorithm).\n\n- You can modify it in order to use disk space and a small amount of memory at the same time without a huge disk I/O penalty. The idea is to load in memory only the parts that are currently processed. This is important when you need to sort a multi-gigabyte table with only a memory buffer of 100 megabytes.\n\nNote: this kind of algorithms is called [external sorting](https://en.wikipedia.org/wiki/External_sorting).\n\n- You can modify it to run on multiple processes/threads/servers.\n\nFor example, the distributed merge sort is one of the key components of [Hadoop](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapreduce/Reducer.html) (which is THE framework in Big Data).\n\nT- his algorithm can turn lead into gold (true fact!).\n\nThis sorting algorithm is used in most (if not all) databases but it’s not the only one. If you want to know more, you can read this [research paper](http://wwwlgis.informatik.uni-kl.de/archiv/wwwdvs.informatik.uni-kl.de/courses/DBSREAL/SS2005/Vorlesungsunterlagen/Implementing_Sorting.pdf) that discusses the pros and cons of the common sorting algorithms in a database.\n\n### Array, Tree and Hash table\n\nNow that we understand the idea behind time complexity and sorting, I have to tell you about 3 data structures. It’s important because they’re **the backbone of modern databases**. I’ll also introduce the notion of **database index**.\n\n#### Array\n\nThe two-dimensional array is the simplest data structure. A table can be seen as an array. For example:\n\n![array.png](https://i.loli.net/2020/12/09/lok4xce6wBCRgsp.png)\n\nThis 2-dimensional array is a table with rows and columns:\n\n- Each row represents a subject\n- The columns the features that describe the subjects.\n- Each column stores a certain type of data (integer, string, date …).\n\nThough it’s great to store and visualize data, when you need to look for a specific value it sucks.\n\nFor example, **if you want to find all the guys who work in the UK**, you’ll have to look at each row to find if the row belongs to the UK. **This will cost you N operations** (N being the number of rows) which is not bad but could there be a faster way? This is where trees come into play.\n\nNote: Most modern databases provide advanced arrays to store tables efficiently like heap-organized tables or index-organized tables. But it doesn’t change the problem of fast searching for a specific condition on a group of columns.\n\n#### Tree and database index\n\nA binary search tree is a binary tree with a special property, the key in each node must be:\n\n- greater than all keys stored in the left sub-tree\n- smaller than all keys stored in the right sub-tree\n\nLet’s see what it means visually\n\n##### The idea\n\n![image.png](https://i.loli.net/2020/12/09/qrIRuHnjTLyYS82.png)\n\n\nThis tree has N=15 elements. Let’s say I’m looking for 208:\n\n- I start with the root whose key is 136. Since 136<208, I look at the right sub-tree of the node 136.\n- 398>208 so, I look at the left sub-tree of the node 398\n- 250>208 so, I look at the left sub-tree of the node 250\n- 200<208 so, I look at the right sub-tree of the node 200. But 200 doesn’t have a right subtree, the value doesn’t exist (because if it did exist it would be in the right subtree of 200)\n\nNow let’s say I’m looking for 40\n\n- I start with the root whose key is 136. Since 136>40, I look at the left sub-tree of the node 136.\n- 80>40 so, I look at the left sub-tree of the node 80\n- 40= 40, the node exists. I extract the id of the row inside the node (it’s not in the figure) and look at the table for the given row id.\n- Knowing the row id let me know where the data is precisely on the table and therefore I can get it instantly.\n\nIn the end, both searches cost me the number of levels inside the tree. If you read carefully the part on the merge sort you should see that there are log(N) levels. So the **cost of the search is log(N)**, not bad!\n\n##### Back to our problem\n\nBut this stuff is very abstract so let’s go back to our problem. Instead of a stupid integer, imagine the string that represents the country of someone in the previous table. Suppose you have a tree that contains the column “country” of the table:\n\n- If you want to know who is working in the UK\n- you look at the tree to get the node that represents the UK\n- inside the “UK node” you’ll find the locations of the rows of the UK workers.\n\nThis search only costs you log(N) operations instead of N operations if you directly use the array. What you’ve just imagined was a **database index**.\n\nYou can build a tree index for any group of columns (a string, an integer, 2 strings, an integer and a string, a date …) as long as you have a function to compare the keys (i.e. the group of columns) so that you can establish an **order among the keys** (which is the case for any basic types in a database).\n\n##### B+Tree Index\n\nAlthough this tree works well to get a specific value, there is a BIG problem when you need to **get multiple elements between two values**. It will cost O(N) because you’ll have to look at each node in the tree and check if it’s between these 2 values (for example, with an in-order traversal of the tree). Moreover this operation is not disk I/O friendly since you’ll have to read the full tree. We need to find a way to efficiently do a **range query**. To answer this problem, modern databases use a modified version of the previous tree called B+Tree. In a B+Tree:\n\n- only the lowest nodes (the leaves) store information (the location of the rows in the associated table)\n- the other nodes are just here to route to the right node during the search.\n\n![image.png](https://i.loli.net/2020/12/09/xPTLm3d4CtSVNcH.png)\n\nAs you can see, there are more nodes (twice more). Indeed, you have additional nodes, the “decision nodes” that will help you to find the right node (that stores the location of the rows in the associated table). But the search complexity is still in O(log(N)) (there is just one more level). The big difference is that **the lowest nodes are linked to their successors**.\n\nWith this B+Tree, if you’re looking for values between 40 and 100:\n\n- You just have to look for 40 (or the closest value after 40 if 40 doesn’t exist) like you did with the previous tree.\n- Then gather the successors of 40 using the direct links to the successors until you reach 100.\n\nLet’s say you found M successors and the tree has N nodes. The search for a specific node costs log(N) like the previous tree. But, once you have this node, you get the M successors in M operations with the links to their successors. **This search only costs M + log(N)** operations vs N operations with the previous tree. Moreover, you don’t need to read the full tree (just M + log(N) nodes), which means less disk usage. If M is low (like 200 rows) and N large (1 000 000 rows) it makes a BIG difference.\n\nBut there are new problems (again!). If you add or remove a row in a database (and therefore in the associated B+Tree index):\n\n- you have to keep the order between nodes inside the B+Tree otherwise you won’t be able to find nodes inside the mess.\n- you have to keep the lowest possible number of levels in the B+Tree otherwise the time complexity in O(log(N)) will become O(N).\n\nI other words, the B+Tree needs to be self-ordered and self-balanced. Thankfully, this is possible with smart deletion and insertion operations. But this comes with a cost: the insertion and deletion in a B+Tree are in O(log(N)). This is why some of you have heard that **using too many indexes is not a good idea**. Indeed, **you’re slowing down the fast insertion/update/deletion of a row** in a table since the database needs to update the indexes of the table with a costly O(log(N)) operation per index. Moreover, adding indexes means more workload for the **transaction manager** (we will see this manager at the end of the article).\n\nFor more details, you can look at the Wikipedia 【article about B+Tree](https://en.wikipedia.org/wiki/B%2B_tree). If you want an example of a B+Tree implementation in a database, look at [this article](http://blog.jcole.us/2013/01/07/the-physical-structure-of-innodb-index-pages/) and [this article](http://blog.jcole.us/2013/01/10/btree-index-structures-in-innodb/) from a core developer of MySQL. They both focus on how innoDB (the engine of MySQL) handles indexes.\n\nNote: I was told by a reader that, because of low-level optimizations, the B+Tree needs to be fully balanced.\n\n#### Hash table\n\nOur last important data structure is the hash table. It’s very useful when you want to quickly look for values.  Moreover, understanding the hash table will help us later to understand a common database join operation called the **hash join**. This data structure is also used by a database to store some internal stuff (like the **lock table** or the **buffer pool**, we’ll see both concepts later)\n\nThe hash table is a data structure that quickly finds an element with its key. To build a hash table you need to define:\n\n- **a key** for your elements\n- **a hash function** for the keys. The computed hashes of the keys give the locations of the elements (called buckets).\n- **a function to compare the keys**. Once you found the right bucket you have to find the element you’re looking for inside the bucket using this comparison.\n\n##### A simple example\n\nLet’s have a visual example:\n\n![image.png](https://i.loli.net/2020/12/09/VCh1Jud94jsLT7E.png)\n\nThis hash table has 10 buckets. Since I’m lazy I only drew 5 buckets but I know you’re smart so I let you imagine the 5 others. The Hash function I used is the modulo 10 of the key. In other words I only keep the last digit of the key of an element to find its bucket:\n\n- if the last digit is 0 the element ends up in the bucket 0,\n- if the last digit is 1 the element ends up in the bucket 1,\n- if the last digit is 2 the element ends up in the bucket 2,\n- …\n\nThe compare function I used is simply the equality between 2 integers.\n\nLet’s say you want to get the element 78:\n\n- The hash table computes the hash code for 78 which is 8.\n- It looks in the bucket 8, and the first element it finds is 78.\n- It gives you back the element 78\n- **The search only costs 2 operations** (1 for computing the hash value and the other for finding the element inside the bucket).\n\nNow, let’s say you want to get the element 59:\n\n- The hash table computes the hash code for 59 which is 9.\n- It looks in the bucket 9, and the first element it finds is 99. Since 99!=59, element 99 is not the right element.\n- Using the same logic, it looks at the second element (9), the third (79), … , and the last (29).\n- The element doesn’t exist.\n- **The search costs 7 operations**.\n \n\n##### A good hash function\n\nAs you can see, depending on the value you’re looking for, the cost is not the same!\n\nIf I now change the hash function with the modulo 1 000 000 of the key (i.e. taking the last 6 digits), the second search only costs 1 operation because there are no elements in the bucket 000059. **The real challenge is to find a good hash function that will create buckets that contain a very small amount of elements**.\n\nIn my example, finding a good hash function is easy. But this is a simple example, finding a good hash function is more difficult when the key is:\n\n- a string (for example the last name of a person)\n- 2 strings (for example the last name and the first name of a person)\n- 2 strings and a date (for example the last name, the first name and the birth date of a person)\n- …\n\n**With a good hash function, the search in a hash table is in O(1)**.\n\n##### Array vs hash table\n\nWhy not using an array?\n\nHum, you’re asking a good question.\n\n- A hash table can be half loaded in memory and the other buckets can stay on disk.\n- With an array you have to use a contiguous space in memory. If you’re loading a large table it’s very difficult to have enough contiguous space.\n- With a hash table you can choose the key you want (for example the country AND the last name of a person).\n\nFor more information, you can read my article on the [Java HashMap](http://coding-geek.com/how-does-a-hashmap-work-in-java/) which is an efficient hash table implementation; you don’t need to understand Java to understand the concepts inside this article.\n\n## Global overview\n\nWe’ve just seen the basic components inside a database. We now need to step back to see the big picture.\n\nA database is a collection of information that can easily be accessed and modified. But a simple bunch of files could do the same. In fact, the simplest databases like SQLite are nothing more than a bunch of files. But SQLite is a well-crafted bunch of files because it allows you to:\n\n- use transactions that ensure data are safe and coherent\n- quickly process data even when you’re dealing with millions of data\n\nMore generally, a database can be seen as the following figure:\n\n![image.png](https://i.loli.net/2020/12/09/kPF2rUq8ICYOa6X.png)\n\nBefore writing this part, I’ve read multiple books/papers and every source had its on way to represent a database. So, don’t focus too much on how I organized this database or how I named the processes because I made some choices to fit the plan of this article. What matters are the different components; the overall idea is that **a database is divided into multiple components that interact with each other**.\n\n<u>The core components:</u>\n\n- **The process manager**: Many databases have a pool of processes/threads that needs to be managed. Moreover, in order to gain nanoseconds, some modern databases use their own threads instead of the Operating System threads.\n- **The network manager**: Network I/O is a big issue, especially for distributed databases. That’s why some databases have their own manager.\n- **File system manager**: Disk I/O is the first bottleneck of a database. Having a manager that will perfectly handle the Operating System file system or even replace it is important.\n- **The memory manager**: To avoid the disk I/O penalty a large quantity of ram is required. But if you handle a large amount of memory, you need an efficient memory manager. Especially when you have many queries using memory at the same time.\n- **Security Manager**: for managing the authentication and the authorizations of the users\n- **Client manager**: for managing the client connections\n- …\n\n<u>The tools:</u>\n\n- **Backup manager**: for saving and restoring a database.\n- **Recovery manager**: for restarting the database in a coherent state after a crash\n- **Monitor manager**: for logging the activity of the database and providing tools to monitor a database\n- **Administration manager**: for storing metadata (like the names and the structures of the tables) and providing tools to manage databases, schemas, tablespaces, …\n- …\n\n<u>The query Manager:</u>\n\n- **Query parser**: to check if a query is valid\n- **Query rewriter**: to pre-optimize a query\n- **Query optimizer**: to optimize a query\n- **Query executor**: to compile and execute a query\n\n<u>The data manager:</u>\n\n- **Transaction manager**: to handle transactions\n- **Cache manager**: to put data in memory before using them and put data in memory before writing them on disk\n- **Data access manager**: to access data on disk\n\nFor the rest of this article, I’ll focus on how a database manages an SQL query through the following processes:\n\n- the client manager\n- the query manager\n- the data manager (I’ll also include the recovery manager in this part)\n\n\n## Client manager\n\n![image.png](https://i.loli.net/2020/12/09/gibURf7QSyF5ZWp.png)\n\nThe client manager is the part that handles the communications with the client. The client can be a (web) server or an end-user/end-application. The client manager provides different ways to access the database through a set of well-known APIs: JDBC, ODBC, OLE-DB …\n\nIt can also provide proprietary database access APIs.\n\nWhen you connect to a database:\n\n- The manager first checks your **authentication** (your login and password) and then checks if you have the **authorizations** to use the database. These access rights are set by your DBA.\n- Then, it checks if there is a process (or a thread) available to manage your query.\n- It also checks if the database if not under heavy load.\n- It can wait a moment to get the required resources. If this wait reaches a timeout, it closes the connection and gives a readable error message.\n- Then it **sends your query to the query manager** and your query is processed\n- Since the query processing is not an “all or nothing” thing, as soon as it gets data from the query manager, it **stores the partial results in a buffer and start sending** them to you.\n- In case of problem, it stops the connection, gives you a **readable explanation** and releases the resources.\n\n## Query manager\n\n![image.png](https://i.loli.net/2020/12/09/vY4P3QwCyxoF57z.png)\n\n**This part is where the power of a database lies**. During this part, an ill-written query is transformed into a **fast** executable code. The code is then executed and the results are returned to the client manager. It’s a multiple-step operation:\n\n- the query is first **parsed** to see if it’s valid\n- it’s then **rewritten** to remove useless operations and add some pre-optimizations\n- it’s then **optimized** to improve the performances and transformed into an execution and data access plan.\n- then the plan is **compiled**\n- at last, it’s **executed**\n\nIn this part, I won’t talk a lot about the last 2 points because they’re less important.\n\nAfter reading this part, if you want a better understanding I recommend reading:\n\n- The initial research paper (1979) on cost based optimization: [Access Path Selection in a Relational Database Management System](https://www.cs.berkeley.edu/~brewer/cs262/3-selinger79.pdf). - This article is only 12 pages and understandable with an average level in computer science.\n- A very good and in-depth presentation on how DB2 9.X optimizes queries [here](http://infolab.stanford.edu/~hyunjung/cs346/db2-talk.pdf)\n- A very good presentation on how PostgreSQL optimizes queries [here](http://momjian.us/main/writings/pgsql/optimizer.pdf). It’s the most accessible document since it’s more a presentation on “let’s see what query plans PostgreSQL gives in these situations“ than a “let’s see the algorithms used by PostgreSQL”.\n- The official [SQLite documentation](https://www.sqlite.org/optoverview.html) about optimization. It’s “easy” to read because SQLite uses simple rules. Moreover, it’s the only official documentation that really explains how it works.\n- A good presentation on how SQL Server 2005 optimizes queries [here](https://blogs.msdn.com/cfs-filesystemfile.ashx/__key/communityserver-components-postattachments/00-08-50-84-93/QPTalk.pdf)\n- A white paper about optimization in Oracle 12c [here](http://www.oracle.com/technetwork/database/bi-datawarehousing/twp-optimizer-with-oracledb-12c-1963236.pdf)\n- 2 theoretical courses on query optimization from the authors of the book “DATABASE SYSTEM CONCEPTS” [here](http://codex.cs.yale.edu/avi/db-book/db6/slide-dir/PPT-dir/ch12.ppt) and [here](http://codex.cs.yale.edu/avi/db-book/db6/slide-dir/PPT-dir/ch13.ppt). A good read that focuses on disk I/O cost but a good level in CS is required.\n- Another [theoretical course](https://www.informatik.hu-berlin.de/de/forschung/gebiete/wbi/teaching/archive/sose05/dbs2/slides/09_joins.pdf) that I find more accessible but that only focuses on join operators and disk I/O.\n\n### Query parser\n\nEach SQL statement is sent to the parser where it is checked for correct syntax. If you made a mistake in your query the parser will reject the query. For example, if you wrote “SLECT …” instead of “SELECT …”,  the story ends here.\n\nBut this goes deeper. It also checks that the keywords are used in the right order. For example a WHERE before a SELECT will be rejected.\n\nThen, the tables and the fields inside the query are analyzed. The parser uses the metadata of the database to check:\n\n- If the **tables exist**\n- If the **fields** of the tables exist\n- If the **operations** for the types of the fields **are possible** (for example you can’t compare an integer with a string, you can’t use a substring() function on an integer)\n \n\nThen it checks if you have the **authorizations** to read (or write) the tables in the query. Again, these access rights on tables are set by your DBA.\n\nDuring this parsing, the SQL query is transformed into an internal representation (often a tree)\n\nIf everything is ok then the internal representation is sent to the query rewriter.\n\n### Query rewriter\n\nAt this step, we have an internal representation of a query. The aim of the rewriter is:\n\n- to pre-optimize the query\n- to avoid unnecessary operations\n- to help the optimizer to find the best possible solution\n \nThe rewriter executes a list of known rules on the query. If the query fits a pattern of a rule, the rule is applied and the query is rewritten.  Here is a non-exhaustive list of (optional) rules:\n\n- **View merging**: If you’re using a view in your query, the view is transformed with the SQL code of the view.\n- **Subquery flattening**: Having subqueries is very difficult to optimize so the rewriter will try to modify a query with a subquery to remove the subquery.\n\nFor example\n\n```sql\nSELECT PERSON.*\nFROM PERSON\nWHERE PERSON.person_key IN\n(SELECT MAILS.person_key\nFROM MAILS\nWHERE MAILS.mail LIKE 'christophe%');\n```\n\nWill be replaced by\n\n```sql\nSELECT PERSON.*\nFROM PERSON, MAILS\nWHERE PERSON.person_key = MAILS.person_key\nand MAILS.mail LIKE 'christophe%';\n```\n\n- **Removal of unnecessary operators**: For example if you use a DISTINCT whereas you have a UNIQUE constraint that prevents the data from being non-unique, the DISTINCT keyword is removed.\n- **Redundant join elimination**: If you have twice the same join condition because one join condition is hidden in a view or if by transitivity there is a useless join, it’s removed.\n- **Constant arithmetic evaluation**: If you write something that requires a calculus, then it’s computed once during the rewriting. For example WHERE AGE > 10+2 is transformed into WHERE AGE > 12 and TODATE(“some date”) is transformed into the date in the datetime format\n- **(Advanced) Partition Pruning**: If you’re using a partitioned table, the rewriter is able to find what partitions to use.\n- **(Advanced) Materialized view rewrite**: If you have a materialized view that matches a subset of the predicates in your query, the rewriter checks if the view is up to date and modifies the query to use the materialized view instead of the raw tables.\n- **(Advanced) Custom rules**: If you have custom rules to modify a query (like Oracle policies), then the rewriter executes these rules\n- **(Advanced) Olap transformations**: analytical/windowing functions, star joins, rollup … are also transformed (but I’m not sure if it’s done by the rewriter or the optimizer, since both processes are very close it must depends on the database).\n\nThis rewritten query is then sent to the query optimizer where the fun begins!\n\n### Statistics\n\nBefore we see how a database optimizes a query we need to speak about **statistics** because **without them a database is stupid**. If you don’t tell the database to analyze its own data, it will not do it and it will make (very) bad assumptions.\n\nBut what kind of information does a database need?\n\nI have to (briefly) talk about how databases and Operating systems store data. They’re using a minimum unit called **a page** or a block (4 or 8 kilobytes by default). This means that if you only need 1 Kbytes it will cost you one page anyway. If the page takes 8 Kbytes then you’ll waste 7 Kbytes.\n\n \n\nBack to the statistics! When you ask a database to gather statistics, it computes values like:\n\n- The number of rows/pages in a table\n- For each column in a table:\n   - distinct data values\n   - the length of data values (min, max, average)\n   - data range information (min, max, average)\n- Information on the indexes of the table.\n\n**These statistics will help the optimizer to estimate the disk I/O, CPU and memory usages of the query.**\n\nThe statistics for each column are very important. For example if a table PERSON needs to be joined on 2 columns: LAST_NAME, FIRST_NAME. With the statistics, the database knows that there are only 1 000 different values on FIRST_NAME and 1 000 000 different values on LAST_NAME. Therefore, the database will join the data on LAST_NAME, FIRST_NAME instead of FIRST_NAME,LAST_NAME because it produces way less comparisons since the LAST_NAME are unlikely to be the same so most of the time a comparison on the 2 (or 3) first characters of the LAST_NAME is enough.\n\nBut these are basic statistics. You can ask a database to compute advanced statistics called **histograms**.  Histograms are statistics that inform about the distribution of the values inside the columns. For example\n\n- the most frequent values\n- the quantiles\n- …\n\nThese extra statistics will help the database to find an even better query plan. Especially for equality predicate (ex: WHERE AGE = 18 ) or range predicates (ex: WHERE  AGE > 10 and AGE <40 ) because the database will have a better idea of the number rows concerned by these predicates (note: the technical word for this concept is selectivity).\n\nThe statistics are stored in the metadata of the database. For example you can see the statistics for the (non-partitioned) tables:\n\n- in USER/ALL/DBA_TABLES and USER/ALL/DBA_TAB_COLUMNS for Oracle\n- in SYSCAT.TABLES and SYSCAT.COLUMNS for DB2.\n\nThe **statistics have to be up to date**. There is nothing worse than a database thinking a table has only 500 rows whereas it has 1 000 000 rows. The only drawback of the statistics is that **it takes time to compute them**. This is why they’re not automatically computed by default in most databases. It becomes difficult with millions of data to compute them. In this case, you can choose to compute only the basics statistics or to compute the stats on a sample of the database.\n\nFor example, when I was working on a project dealing with hundreds of millions rows in each tables, I chose to compute the statistics on only 10%, which led to a huge gain in time. For the story it turned out to be a bad decision because occasionally the 10% chosen by Oracle 10G for a specific column of a specific table were very different from the overall 100% (which is very unlikely to happen for a table with 100M rows). This wrong statistic led to a query taking occasionally 8 hours instead of 30 seconds; a nightmare to find the root cause. This example shows how important the statistics are.\n\nNote: Of course, there are more advanced statistics specific for each database. If you want to know more, read the documentations of the databases. That being said, I’ve tried to understand how the statistics are used and the best official documentation I found was the [one from PostgreSQL](https://www.postgresql.org/docs/9.4/static/row-estimation-examples.html).\n\n### Query optimizer\n\n![image.png](https://i.loli.net/2020/12/09/lC8BeAgt7Uv5MsD.png)\n\nAll modern databases are using a **Cost Based Optimization** (or **CBO**) to optimize queries. The idea is to put a cost an every operation and find the best way to reduce the cost of the query by using the cheapest chain of operations to get the result.\n\nTo understand how a cost optimizer works I think it’s good to have an example to “feel” the complexity behind this task. In this part I’ll present you the 3 common ways to join 2 tables and we will quickly see that even a simple join query is a nightmare to optimize. After that, we’ll see how real optimizers do this job.\n\nFor these joins, I’ll focus on their time complexity but **a database optimizer computes** their **CPU cost, disk I/O cost and memory requirement**. The difference between time complexity and CPU cost is that time cost is very approximate (it’s for lazy guys like me). For the CPU cost, I should count every operation like an addition, an “if statement”, a multiplication, an iteration … Moreover:\n\n- Each high level code operation has a specific number of low level CPU operations.\n- The cost of a CPU operation is not the same (in terms of CPU cycles) whether you’re using an Intel Core i7, an Intel Pentium 4, an AMD Opteron…. In other words it depends on the CPU architecture.\n\nUsing the time complexity is easier (at least for me) and with it we can still get the concept of CBO. I’ll sometimes speak about disk I/O since it’s an important concept. Keep in mind that **the bottleneck is most of the time the disk I/O and not the CPU usage**.\n\n#### Indexes\n\nWe talked about indexes when we saw the B+Trees. Just remember that these **indexes are already sorted**.\n\nFYI, there are other types of indexes like **bitmap indexes**. They don’t offer the same cost in terms of CPU, disk I/O and memory than B+Tree indexes.\n\nMoreover, many modern databases can **dynamically create temporary indexes** just for the current query if it can improve the cost of the execution plan.\n\n#### Access Path\n\nBefore applying your join operators, you first need to get your data. Here is how you can get your data.\n\nNote: Since the real problem with all the access paths is the disk I/O, I won’t talk a lot about time complexity.\n\n##### Full scan\n\nIf you’ve ever read an execution plan you must have seen the word **full scan** (or just scan). A full scan is simply the database reading a table or an index entirely. **In terms of disk I/O, a table full scan is obviously more expensive than an index full scan**.\n\n##### Range Scan\n\nThere are other types of scan like **index range scan**. It is used for example when you use a predicate like “WHERE AGE > 20 AND AGE <40”.\n\nOf course you need have an index on the field AGE to use this index range scan.\n\nWe already saw in the first part that the time cost of a range query is something like log(N) +M, where N is the number of data in this index and M an estimation of the number of rows inside this range. **Both N and M values are known thanks to the statistics** (Note: M is the selectivity for the predicate AGE >20 AND AGE<40). Moreover, for a range scan you don’t need to read the full index so it’s **less expensive in terms of disk I/O than a full scan**.\n\n##### Unique scan\n\nIf you only need one value from an index you can use the **unique scan**.\n\n##### Access by row id\n\nMost of the time, if the database uses an index, it will have to look for the rows associated to the index. To do so it will use an access by row id.\n\nFor example, if you do something like\n\n```sql\nSELECT LASTNAME, FIRSTNAME from PERSON WHERE AGE = 28\n```\n\nIf you have an index for person on column age, the optimizer will use the index to find all the persons who are 28 then it will ask for the associate rows in the table because the index only has information about the age and you want to know the lastname and the firstname.\n\nBut, if now you do something like\n\n```sql\nSELECT TYPE_PERSON.CATEGORY from PERSON ,TYPE_PERSON\nWHERE PERSON.AGE = TYPE_PERSON.AGE\n```\n\nThe index on PERSON will be used to join with TYPE_PERSON but the table PERSON will not be accessed by row id since you’re not asking information on this table.\n\nThough it works great for a few accesses, the real issue with this operation is the disk I/O. If you need too many accesses by row id the database might choose a full scan.\n\n##### Others paths\n\nI didn’t present all the access paths. If you want to know more, you can read the [Oracle documentation](https://docs.oracle.com/database/121/TGSQL/tgsql_optop.htm). The names might not be the same for the other databases but the concepts behind are the same.\n\n#### Join operators\n\nSo, we know how to get our data, let’s join them!\n\nI’ll present the 3 common join operators: Merge Join, Hash Join and Nested Loop Join. But before that, I need to introduce new vocabulary: **inner relation** and **outer relation**. A relation can be:\n\n- a table\n- an index\n- an intermediate result from a previous operation (for example the result of a previous join)\n\nWhen you’re joining two relations, the join algorithms manage the two relations differently. In the rest of the article, I’ll assume that:\n\n- the outer relation is the left data set\n- the inner relation is the right data set\n\nFor example, A JOIN B is the join between A and B where A is the outer relation and B the inner relation.\n\nMost of the time, **the cost of A JOIN B is not the same as the cost of B JOIN A**.\n\n**In this part, I’ll also assume that the outer relation has N elements and the inner relation M elements**. Keep in mind that a real optimizer knows the values of N and M with the statistics.\n\nNote: N and M are the cardinalities of the relations.\n\n##### Nested loop join\n\nThe nested loop join is the easiest one.\n\n![image.png](https://i.loli.net/2020/12/10/R3bJ4UjLBO1YZ79.png)\n\nHere is the idea:\n\n- for each row in the outer relation\n- you look at all the rows in the inner relation to see if there are rows that match\n\nHere is a pseudo code:\n\n```cpp\nnested_loop_join(array outer, array inner)\n  for each row a in outer\n    for each row b in inner\n      if (match_join_condition(a,b))\n        write_result_in_output(a,b)\n      end if\n    end for\n   end for\n```\n\nSince it’s a double iteration, **the time complexity is O(N*M)**\n\nIn term of disk I/O, for each of the N rows in the outer relation, the inner loop needs to read M rows from the inner relation. This algorithm needs to read N + N*M rows from disk. But, if the inner relation is small enough, you can put the relation in memory and just have M +N reads. With this modification, **the inner relation must be the smallest one** since it has more chance to fit in memory.\n\nIn terms of time complexity it makes no difference but in terms of disk I/O it’s way better to read only once both relations.      \n\nOf course, the inner relation can be replaced by an index, it will be better for the disk I/O.\n\nSince this algorithm is very simple, here is another version that is more disk I/O friendly if the inner relation is too big to fit in memory. Here is the idea:\n\n- instead of reading both relation row by row,\n- you read them bunch by bunch and keep 2 bunches of rows (from each relation) in memory,\n- you compare the rows inside the two bunches and keep the rows that match,\n- then you load new bunches from disk and compare them\n- and so on until there are no bunches to load.\n\nHere is a possible algorithm:\n\n```cpp\n// improved version to reduce the disk I/O.\nnested_loop_join_v2(file outer, file inner)\n  for each bunch ba in outer\n  // ba is now in memory\n    for each bunch bb in inner\n        // bb is now in memory\n        for each row a in ba\n          for each row b in bb\n            if (match_join_condition(a,b))\n              write_result_in_output(a,b)\n            end if\n          end for\n       end for\n    end for\n   end for\n```\n\n**With this version, the time complexity remains the same, but the number of disk access decreases:**\n\n- With the previous version, the algorithm needs N + N*M accesses (each access gets one row).\n- With this new version, the number of disk accesses becomes number_of_bunches_for(outer)+ number_of_ bunches_for(outer)* number_of_ bunches_for(inner).\n- If you increase the size of the bunch you reduce the number of disk accesses.\n\nNote: Each disk access gathers more data than the previous algorithm but it doesn’t matter since they’re sequential accesses (the real issue with mechanical disks is the time to get the first data).\n\n##### Hash join\n\nThe hash join is more complicated but gives a better cost than a nested loop join in many situations.\n\n![image.png](https://i.loli.net/2020/12/10/QiB9u4jNL65XsYq.png)\n\nThe idea of the hash join is to:\n\n- 1) Get all elements from the inner relation\n- 2) Build an in-memory hash table\n- 3) Get all elements of the outer relation one by one\n- 4) Compute the hash of each element (with the hash function of the hash table) to find the associated bucket of the inner relation\n- 5) find if there is a match between the elements in the bucket and the element of the outer table\n\nIn terms of time complexity I need to make some assumptions to simplify the problem:\n\n- The inner relation is divided into X buckets\n- The hash function distributes hash values almost uniformly for both relations. In other words the buckets are equally sized.\n- The matching between an element of the outer relation and all elements inside a bucket costs the number of elements inside the buckets.\n\nThe time complexity is (M/X) * N + cost_to_create_hash_table(M) + cost_of_hash_function*N\n\nIf the Hash function creates enough small-sized buckets then **the time complexity is O(M+N)**\n\nHere is another version of the hash join which is more memory friendly but less disk I/O friendly. This time:\n\n- 1) you compute the hash tables for both the inner and outer relations\n- 2) then you put them on disk\n- 3) then you compare the 2 relations bucket by bucket (with one loaded in-memory and the other read row by row)\n\n##### Merge join\n\n**The merge join is the only join that produces a sorted result**.\n\nNote: In this simplified merge join, there are no inner or outer tables; they both play the same role. But real implementations make a difference, for example, when dealing with duplicates.\n\nThe merge join can be divided into of two steps:\n\n1. (Optional) Sort join operations: Both the inputs are sorted on the join key(s).\n2. Merge join operation: The sorted inputs are merged together.\n\n<u>Sort</u>\n\nWe already spoke about the merge sort, in this case a merge sort in a good algorithm (but not the best if memory is not an issue).\n\nBut sometimes the data sets are already sorted, for example:\n\n- If the table is natively ordered, for example an index-organized table on the join condition\n- If the relation is an index on the join condition\n- If this join is applied on an intermediate result already sorted during the process of the query\n\n<u>Merge join</u>\n\n![image.png](https://i.loli.net/2020/12/10/7DCA8wpQJqg5c14.png)\n\nThis part is very similar to the merge operation of the merge sort we saw. But this time, instead of picking every element from both relations, we only pick the elements from both relations that are equals. Here is the idea:\n\n- 1) you compare both current elements in the 2 relations (current=first for the first time)\n- 2) if they’re equal, then you put both elements in the result and you go to the next element for both relations\n- 3) if not, you go to the next element for the relation with the lowest element (because the next element might match)\n- 4) and repeat 1,2,3 until you reach the last element of one of the relation.\n\nThis works because both relations are sorted and therefore you don’t need to “go back” in these relations.\n\nThis algorithm is a simplified version because it doesn’t handle the case where the same data appears multiple times in both arrays (in other words a multiple matches). The real version is more complicated “just” for this case; this is why I chose a simplified version.\n\nIf both relations are already sorted then **the time complexity is O(N+M)**\n\nIf both relations need to be sorted then the time complexity is the cost to sort both relations: **O(N\\*Log(N) + M\\*Log(M))**\n\nFor the CS geeks, here is a possible algorithm that handles the multiple matches (note: I’m not 100% sure about my algorithm):\n\n```cpp\nmergeJoin(relation a, relation b)\n  relation output\n  integer a_key:=0;\n  integer b_key:=0;\n  \n  while (a[a_key]!=null or b[b_key]!=null)\n    if (a[a_key] < b[b_key])\n      a_key++;\n    else if (a[a_key] > b[b_key])\n      b_key++;\n    else //Join predicate satisfied\n    //i.e. a[a_key] == b[b_key]\n \n      //count the number of duplicates in relation a\n      integer nb_dup_in_a = 1:\n      while (a[a_key]==a[a_key+nb_dup_in_a])\n        nb_dup_in_a++;\n         \n      //count the number of duplicates in relation b\n      integer dup_in_b = 1:\n      while (b[b_key]==b[b_key+nb_dup_in_b])\n        nb_dup_in_b++;\n         \n      //write the duplicates in output\n       for (int i = 0 ; i< nb_dup_in_a ; i++)\n         for (int j = 0 ; i< nb_dup_in_b ; i++)     \n           write_result_in_output(a[a_key+i],b[b_key+j])\n            \n      a_key=a_key + nb_dup_in_a-1;\n      b_key=b_key + nb_dup_in_b-1;\n \n    end if\n  end while\n```\n\n##### Which one is the best?\n\nIf there was a best type of joins, there wouldn’t be multiple types. This question is very difficult because many factors come into play like:\n\n- The **amount of free memory**: without enough memory you can say goodbye to the powerful hash join (at least the full in-memory hash join)\n- The **size of the 2 data sets**. For example if you have a big table with a very small one, a nested loop join will be faster than a hash join because the hash join has an expensive creation of hashes. If you have 2 very large tables the nested loop join will be very CPU expensive.\n- The **presence of indexes**. With 2 B+Tree indexes the smart choice seems to be the merge join\n- If **the result need to be sorted**: Even if you’re working with unsorted data sets, you might want to use a costly merge join (with the sorts) because at the end the result will be sorted and you’ll be able to chain the result with another merge join (or maybe because the query asks implicitly/explicitly for a sorted result with an ORDER BY/GROUP BY/DISTINCT operation)\n- If **the relations are already sorted**: In this case the merge join is the best candidate\n- The type of joins you’re doing: is it an **equijoin** (i.e.: tableA.col1 = tableB.col2)? Is it an **inner join**, an **outer join**, a **cartesian product** or a **self-join**? Some joins can’t work in certain situations.\n- The **distribution of data**. If the data on the join condition are **skewed** (For example you’re joining people on their last name but many people have the same), using a hash join will be a disaster because the hash function will create ill-distributed buckets.\nIf you want the join to be executed by **multiple threads/process**\n\nFor more information, you can read the [DB2](https://www-01.ibm.com/support/knowledgecenter/SSEPGG_9.7.0/com.ibm.db2.luw.admin.perf.doc/doc/c0005311.html), [ORACLE](https://docs.oracle.com/cd/B28359_01/server.111/b28274/optimops.htm#i76330) or [SQL Server](https://technet.microsoft.com/en-us/library/ms191426(v=sql.105).aspx) documentations.\n\n#### Simplified example\n\nWe’ve just seen 3 types of join operations.\n\nNow let’s say we need to join 5 tables to have a full view of a person. A PERSON can have:\n\n- multiple MOBILES\n- multiple MAILS\n- multiple ADRESSES\n- multiple BANK_ACCOUNTS\n\nIn other words we need a quick answer for the following query:\n\n```sql\nSELECT * from PERSON, MOBILES, MAILS,ADRESSES, BANK_ACCOUNTS\nWHERE\nPERSON.PERSON_ID = MOBILES.PERSON_ID\nAND PERSON.PERSON_ID = MAILS.PERSON_ID\nAND PERSON.PERSON_ID = ADRESSES.PERSON_ID\nAND PERSON.PERSON_ID = BANK_ACCOUNTS.PERSON_ID\n```\n\nAs a query optimizer, I have to find the best way to process the data. But there are 2 problems:\n\n- What kind of join should I use for each join?\n\nI have 3 possible joins (Hash Join, Merge Join, Nested Join) with the possibility to use 0,1 or 2 indexes (not to mention that there are different types of indexes).\n\n- What order should I choose to compute the join?\n\nFor example, the following figure shows different possible plans for only 3 joins on 4 tables\n\n![image.png](https://i.loli.net/2020/12/12/HnoJvTiW4paBGRm.png)\n\nSo here are my possibilities:\n\n- 1) I use a brute force approach\nUsing the database statistics, I **compute the cost for every possible plan** and I keep the best one. But there are many possibilities. For a given order of joins, each join has 3 possibilities: HashJoin, MergeJoin, NestedJoin. So, for a given order of joins there are $3^4$ possibilities. The join ordering is a permutation problem on a binary tree and there are (2\\*4)!/(4+1)! possible orders. For this very simplified problem, I end up with $3^4$\\*(2\\*4)!/(4+1)! possibilities.\n\nIn non-geek terms, it means 27 216 possible plans. If I now add the possibility for the merge join to take 0,1 or 2 B+Tree indexes, the number of possible plans becomes 210 000. Did I forget to mention that this query is VERY SIMPLE?\n\n- 2) I cry and quit this job\n\nIt’s very tempting but you wouldn’t get your result and I need money to pay the bills.\n\n- 3) I only try a few plans and take the one with the lowest cost.\n\nSince I’m not superman, I can’t compute the cost of every plan. Instead, I can **arbitrary choose a subset of all the possible plans**, compute their costs and give you the best plan of this subset.\n\n- 4) I apply smart **rules to reduce the number of possible plans**.\n\nThere are 2 types of rules:\n\nI can use “logical” rules that will remove useless possibilities but they won’t filter a lot of possible plans. For example: “the inner relation of the nested loop join must be the smallest data set”\n\nI accept not finding the best solution and apply more aggressive rules to reduce a lot the number of possibilities. For example “If a relation is small, use a nested loop join and never use a merge join or a hash join” \n\nIn this simple example, I end up with many possibilities. But **a real query can have other relational operators** like OUTER JOIN, CROSS JOIN, GROUP BY, ORDER BY, PROJECTION, UNION, INTERSECT, DISTINCT … **which means even more possibilities**.\n\nSo, how a database does it?\n\n#### Dynamic programming, greedy algorithm and heuristic\n\nA relational database tries the multiple approaches I’ve just said. The real job of an optimizer is to find a good solution on a limited amount of time.\n\n**Most of the time an optimizer doesn’t find the best solution but a “good” one**.\n\nFor small queries, doing a brute force approach is possible. But there is a way to avoid unnecessary computations so that even medium queries can use the brute force approach. This is called dynamic programming.\n\n##### Dynamic Programming\n\nThe idea behind these 2 words is that many executions plan are very similar. If you look at the following plans:\n\n![image.png](https://i.loli.net/2020/12/12/JTYGRpkFe8Icmoy.png)\n\nThey share the same (A JOIN B) subtree. So, instead of computing the cost of this subtree in every plan, we can compute it once, save the computed cost and reuse it when we see this subtree again. More formally, we’re facing an overlapping problem. To avoid the extra-computation of the partial results we’re using memoization.\n\nUsing this technique, instead of having a (2\\*N)!/(N+1)! time complexity, we “just” have $3^N$. In our previous example with 4 joins, it means passing from 336 ordering to 81. If you take a bigger **query with 8 joins** (which is not big), **it means passing from 57 657 600 to 6561**.\n\nFor the CS geeks, here is an algorithm I found on the [formal course I already gave you](http://codex.cs.yale.edu/avi/db-book/db6/slide-dir/PPT-dir/ch13.ppt). I won’t explain this algorithm so read it only if you already know dynamic programming or if you’re good with algorithms (you’ve been warned!):\n\n```cpp\nprocedure findbestplan(S)\nif (bestplan[S].cost infinite)\n   return bestplan[S]\n// else bestplan[S] has not been computed earlier, compute it now\nif (S contains only 1 relation)\n         set bestplan[S].plan and bestplan[S].cost based on the best way\n         of accessing S  /* Using selections on S and indices on S */\n     else for each non-empty subset S1 of S such that S1 != S\n   P1= findbestplan(S1)\n   P2= findbestplan(S - S1)\n   A = best algorithm for joining results of P1 and P2\n   cost = P1.cost + P2.cost + cost of A\n   if cost < bestplan[S].cost\n       bestplan[S].cost = cost\n      bestplan[S].plan = “execute P1.plan; execute P2.plan;\n                 join results of P1 and P2 using A”\nreturn bestplan[S]\n```\n\nFor bigger queries you can still do a dynamic programming approach but with extra rules (or **heuristics**) to remove possibilities:\n\n- If we analyze only a certain type of plan (for example: the left-deep trees) we end up with $n*2^n$ instead of $3^n$\n\n![image.png](https://i.loli.net/2020/12/12/JTYGRpkFe8Icmoy.png)\n\n- If we add logical rules to avoid plans for some patterns (like “if a table as an index for the given predicate, don’t try a merge join on the table but only on the index”) it will reduce the number of possibilities without hurting to much the best possible solution.\n- If we add rules on the flow (like “perform the join operations BEFORE all the other relational operations”) it also reduces a lot of possibilities.\n- …\n\n##### Greedy algorithms\n\nBut for a very big query or to have a very fast answer (but not a very fast query), another type of algorithms is used, the greedy algorithms.\n\nThe idea is to follow a rule (or **heuristic**) to build a query plan in an incremental way. With this rule, a greedy algorithm finds the best solution to a problem one step at a time.  The algorithm starts the query plan with one JOIN. Then, at each step, the algorithm adds a new JOIN to the query plan using the same rule.\n\nLet’s take a simple example. Let’s say we have a query with 4 joins on 5 tables (A, B, C, D and E). To simplify the problem we just take the nested join as a possible join. Let’s use the rule “use the join with the lowest cost”\n\n- we arbitrary start on one of the 5 tables (let’s choose A)\n- we compute the cost of every join with A (A being the inner or outer relation).\n- we find that A JOIN B gives the lowest cost.\n- we then compute the cost of every join with the result of A JOIN B (A JOIN B being the inner or outer relation).\n- we find that (A JOIN B) JOIN C gives the best cost.\n- we then compute the cost of every join with the result of the (A JOIN B) JOIN C …\n- ….\n- At the end we find the plan (((A JOIN B) JOIN C) JOIN D) JOIN E)\n\nSince we arbitrary started with A, we can apply the same algorithm for B, then C then D then E. We then keep the plan with the lowest cost.\n\nBy the way, this algorithm has a name: it’s called the [Nearest neighbor algorithm](https://en.wikipedia.org/wiki/Nearest_neighbour_algorithm).\n\nI won’t go into details, but with a good modeling and a sort in N\\*log(N) this problem can [easily be solved](https://www.google.fr/url?sa=t&rct=j&q=&esrc=s&source=web&cd=5&cad=rja&uact=8&ved=0CE0QFjAEahUKEwjR8OLUmv3GAhUJuxQKHdU-DAA&url=http%3A%2F%2Fwww.cs.bu.edu%2F~steng%2Fteaching%2FSpring2004%2Flectures%2Flecture3.ppt&ei=hyK3VZGRAYn2UtX9MA&usg=AFQjCNGL41kMNkG5cH). The **cost of this algorithm is in O(N\\*log(N)) vs O($3^N$) for the full dynamic programming version**. If you have a big query with 20 joins, it means 26 vs 3 486 784 401, a BIG difference!\n\nThe problem with this algorithm is that we assume that finding the best join between 2 tables will give us the best cost if we keep this join and add a new join.  But:\n\n- even if A JOIN B gives the best cost between A, B and C\n- (A JOIN C) JOIN B might give a better result than (A JOIN B) JOIN C.\n\nTo improve the result, you can run multiple greedy algorithms using different rules and keep the best plan.\n\n##### Other algorithms\n\n[If you’re already fed up with algorithms, skip to the next part, what I’m going to say is not important for the rest of the article]\n\nThe problem of finding the best possible plan is an active research topic for many CS researchers. They often try to find better solutions for more precise problems/patterns. For example,\n\n- if the query is a star join (it’s a certain type of multiple-join query), some databases will use a specific algorithm.\n- if the query is a parallel query, some databases will use a specific algorithm\n- …\n\nOther algorithms are also studied to replace dynamic programming for large queries. Greedy algorithms belong to larger family called **heuristic algorithms**. A greedy algorithm follows a rule (or heuristic), keeps the solution it found at the previous step and “appends” it to find the solution for the current step. Some algorithms follow a rule and apply it in a step-by-step way but don’t always keep the best solution found in the previous step. They are called heuristic algorithms.\n\nFor example, **genetic algorithms** follow a rule but the best solution of the last step is not often kept:\n\n- A solution represents a possible full query plan\n- Instead of one solution (i.e. plan) there are P solutions (i.e. plans) kept at each step.\n- 0) P query plans are randomly created\n- 1) Only the plans with the best costs are kept\n- 2) These best plans are mixed up to produce P news plans\n- 3) Some of the P new plans are randomly modified\n- 4) The step 1,2,3 are repeated T times\n- 5) Then you keep the best plan from the P plans of the last loop.\n\nThe more loops you do the better the plan will be.\n\nIs it magic? No, it’s the laws of nature: only the fittest survives!\n\nFYI, genetic algorithms are implemented in PostgreSQL but I wasn’t able to find if they’re used by default.\n\nThere are other heuristic algorithms used in databases like Simulated Annealing, Iterative Improvement, Two-Phase Optimization… But I don’t know if they’re currently used in enterprise databases or if they’re only used in research databases.\n\nFor more information, you can read the following research article that presents more possible algorithms: [Review of Algorithms for the Join Ordering Problem in Database Query Optimization](http://www.acad.bg/rismim/itc/sub/archiv/Paper6_1_2009.PDF)\n\n#### Real optimizers\n\n[You can skip to the next part, what I’m going to say is not important]\n\nBut, all this blabla is very theoretical. Since I’m a developer and not a researcher, I like **concrete examples**.\n\nLet’s see how the [SQLite optimizer](https://www.sqlite.org/optoverview.html) works. It’s a light database so it uses a simple optimization based on a greedy algorithm with extra-rules to limit the number of possibilities:\n\n- SQLite chooses to never reorder tables in a CROSS JOIN operator\n-** joins are implemented as nested joins**\n- outer joins are always evaluated in the order in which they occur\n- …\n- Prior to version 3.8.0, **SQLite uses the “Nearest Neighbor” greedy algorithm when searching for the best query plan**\n\nWait a minute … we’ve already seen this algorithm! What a coincidence!\n\n- Since version 3.8.0 (released in 2015), SQLite uses the “[N Nearest Neighbors](https://www.sqlite.org/queryplanner-ng.html)” **greedy algorithm** when searching for the best query plan\n\nLet’s see how another optimizer does his job. IBM DB2 is like all the enterprise databases but I’ll focus on this one since it’s the last one I’ve really used before switching to Big Data.\n\nIf we look at the [official documentation](https://www-01.ibm.com/support/knowledgecenter/SSEPGG_9.7.0/com.ibm.db2.luw.admin.perf.doc/doc/r0005278.html), we learn that the DB2 optimizer let you use 7 different levels of optimization:\n\n- Use greedy algorithms for the joins\n  - 0 – minimal optimization, use index scan and nested-loop join and avoid some Query Rewrite\n  - 1 – low optimization\n  - 2 – full optimization\n- Use dynamic programming for the joins\n  - 3 – moderate optimization and rough approximation\n  - 5 – full optimization, uses all techniques with heuristics\n  - 7 – full optimization similar to 5, without heuristics\n  - 9 – maximal optimization spare no effort/expense **considers all possible join orders, including Cartesian products**\n\nWe can see that **DB2 uses greedy algorithms and dynamic programming**. Of course, they don’t share the heuristics they use since the query optimizer is the main power of a database.\n\nFYI, **the default level is 5**. By default the optimizer uses the following characteristics:\n\n- **All available statistics**, including frequent-value and quantile statistics, are used.\n- **All query rewrite rules** (including materialized query table routing) are applied, except computationally intensive rules that are applicable only in very rare cases.\n- **Dynamic programming join enumeration** is used, with:\n  - Limited use of composite inner relation\n  - Limited use of Cartesian products for star schemas involving lookup tables\n- A wide range of access methods is considered, including list prefetch (note: will see what is means), index ANDing (note: a special operation with indexes), and materialized query table routing.\n\nBy default, **DB2 uses dynamic programming limited by heuristics for the join ordering**.\n\nThe others conditions (GROUP BY, DISTINCT…) are handled by simple rules.\n\n#### Query Plan Cache\n\nSince the creation of a plan takes time, most databases store the plan into a **query plan cache** to avoid useless re-computations of the same query plan. It’s kind of a big topic since the database needs to know when to update the outdated plans. The idea is to put a threshold and if the statistics of a table have changed above this threshold then the query plan involving this table is purged from the cache.\n\n### Query executor\nAt this stage we have an optimized execution plan. This plan is compiled to become an executable code. Then, if there are enough resources (memory, CPU) it is executed by the query executor. The operators in the plan (JOIN, SORT BY …) can be executed in a sequential or parallel way; it’s up to the executor. To get and write its data, the query executor interacts with the data manager, which is the next part of the article.\n\n## Data manager\n\n![image.png](https://i.loli.net/2020/12/13/Luyz183RltwWqUe.png)\n\nAt this step, the query manager is executing the query and needs the data from the tables and indexes. It asks the data manager to get the data, but there are 2 problems:\n\n- Relational databases use a transactional model. So, you can’t get any data at any time because someone else might be using/modifying the data at the same time.\n- **Data retrieval is the slowest operation in a database**, therefore the data manager needs to be smart enough to get and keep data in memory buffers.\n\nIn this part, we’ll see how relational databases handle these 2 problems. I won’t talk about the way the data manager gets its data because it’s not the most important (and this article is long enough!).\n\n### Cache manager\n\nAs I already said, the main bottleneck of databases is disk I/O. To improve performance, modern databases use a cache manager.\n\nInstead of directly getting the data from the file system, the query executor asks for the data to the cache manager. The cache manager has an in-memory cache called **buffer pool**. **Getting data from memory dramatically speeds up a database**. It’s difficult to give an order of magnitude because it depends on the operation you need to do:\n\n- sequential access (ex: full scan) vs random access (ex: access by row id),\n- read vs write\n\nand the type of disks used by the database:\n\n- 7.2k/10k/15k rpm HDD\n- SSD\n- RAID 1/5/…\n\nbut I’d say **memory is 100 to 100k times faster than disk**.\n\nBut, this leads to another problem (as always with databases…). The cache manager needs to get the data in memory BEFORE the query executor uses them; otherwise the query manager has to wait for the data from the slow disks.\n\n#### Prefetching\n\nThis problem is called prefetching. A query executor knows the data it’ll need because it knows the full flow of the query and has knowledge of the data on disk with the statistics. Here is the idea:\n\n- When the query executor is processing its first bunch of data\n- It asks the cache manager to pre-load the second bunch of data\n- When it starts processing the second bunch of data\n- It asks the CM to pre-load the third bunch and informs the CM that the first bunch can be purged from cache.\n- …\n\nThe CM stores all these data in its buffer pool. In order to know if a data is still needed, the cache manager adds an extra-information about the cached data (called a **latch**).\n\nSometimes the query executor doesn’t know what data it’ll need and some databases don’t provide this functionality. Instead, they use a speculative prefetching (for example: if the query executor asked for data 1,3,5 it’ll likely ask for 7,9,11 in a near future) or a sequential prefetching (in this case the CM simply loads from disks the next contiguous data after the ones asked).\n\nTo monitor how well the prefetching is working, modern databases provide a metric called **buffer/cache hit ratio**. The hit ratio shows how often a requested data has been found in the buffer cache without requiring disk access.\n\nNote: a poor cache hit ratio doesn’t always mean that the cache is ill-working. For more information, you can read the [Oracle documentation](https://docs.oracle.com/database/121/TGDBA/tune_buffer_cache.htm).\n\nBut, a buffer is a **limited** amount of memory. Therefore, it needs to remove some data to be able to load new ones. Loading and purging the cache has a cost in terms of disk and network I/O. If you have a query that is often executed, it wouldn’t be efficient to always load then purge the data used by this query. To handle this problem, modern databases use a buffer replacement strategy.\n\n#### Buffer-Replacement strategies\n\nMost modern databases (at least SQL Server, MySQL, Oracle and DB2) use an LRU algorithm.\n\n##### LRU\nLRU stands for Least Recently Used. The idea behind this algorithm is to keep in the cache the data that have been recently used and, therefore, are more likely to be used again.\n\nHere is a visual example:\n\n![image.png](https://i.loli.net/2020/12/13/HwZx4RnveKTENrb.png)\n\nFor the sake of comprehension, I’ll assume that the data in the buffer are not locked by latches (and therefore can be removed). In this simple example the buffer can store 3 elements:\n\n- 1: the cache manager uses the data 1 and puts the data into the empty buffer\n- 2: the CM uses the data 4 and puts the data into the half-loaded buffer\n- 3: the CM uses the data 3 and puts the data into the half-loaded buffer\n- 4: the CM uses the data 9. The buffer is full so **data 1 is removed since it’s the last recently used data**. Data 9 is added into the buffer\n- 5: the CM uses the data 4. **Data 4 is already in the buffer therefore it becomes the first recently used data again**.\n- 6: the CM uses the data 1. The buffer is full so **data 9 is removed since it’s the last recently used data**. Data 1 is added into the buffer\n- …\nThis algorithm works well but there are some limitations. What if there is a full scan on a large table? In other words, what happens when the size of the table/index is above the size of the buffer? Using this algorithm will remove all the previous values in the cache whereas the data from the full scan are likely to be used only once.\n\n##### Improvements\n\nTo prevent this to happen, some databases add specific rules. For example according to [Oracle documentation](https://docs.oracle.com/database/121/CNCPT/memory.htm#i10221):\n\n> “For very large tables, the database typically uses a direct path read, which loads blocks directly […], to avoid populating the buffer cache. For medium size tables, the database may use a direct read or a cache read. If it decides to use a cache read, then the database places the blocks at the end of the LRU list to prevent the scan from effectively cleaning out the buffer cache.”\n\nThere are other possibilities like using an advanced version of LRU called LRU-K. For example SQL Server uses LRU-K for K =2.\n\nThis idea behind this algorithm is to take into account more history. With the simple LRU (which is also LRU-K for K=1), the algorithm only takes into account the last time the data was used. With the LRU-K:\n\n- It takes into account the **K last times the data was used**.\n- **A weight is put** on the number of times the data was used\n- If a bunch of new data is loaded into the cache, the old but often used data are not removed (because their weights are higher).\n- But the algorithm can’t keep old data in the cache if they aren’t used anymore.\n- So the **weights decrease over time if the data is not used**.\n\nThe computation of the weight is costly and this is why SQL Server only uses K=2. This value performs well for an acceptable overhead.\n\nFor a more in-depth knowledge of LRU-K, you can read the original research paper (1993): [The LRU-K page replacement algorithm for database disk buffering](https://www.cs.cmu.edu/~christos/courses/721-resources/p297-o_neil.pdf).\n\n##### Other algorithms\n\nOf course there are other algorithms to manage cache like\n\n- 2Q (a LRU-K like algorithm)\n- CLOCK (a LRU-K like algorithm)\n- MRU (most recently used, uses the same logic than LRU but with another rule)\n- LRFU (Least Recently and Frequently Used)\n- …\n\nSome databases let the possibility to use another algorithm than the default one.\n\n#### Write buffer\n\nI only talked about read buffers that load data before using them. But in a database you also have write buffers that store data and flush them on disk by bunches instead of writing data one by one and producing many single disk accesses.\n\nKeep in mind that buffers store **pages** (the smallest unit of data) and not rows (which is a logical/human way to see data). A page in a buffer pool is **dirty** if the page has been modified and not written on disk. There are multiple algorithms to decide the best time to write the dirty pages on disk but it’s highly linked to the notion of transaction, which is the next part of the article.\n\n### Transaction manager\n\nLast but not least, this part is about the transaction manager. We’ll see how this process ensures that each query is executed in its own transaction. But before that, we need to understand the concept of ACID transactions.\n\n#### I’m on acid\n\nAn ACID transaction is a **unit of work** that ensures 4 things:\n\n- **Atomicity**: the transaction is “all or nothing”, even if it lasts 10 hours. If the transaction crashes, the state goes back to before the transaction (the transaction is **rolled back**).\n- **Isolation**: if 2 transactions A and B run at the same time, the result of transactions A and B must be the same whether A finishes before/after/during transaction B.\n- **Durability**: once the transaction is **committed** (i.e. ends successfully), the data stay in the database no matter what happens (crash or error).\n- **Consistency**: only valid data (in terms of relational constraints and functional constraints) are written to the database. The consistency is related to atomicity and isolation.\n\n![image.png](https://i.loli.net/2020/12/13/7UPXry9qjCTJQDn.png)\n\nDuring the same transaction, you can run multiple SQL queries to read, create, update and delete data. The mess begins when two transactions are using the same data. The classic example is a money transfer from an account A to an account B.  Imagine you have 2 transactions:\n\n- Transaction 1 that takes 100$ from account A and gives them to account B\n- Transaction 2 that takes 50$ from account A and gives them to account B\n\nIf we go back to the **ACID** properties:\n\n- **Atomicity** ensures that no matter what happens during T1 (a server crash, a network failure …), you can’t end up in a situation where the 100$ are withdrawn from A and not given to B (this case is an inconsistent state).\n- **Isolation** ensures that if T1 and T2 happen at the same time, in the end A will be taken 150$ and B given 150$ and not, for example, A taken 150$ and B given just $50 because T2 has partially erased the actions of T1 (this case is also an inconsistent state).\n- **Durability** ensures that T1 won’t disappear into thin air if the database crashes just after T1 is committed.\n- **Consistency** ensures that no money is created or destroyed in the system.\n\n[You can skip to the next part if you want, what I’m going to say is not important for the rest of the article]\n\nMany modern databases don’t use a pure isolation as a default behavior because it comes with a huge performance overhead. The SQL norm defines 4 levels of isolation:\n\n- **Serializable** (default behaviour in SQLite): The highest level of isolation. Two transactions happening at the same time are 100% isolated. Each transaction has its own “world”.\n- **Repeatable read** (default behavior in MySQL): Each transaction has its own “world” except in one situation. If a transaction ends up successfully and adds new data, these data will be visible in the other and still running transactions. But if A modifies a data and ends up successfully, the modification won’t be visible in the still running transactions. So, this break of isolation between transactions is only about new data, not the existing ones.\n\nFor example,  if a transaction A does a “SELECT count(1) from TABLE_X” and then a new data is added and committed in TABLE_X by Transaction B, if transaction A does again a count(1) the value won’t be the same.\n\nThis is called a **phantom read**.\n\n- **Read committed** (default behavior in Oracle, PostgreSQL and SQL Server): It’s a repeatable read + a new break of isolation. If a transaction A reads a data D and then this data is modified (or deleted) and committed by a transaction B, if A reads data D again it will see the modification (or deletion) made by B on the data.\n\nThis is called a **non-repeatable read**.\n\n- **Read uncommitted**: the lowest level of isolation. It’s a read committed + a new break of isolation. If a transaction A reads a data D and then this data D is modified by a transaction B (that is not committed and still running), if A reads data D again it will see the modified value. If transaction B is rolled back, then data D read by A the second time doesn’t make no sense since it has been modified by a transaction B that never happened (since it was rolled back).\n\nThis is called a **dirty read**.\n\nMost databases add their own custom levels of isolation (like the snapshot isolation used by PostgreSQL, Oracle and SQL Server). Moreover, most databases don’t implement all the levels of the SQL norm (especially the read uncommitted level).\n\nThe default level of isolation can be overridden by the user/developer at the beginning of the connection (it’s a very simple line of code to add).\n\n#### Concurrency Control\n\nThe real issue to ensure isolation, coherency and atomicity is the **write operations on the same data** (add, update and delete):\n\n- if all transactions are only reading data, they can work at the same time without modifying the behavior of another transaction.\n- if (at least) one of the transactions is modifying a data read by other transactions, the database needs to find a way to hide this modification from the other transactions. Moreover, it also needs to ensure that this modification won’t be erased by another transaction that didn’t see the modified data.\n\nThis problem is a called **concurrency control**.\n\nThe easiest way to solve this problem is to run each transaction one by one (i.e. sequentially). But that’s not scalable at all and only one core is working on the multi-processor/core server, not very efficient…\n\nThe ideal way to solve this problem is, every time a transaction is created or cancelled:\n\n- to monitor all the operations of all the transactions\n- to check if the parts of 2 (or more) transactions are in conflict because they’re reading/modifying the same data.\n- to reorder the operations inside the conflicting transactions to reduce the size of the conflicting parts\n- to execute the conflicting parts in a certain order (while the non-conflicting transactions are still running concurrently).\n- to take into account that a transaction can be cancelled.\n\nMore formally it’s a scheduling problem with conflicting schedules. More concretely, it’s a very difficult and CPU-expensive optimization problem. Enterprise databases can’t afford to wait hours to find the best schedule for each new transaction event. Therefore, they use less ideal approaches that lead to more time wasted between conflicting transactions.\n\n#### Lock manager\n\nTo handle this problem, most databases are using **locks** and/or **data versioning**. Since it’s a big topic, I’ll focus on the locking part then I’ll speak a little bit about data versioning.\n\n**Pessimistic locking**\n\nThe idea behind locking is:\n\n- if a transaction needs a data,\n- it locks the data\n- if another transaction also needs this data,\n- it’ll have to wait until the first transaction releases the data.\n\nThis is called an *exclusive lock*.\n\nBut using an exclusive lock for a transaction that only needs to read a data is very expensive since **it forces other transactions that only want to read the same data to wait**. This is why there is another type of lock, the **shared lock**.\n\nWith the shared lock:\n\n- if a transaction needs only to read a data A,\n- it “shared locks” the data and reads the data\n- if a second transaction also needs only to read data A,\n- it “shared locks” the data and reads the data\n- if a third transaction needs to modify data A,\n- it “exclusive locks” the data but it has to wait until the 2 other transactions release their shared locks to apply its exclusive lock on data A.\n\nStill, if a data as an exclusive lock, a transaction that just needs to read the data will have to wait the end of the exclusive lock to put a shared lock on the data.\n\n![image.png](https://i.loli.net/2020/12/14/LmAqOvB7UFWPeJc.png)\n\nThe lock manager is the process that gives and releases locks. Internally, it stores the locks in a hash table (where the key is the data to lock) and knows for each data:\n\n- which transactions are locking the data\n- which transactions are waiting for the data\n\n##### Deadlock\n\nBut the use of locks can lead to a situation where 2 transactions are waiting forever for a data:\n\n![image.png](https://i.loli.net/2020/12/14/5tO1APi7DlmRU6J.png)\n\nIn this figure:\n\n- transaction A has an exclusive lock on data1 and is waiting to get data2\n- transaction B has an exclusive lock on data2 and is waiting to get data1\n\nThis is called a deadlock.\n\nDuring a deadlock, the lock manager chooses which transaction to cancel (rollback) in order to remove the deadlock. This decision is not easy:\n\n- Is it better to kill the transaction that modified the least amount of data (and therefore that will produce the least expensive rollback)?\n- Is it better to kill the least aged transaction because the user of the other transaction has waited longer?\n- Is it better to kill the transaction that will take less time to finish (and avoid a possible starvation)?\n- In case of rollback, how many transactions will be impacted by this rollback?\n\nBut before making this choice, it needs to check if there are deadlocks.\n\nThe hash table can be seen as a graph (like in the previous figures). There is a deadlock if there is a cycle in the graph. Since it’s expensive to check for cycles (because the graph with all the locks is quite big), a simpler approach is often used: using a **timeout**. If a lock is not given within this timeout, the transaction enters a deadlock state.\n\nThe lock manager can also check before giving a lock if this lock will create a deadlock. But again it’s computationally expensive to do it perfectly. Therefore, these pre-checks are often a set of basic rules.\n\n##### Two-phase locking\n\nThe **simplest way** to ensure a pure isolation is if a lock is acquired at the beginning of the transaction and released at the end of the transaction. This means that a transaction has to wait for all its locks before it starts and the locks held by a transaction are released when the transaction ends. It works but it **produces a lot of time wasted** to wait for all locks.\n\nA faster way is the **Two-Phase Locking Protocol** (used by DB2 and SQL Server) where a transaction is divided into 2 phases:\n\n- the **growing phase** where a transaction can obtain locks, but can’t release any lock.\n- the **shrinking phase** where a transaction can release locks (on the data it has already processed and won’t process again), but can’t obtain new locks.\n \n![image.png](https://i.loli.net/2020/12/14/mcNudEvYJx6bBhM.png)\n\nThe idea behind these 2 simple rules is:\n\n- to release the locks that aren’t used anymore to reduce the wait time of other transactions waiting for these locks\n- to prevent from cases where a transaction gets data modified after the transaction started and therefore aren’t coherent with the first data the transaction acquired.\n\nThis protocol works well except if a transaction that modified a data and released the associated lock is cancelled (rolled back). You could end up in a case where another transaction reads the modified value whereas this value is going to be rolled back. To avoid this problem, **all the exclusive locks must be released at the end of the transaction**.\n\n##### A few words\n\nOf course a real database uses a more sophisticated system involving more types of locks (like intention locks) and more granularities (locks on a row, on a page, on a partition, on a table, on a tablespace) but the idea remains the same.\n\nI only presented the pure lock-based approach. **Data versioning is another way to deal with this problem**.\n\nThe idea behind versioning is that:\n\n- every transaction can modify the same data at the same time\n- each transaction has its own copy (or version) of the data\n- if 2 transactions modify the same data, only one modification will be accepted, the other will be refused and the associated transaction will be rolled back (and maybe re-run).\n\nIt increases the performance since:\n\n- **reader transactions don’t block writer transactions**\n- **writer transactions don’t block reader transactions**\n- there is no overhead from the “fat and slow” lock manager\n\nEverything is better than locks except when 2 transactions write the same data. Moreover, you can quickly end up with a huge disk space overhead.\n\nData versioning and locking are two different visions: **optimistic locking vs pessimistic locking**. They both have pros and cons; it really depends on the use case (more reads vs more writes). For a presentation on data versioning, I recommend [this very good presentation](http://momjian.us/main/writings/pgsql/mvcc.pdf) on how PostgreSQL implements multiversion concurrency control.\n\nSome databases like DB2 (until DB2 9.7) and SQL Server (except for snapshot isolation) are only using locks. Other like PostgreSQL, MySQL and Oracle use a mixed approach involving locks and data versioning. I’m not aware of a database using only data versioning (if you know a database based on a pure data versioning, feel free to tell me).\n\n[UPDATE 08/20/2015] I was told by a reader that:\n\n> Firebird and Interbase use versioning without record locking.\n\n> Versioning has an interesting effect on indexes: sometimes a unique index contains duplicates, the index can have more entries than the table has rows, etc.\n\nIf you read the part on the different levels of isolation, when you increase the isolation level you increase the number of locks and therefore the time wasted by transactions to wait for their locks. This is why most databases don’t use the highest isolation level (Serializable) by default.\n\nAs always, you can check by yourself in the documentation of the main databases (for example [MySQL](https://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-model.html), [PostgreSQL](https://www.postgresql.org/docs/9.4/static/mvcc.html) or [Oracle](https://docs.oracle.com/cd/B28359_01/server.111/b28318/consist.htm#i5337)).\n\n#### Log manager\n\nWe’ve already seen that to increase its performances, a database stores data in memory buffers. But if the server crashes when the transaction is being committed, you’ll lose  the data still in memory during the crash, which breaks the Durability of a transaction.\n\nYou can write everything on disk but if the server crashes, you’ll end up with the data half written on disk, which breaks the Atomicity of a transaction.\n\n**Any modification written by a transaction must be undone or finished**.\n\nTo deal with this problem, there are 2 ways:\n\n- **Shadow copies/pages**: Each transaction creates its own copy of the database (or just a part of the database) and works on this copy. In case of error, the copy is removed. In case of success, the database switches instantly the data from the copy with a filesystem trick then it removes the “old” data.\n- **Transaction log**: A transaction log is a storage space. Before each write on disk, the database writes an info on the transaction log so that in case of crash/cancel of a transaction, the database knows how to remove (or finish) the unfinished transaction.\n\n##### WAL\n\nThe shadow copies/pages creates a huge disk overhead when used on large databases involving many transactions. That’s why modern databases use a **transaction log**. The transaction log must be stored on a **stable storage**. I won’t go deeper on storage technologies but using (at least) RAID disks is mandatory to prevent from a disk failure.\n\nMost databases (at least Oracle, [SQL Server](https://technet.microsoft.com/en-us/library/ms186259(v=sql.105).aspx), [DB2](https://www.ibm.com/developerworks/data/library/techarticle/0301kline/0301kline.html), [PostgreSQL](https://www.postgresql.org/docs/9.4/static/wal.html), MySQL and [SQLite](https://www.sqlite.org/wal.html)) deal with the transaction log using the **Write-Ahead Logging protocol** (WAL). The WAL protocol is a set of 3 rules:\n\n- 1) Each modification into the database produces a log record, and **the log record must be written into the transaction log before the data is written on disk**.\n- 2) The log records must be written in order; a log record A that happens before a log record B must but written before B\n- 3) When a transaction is committed, the commit order must be written on the transaction log before the transaction ends up successfully.\n\n![image.png](https://i.loli.net/2020/12/14/IewnyAabukzmKSF.png)\n\nThis job is done by a log manager. An easy way to see it is that between the cache manager and the data access manager (that writes data on disk) the log manager writes every update/delete/create/commit/rollback on the transaction log before they’re written on disk. Easy, right?\n\nWRONG ANSWER! After all we’ve been through, you should know that everything related to a database is cursed by the “database effect”. More seriously, the problem is to find a way to write logs while keeping good performances. If the writes on the transaction log are too slow they will slow down everything.\n\n##### ARIES\n\nIn 1992, IBM researchers “invented” an enhanced version of WAL called ARIES. ARIES is more or less used by most modern databases. The logic might not be the same but the concepts behind ARIES are used everywhere. I put the quotes on invented because, according to this [MIT course](http://db.csail.mit.edu/6.830/lectures/lec15-notes.pdf), the IBM researchers did “nothing more than writing the good practices of transaction recovery”. Since I was 5 when the ARIES paper was published, I don’t care about this old gossip from bitter researchers. In fact, I only put this info to give you a break before we start this last technical part. I’ve read a huge part of the [research paper on ARIES](https://www.cs.berkeley.edu/~brewer/cs262/Aries.pdf) and I find it very interesting! In this part I’ll only give you an overview of ARIES but I strongly recommend to read the paper if you want a real knowledge.\n\nARIES stands for **A**lgorithms for **R**ecovery and **I**solation **E**xploiting **S**emantics.\n\nThe aim of this technique is double:\n\n- 1) Having **good performances when writing logs**\n- 2) Having a fast and **reliable recovery**\n\nThere are multiple reasons a database has to rollback a transaction:\n\n- Because the user cancelled it\n- Because of server or network failures\n- Because the transaction has broken the integrity of the database (for example you have a UNIQUE constraint on a column and the transaction adds a duplicate)\n- Because of deadlocks\n\nSometimes (for example, in case of network failure), the database can recover the transaction.\n\nHow is that possible? To answer this question, we need to understand the information stored in a log record.\n\n##### The logs\n\nEach **operation (add/remove/modify) during a transaction produces a log**. This log record is composed of:\n\n- **LSN**: A unique **L**og **S**equence **N**umber. This LSN is given in a chronological order*. This means that if an operation A happened before an operation B the LSN of log A will be lower than the LSN of log B.\n- **TransID**: the id of the transaction that produced the operation.\n- **PageID**: the location on disk of the modified data. The minimum amount of data on disk is a page so the location of the data is the location of the page that contains the data.\n- **PrevLSN**: A link to the previous log record produced by the same transaction.\n- **UNDO**: a way to remove the effect of the operation\n\nFor example, if the operation is an update, the UNDO will store either the value/state of the updated element before the update (physical UNDO) or the reverse operation to go back at the previous state (logical UNDO)**.\n\n- **REDO**: a way replay the operation\n\nLikewise, there are 2 ways to do that. Either you store the value/state of the element after the operation or the operation itself to replay it.\n\n- …: (FYI, an ARIES log has 2 others fields: the UndoNxtLSN and the Type).\n\nMoreover, each page on disk (that stores the data, not the log) has id of the log record (LSN) of the last operation that modified the data.\n\nThe way the LSN is given is more complicated because it is linked to the way the logs are stored. But the idea remains the same.\n\nARIES uses only logical UNDO because it’s a real mess to deal with physical UNDO.\n\nNote: From my little knowledge, only PostgreSQL is not using an UNDO. It uses instead a garbage collector daemon that removes the old versions of data. This is linked to the implementation of the data versioning in PostgreSQL.\n\nTo give you a better idea, here is a visual and simplified example of the log records produced by the query “UPDATE FROM PERSON SET AGE = 18;”. Let’s say this query is executed in transaction 18.\n\n![image.png](https://i.loli.net/2020/12/14/56SwLnH2gGyeztU.png)\n\nEach log has a unique LSN. The logs that are linked belong to the same transaction. The logs are linked in a chronological order (the last log of the linked list is the log of the last operation).\n\n## To conclude\n\nBefore writing this article, I knew how big the subject was and I knew it would take time to write an in-depth article about it. It turned out that I was very optimistic and I spent twice more time than expected, but I learned a lot.\n\nIf you want a good overview about databases, I recommend reading the research paper “[Architecture of a Database System](http://db.cs.berkeley.edu/papers/fntdb07-architecture.pdf)“. This is a good introduction on databases (110 pages) and for once it’s readable by non-CS guys. This paper helped me a lot to find a plan for this article and it’s not focused on data structures and algorithms like my article but more on the architecture concepts.\n\nIf you read this article carefully you should now understand how powerful a database is. Since it was a very long article, let me remind you about what we’ve seen:\n\n- an overview of the B+Tree indexes\n- a global overview of a database\n- an overview of the cost based optimization with a strong focus on join operators\n- an overview of the buffer pool management\n- an overview of the transaction management\n\nBut a database contains even more cleverness. For example, I didn’t speak about some touchy problems like:\n\n- how to manage clustered databases and global transactions\n- how to take a snapshot when the database is still running\n- how to efficiently store (and compress) data\n- how to manage memory\n\nSo, think twice when you have to choose between a buggy NoSQL database and a rock-solid relational database. Don’t get me wrong, some NoSQL databases are great. But they’re still young and answering specific problems that concern a few applications.\n\nTo conclude, if someone asks you how a database works, instead of running away you’ll now be able to answer:\n\n![magic_low2.gif](https://i.loli.net/2020/12/10/1w974mAH52hOvF3.gif)\n\nOtherwise you can give him/her this article.","date":"Jan 9, 2021","category":{"name":"技术学习","permalink":"http://127.0.0.1:8080/category/技术学习"},"tags":[{"name":"数据库","permalink":"http://127.0.0.1:8080/tags/数据库"}]},{"title":"个人云服务器软件配置备忘录","permalink":"http://127.0.0.1:8080/detail/my-linux-server-config","content":"\n## 创建非root用户\n\n### 创建用户\n```\nuseradd -m -s /bin/bash zxc\n```\n\n其中`useradd`是添加用户的命令，需要root权限，`-m`表示自动创建用户的家目录，`-s /bin/bash`指定用户登录后使用的shell。\n\n### 设置密码\n```\npasswd zxc\n```\n\n重复输入密码，回车确定。\n\n### 将用户加入sudoer\n\n新建的用户默认是无法使用`sudo`命令的，需要修改`/etc/sudoers`文件，而这个文件默认是只读的，`u+w`的`u`表示文件的拥有者，`+w`表示添加写权限，在这里也相当于`chmod 600`了。\n\n```\nchmod u+w /etc/sudoers\n```\n\n然后修改`/etc/sudoers`文件，添加一行\n\n```\nzxc ALL=(ALL:ALL) ALL\n```\n\n这里添加的语句可以有4种形式，分别是：\n\n```\nxxx ALL=(ALL:ALL) ALL\n%xxx ALL=(ALL:ALL) ALL\nxxx ALL=(ALL:ALL) NOPASSWD: ALL\n%xxx ALL=(ALL:ALL) NOPASSWD: ALL\n```\n\n它们的含义分别是:\n允许用户xxx执行sudo命令(需要输入密码)；\n允许用户组xxx里面的用户执行sudo命令(需要输入密码)；\n允许用户xxx执行sudo命令(不需要输入密码)；\n允许用户组xxx里面的用户执行sudo命令(不需要输入密码)。\n\n其中这几个ALL的含义分别是：第一个ALL是主机限定，可以改为localhost，改成别的ip的情况，我猜测**可能可以执行sudo但是只能执行另一台主机的命令**，第二个ALL是限定可以以该用户的角色去执行命令，所以可以改成root，冒号后面的第三个ALL是表示用户组，含义同第二个，最后一个ALL表示可以执行的命令。\n\n最后记得撤销文件的写权限\n\n```\nchmod u-w /etc/sudoers\n```\n\n## 配置ssh密钥登录\n\n### 本地生成ssh密钥\n\n```\nssh-keygen -t rsa\n```\n\n表示使用rsa生成密钥存放在`~/.ssh`目录下，在linux中`~`表示家目录，普通用户就是`/home/xxx`，root用户就是`/root`。\n\n- 将公钥复制到远程主机\n\n```\nssh-copy-id -i ~/.ssh/id_rsa.pub  zxc@xxx.xx.xx.xx\n```\n\n`-i`表示指定公钥文件位置，这个公钥会被复制到远程主机`~/.ssh/`下的`authorized_keys`文件，这个文件保存了被授权允许通过ssh登录的主机公钥。\n\n- 修改文件读写权限\n\n```\nchmod 600 ~/.ssh/authorized_keys\nchmod 700 -R ~/.ssh\n```\n\n为了保证安全，其中600表示只有拥有者有读写权限，700表示只有拥有者有读、写、执行权限。\n\n### 禁止密码登录(可选)\n\n修改`etc/ssh/sshd_conf`文件\n\n```\nPasswordAuthentication no\n```\n\n### 解决客户端SSH连接服务器一段时间不操作之后卡死的问题\n\n修改本地文件`/etc/ssh/ssh_config`，添加两个配置\n\n```\nServerAliveInterval 50 #每隔50秒就向服务器发送一个请求\nServerAliveCountMax 3  #允许超时的次数\n```\n\n然后重启ssh服务\n\n```\nsudo /etc/init.d/ssh restart\n```\n\n## 环境变量\n\n### 配置方法\n\n在Linux系统中，环境变量是用来定义系统运行环境的一些参数。\n\n`export`命令显示系统定义的所有环境变量，而`echo $PATH`命令可以显示`PATH`环境变量的值，也就是在命令行输入一个命令，默认查找的位置。\n\n配置环境变量的方法有以下几种：\n\n#### 方法一 `export`\n\n直接使用`export`命令修改环境变量，比如\n\n```\nexport PGHOST=localhost\n```\n\n设置一个名为`PGHOST`的环境变量，值为`localhost`。\n\n比如\n\n```\nexport PATH=/home/uusama/mysql/bin:$PATH\n# 或者\nexport PATH=$PATH:/home/uusama/mysql/bin\n```\n\n添加`PATH`环境变量的值，其中`$PATH`表示原来的值，和新的值之间用冒号分隔。\n\n**注意：**\n\n| key | value |\n| --- | --- |\n| 生效时间 | 立即生效 |\n| 生效期限 | 当前终端有效，窗口关闭后失效 |\n| 生效范围 | 仅对当前用户有效 |\n\n#### 方法二 `~/.bashrc`\n\n通过修改`~/.bashrc`文件进行配置。\n\n**注意：**\n\n| key | value |\n| --- | --- |\n| 生效时间 | 使用相同用户重新打开终端生效，或者手动执行`source ~/.bashrc`生效 |\n| 生效期限 | 永久有效 |\n| 生效范围 | 仅对当前用户有效 |\n\n#### 方法三 `~/.bash_profile`\n\n通过修改`~/.bash_profile`文件进行配置。\n\n**注意：**\n\n| key | value |\n| --- | --- |\n| 生效时间 | 使用相同用户重新打开终端生效，或者手动执行`source ~/.bash_profile`生效 |\n| 生效期限 | 永久有效 |\n| 生效范围 | 仅对当前用户有效 |\n\n#### 方法四 `/etc/bashrc`\n\n通过修改`/etc/bashrc`文件进行配置，需要root权限以及该文件的写入权限。\n\n**注意：**\n\n| key | value |\n| --- | --- |\n| 生效时间 | 重新打开终端生效，或者手动执行`source /etc/bashrc`生效 |\n| 生效期限 | 永久有效 |\n| 生效范围 | 对所有用户有效 |\n\n#### 方法五 `/etc/profile`\n\n通过修改`/etc/profile`文件进行配置，需要root权限以及该文件的写入权限。\n\n**注意：**\n\n| key | value |\n| --- | --- |\n| 生效时间 | 重新打开终端生效，或者手动执行`source /etc/profile`生效 |\n| 生效期限 | 永久有效 |\n| 生效范围 | 对所有用户有效 |\n\n#### 方法四 `/etc/enviroment`\n\n通过修改`/etc/enviroment`文件进行配置，需要root权限以及该文件的写入权限。\n\n**注意：**\n\n| key | value |\n| --- | --- |\n| 生效时间 | 重新打开终端生效，或者手动执行`source /etc/enviroment`生效 |\n| 生效期限 | 永久有效 |\n| 生效范围 | 对所有用户有效 |\n\n### 加载顺序\n\n基本上是从系统级到用户级，从`profile`到`bashrc`的顺序\n\n- `/etc/environment`\n- `/etc/profile`\n- `/etc/bashrc`\n- `~/.profile`\n- `~/.bashrc`\n\n## PostgreSQL\n\n### 安装\n\n```\nsudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list'\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\nsudo apt-get update\nsudo apt-get -y install postgresql\n```\n\n`sh -c`的作用是将后面的字符串当做完整的命令来执行，这样就可以保证`sudo`的作用范围在整个命令，然后`echo`命令表示输出文本，后面加上重定向符号表示输出到文件`/etc/apt/sources.list.d/pgdg.list`，这个文件以`.list`为拓展名，表示是第三方源，统一放在`sources.list.d`这个文件夹下，而`/etc/sources.list`这个则是官方的软件源。\n\nlist文件的格式主要分为几部分，第一部分通常是`deb`或者`deb-src`，因为ubuntu是基于debian的，`deb`其实就是debian的前三个字母，在这里表示通过`.deb`文件安装或者通过源码安装；第二部分是安装文件的url，在这里是`http://apt.postgresql.org/pub/repos/apt`，第三部分是版本代号，这里是`$(lsb_release -cs)-pgdg`，其中`$()`可以理解为执行括号内的命令得到的输出，在服务器Ubuntu20.04系统上输出是focal，所以对应的可以在刚才的url里找到`./dists/focal-pgdg`这个文件夹里的安装文件，而在本地执行这个命令显示`n/a`，第四部分是限定词，这里是`main`。\n\n配置了软件源，就要通过`apt-get update`更新软件源(**不是更新软件**)，通俗理解是把新的软件源中的软件列表更新到电脑上，而`apt-get upgrade`才是更新软件。\n\n`apt-get -y install`中的`-y`表示跳过确认`yes/no`，通过这个命令，软件默认安装到了`/usr/share`下的对应文件夹。\n\n### 配置\n\nPostgreSQL安装后默认创建了一个名为`postgres`的用户，既是DMBS的用户，也是Linux用户，所以先切换到该用户进入psql。\n\n```\nsudo su - postgres\npsql\n```\n\n此时进入PostgreSQL的交互式shell，修改默认用户密码，并添加一个刚刚创建的Linux用户所对应的数据库用户。\n\n```\nALTER USER postgres WITH PASSWORD 'xxxxxx';\n```\n\n修改密码后，如果当前Linux用户就是数据库对应的用户，也无需密码即可进入，否则通过`psql -U usernmae`再输入密码进入。\n\n此时如果用用户zxc执行命令`psql -U postgres`会报错，有两种解决方案，一个数需要指定`-h localhost`，或者将`PGHOST=localhost`写入环境变量里，另一个是修改配置文件`pg_hba.conf`\n\n```\n# 将这一行最后的peer改为md5(需要密码)或者trust(不需要密码)\nlocal  all  postgres  md5\n```\n\n注意到下面还有一行，也是同样的用处，all表示不指定某一个用户，但是在这里单独修改是不生效的，相当于通用的规则会被上面特定用户的规则所覆盖。\n\n```\nlocal  all  all  md5\n```\n\n修改后通过`service postgresql reload`重启生效，或者在psql shell中执行`SELECT pg_reload_conf()`。\n\n## MongoDB\n\n### 安装\n\n类似的，通过官网配置软件源的方式来安装。\n\n```\nwget -qO - https://www.mongodb.org/static/pgp/server-4.4.asc | sudo apt-key add -\necho \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/4.4 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.4.list\nsudo apt-get update\nsudo apt-get install -y mongodb-org\n```\n\n但是因为对Linux不熟悉，所以我觉得这种安装方法比较难以控制，所以采用tarball压缩包的安装方法。\n\n现在[官网](https://www.mongodb.com/try/download/community)下载一个tgz压缩包并解压，这里我在`~`目录下创建了一个`tgz`文件夹用来放一些软件的压缩包，一个`app`文件夹放这些软件。\n\n```\nwget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-ubuntu2004-4.4.2.tgz\ntar -zxvf mongodb-linux-*-4.4.2.tgz -C ../app/\nmv mongodb-linux-aarch64-ubuntu2004-4.4.2/ mongodb/\n```\n\n下载的时候如果出现域名解析错误，就需要修改`/etc/resolv.conf`，添加googleDNS域名服务器。\n\n```\nnameserver 8.8.8.8\nnameserver 8.8.4.4\n```\n\n然后注意下载的应该是`x86_64`版本，也就是不加`(arm)`的版本，可以通过命令`arch`来查询。\n\n解压时`-C`指定解压目录。\n\n### 配置\n\n然后需要配置环境变量，这里我们配置在`~/.bashrc`文件中。\n\n```\nexport PATH=$PATH:/home/zxc/app/mongodb/bin/\n```\n\n创建数据目录和日志目录，这里我在`~/app`下创建了`mongondb-data`用来存放mongodb相关的所有数据，里面有一个`data`文件夹和一个`log`文件夹。\n\n然后启动`mongod`，这里的`d`就是deamon的意思，守护进程，相当于启动了一个后台监控服务。\n\n```\nmongod --dbpath ~/app/mongodb-data/data --logpath ~/app/mongodb-data/log/mongod.log --fork\n```\n\n默认情况下，MongoDB没有启用访问控制，没有默认的用户名和密码，所以可以直接通过`mongodb`命令进入shell。为了实现用户角色的访问控制，我们需要自己创建用户。\n\n首先创建一个管理员用户`admin`。\n\n```\nuse admin\ndb.createUser({\n        user:\"userAdmin\",\n        pwd:\"xxxxxxxx\",\n        roles:[\n            \"clusterAdmin\",\n            \"dbAdminAnyDatabase\",\n            \"userAdminAnyDatabase\",\n            \"readWriteAnyDatabase\"\n        ]\n    }\n)\n```\n\n分别给这个`userAdmin`的用户分配了集群管理权限，所有数据库管理权限，所有用户管理权限，所有数据库读写权限。\n\n然后重启`mongod`，注意加上`--auth`参数。然后通过`db.auth()`方法进行授权验证，才可以进行其他操作。\n\n注意正确的关闭`mongod`的方法是在mongo shell中使用`db.shutdownServer()`函数。\n\n```\nuse admin\ndb.auth(\"userAdmin\",\"xxxxxxxx\")\n```\n\n然后注意，MongoDB启动之后，默认是只绑定本机ip，要想远程访问，还需要加上参数`--bind_ip=0.0.0.0`，我们可以把这些启动参数保存在一个配置文件\n\n```\ndbpath=/home/zxc/app/mongodb-data/data\nlogpath=/home/zxc/app/mongodb-data/log/mongod.log\nlogappend=true\nport=27017\nfork=true\nauth=true\nbind_ip=0.0.0.0 # 所有ip可访问，为了安全可以指定客户端ip\n```\n\n启动`mongod`指定配置文件即可。\n\n```\nmongod -f mongod.conf\n```\n\n注意`nohttpinterface`这个配置属性默认是关闭的，所以我们也可以通过浏览器访问`ip:27017`来判断`mongod`是否已经正确启动。\n\n## Docker\n\n### 安装\n\n按照官网文档，选择通过存储库(repository)来进行安装。\n\n这是更新apt包索引以及安装允许使用HTTPS存储库的包。\n\n```\nsudo apt-get update\nsudo apt-get install \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    gnupg-agent \\\n    software-properties-common\n```\n\n添加官方密钥。\n\n```\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n```\n\n其中`curl`是也是下载命令，`-fsSL`的`-f`表示当出现HTTP错误时静默，`-s`也表示静默模式，因为这里的输出将作为下一条命令的输入，所以不需要其他的输出，`-S`表示即使开了`-s`，也会输出错误，`-L`表示服从重定向，`|`是管道符号，表示上一条命令的输出作为下一条命令的参数，`apt-key add`表示将软件包密钥添加到本地trusted数据库中，`-`表示`stdin`(标准输入)，也正是上一条命令curl的输出通过管道变成这条命令的标准输入\n\n设置稳定版本的存储库。\n\n```\nsudo add-apt-repository \\\n   \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n   $(lsb_release -cs) \\\n   stable\"\n```\n\n`add-apt-repository`命令其实是一个python脚本，自动在source.list里添加软件源。\n\n安装docker稳定版。\n\n```\nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io\n```\n\n最后通过运行hello world镜像来判断是否安装成功。\n\n```\nsudo docker run hello-world\n```\n\n### 配置\n\n因为众所周知的原因，我们需要修改仓库镜像源，直接修改`/etc/docker/daemon.json`文件，加入中科大，网易，官方中国仓库三个。\n\n```\n{\"registry-mirrors\": [\"https://docker.mirrors.ustc.edu.cn\",\"http://hub-mirror.c.163.com\",\"https://registry.docker-cn.com\"]}\n```\n\n然后顺便搭建一个nextcloud私有云盘。\n\n```\nsudo docker run -d -p 8080:80 nextcloud\n```\n\n`-d`表示detach，默认在后台运行容器，并且返回容器id，`-p`表示将容器的一个端口暴露给外层主机，这里`8080:80`表示访问外部主机8080端口就映射到该容器的80端口，也就是nextcloud的默认端口。\n\ndocker默认每次执行都需要root权限，并且docker守护进程启动的时候，会默认赋予名字为docker的用户组读写Unix socket的权限，因此只需要将当前用户加入到docker用户组即可。\n\n```\nsudo groupadd docker\nsudo gpasswd -a $USER docker\nnewgrp docker\n```\n\n`groupadd docker`是创建一个docker用户组，这里通常已经自动创建好了，`gpasswd -a`表示添加用户到用户组，`newgrp`是切换到指定用户组。\n\n安装docker-compose，docker-compose是一个通过yml文件来管理多容器应用的工具，而不需要每次都去拉去image，再创建多个容器。\n\n下载，如果速度太慢可以本地下载后再上传到服务器。\n\n```\nsudo curl -L \"https://github.com/docker/compose/releases/download/1.27.4/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\n```\n\n添加执行权限。\n\n```\nsudo chmod +x /usr/local/bin/docker-compose\n```\n\n创建软链接。\n\n```\nsudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose\n```\n\n测试是否安装成功。\n\n```\ndocker-compose --version\n```\n","date":"Nov 29, 2020","category":{"name":"技术学习","permalink":"http://127.0.0.1:8080/category/技术学习"},"tags":[{"name":"服务器","permalink":"http://127.0.0.1:8080/tags/服务器"},{"name":"Linux","permalink":"http://127.0.0.1:8080/tags/Linux"},{"name":"ssh","permalink":"http://127.0.0.1:8080/tags/ssh"},{"name":"环境变量","permalink":"http://127.0.0.1:8080/tags/环境变量"},{"name":"PostgreSQL","permalink":"http://127.0.0.1:8080/tags/PostgreSQL"},{"name":"MongoDb","permalink":"http://127.0.0.1:8080/tags/MongoDb"}]},{"title":"一次关于git的简单查错","permalink":"http://127.0.0.1:8080/detail/a-suck-debug-for-git","content":"\n本地一个项目突然push不上，报错是\n\n```\nERROR: Repository not found.\nfatal: Could not read from remote repository.\n\nPlease make sure you have the correct access rights\nand the repository exists.\n```\n\n没有权限，第一步检查了github，**确定有这个仓库**，然后**检查了ssh key**，本地和github上的都没有问题，也重新添加了，仍然不行，所以不是这个问题。\n\n再接下来通过`git remote -v`**检查远程仓库链接**，没有问题，通过`git remote rm origin`和`git remote add origin`重新添加后还行不行。\n\n然后通过`git config user.name/user.email`检查本地仓库的用户名和邮箱，也没问题。\n\n想到大概率是两个ssh key的问题，因为之前博客放在小号的github上，所以本地配置了两个ssh密钥对，附上过程：\n\n通过`ssh-keygen -t rsa -C 'xx@xx.com' -f ~/.ssh/xxx`生成一个新的密钥对，然后在`~/.ssh/config`中配置\n\n```\nHost github.com\nHostName github.com\nPreferredAuthentications publickey\nIdentityFile ~/.ssh/id_rsa\n\nHost zxc.github.com\nHostName github.com\nPreferredAuthentications publickey\t\t\nIdentityFile ~/.ssh/xxx\n```\n\n这里因为两个都是用在github上，所以host应该加个子域名区分，如果是不同网站，比如gitee，直接gitee.com就行。\n\n因此，重新仔细检查了`~/.ssh/config`文件，最后发现问题是把`id_rsa`打成了`id-rsa`...","date":"Dec 28, 2020","category":{"name":"杂七杂八","permalink":"http://127.0.0.1:8080/category/杂七杂八"},"tags":[{"name":"git","permalink":"http://127.0.0.1:8080/tags/git"},{"name":"debug","permalink":"http://127.0.0.1:8080/tags/debug"}]},{"title":"关于for循环用无符号byte引发的一个bug","permalink":"http://127.0.0.1:8080/detail/a-bug-caused-by-for-a-ubyte","content":"\r\n突然脑抽for循环定义byte变量\r\n\r\n```c#\r\nfor (byte i = 0; i < 256; i++) {\r\n    data.Add(i);\r\n}\r\n```\r\n\r\n看似很有道理，byte的范围是0-255，那我就遍历到255就行，结果单元测试一直炸，而且电脑变卡，所以猜测是内存炸了。\r\n\r\n定睛一看，这TM不是一个死循环！i到255后在加1，又变成0...\r\n\r\n解决，又顺利浪费了20分钟。","date":"Jan 8, 2021","category":{"name":"杂七杂八","permalink":"http://127.0.0.1:8080/category/杂七杂八"},"tags":[{"name":"debug","permalink":"http://127.0.0.1:8080/tags/debug"}]},{"title":"正则表达式贪婪匹配和非贪婪匹配","permalink":"http://127.0.0.1:8080/detail/regular-expression-greedy","content":"\n在做一个简单的Lexer时遇到的一个问题，需求是使用正则表达式去匹配mongodb查询语句中的JSON对象，例如`find({}).forPurpose({})`，一开始使用的正则是`{.*}`，这样其实默认是贪婪模式，会尽量匹配长的子串，所以会匹配到`{}).forPurpose({}`，所以在这里应该使用非贪婪模式`{.*?}`，也就是在量词后面再加一个`?`即可。","date":"Dec 26, 2020","category":{"name":"杂七杂八","permalink":"http://127.0.0.1:8080/category/杂七杂八"},"tags":[{"name":"正则表达式","permalink":"http://127.0.0.1:8080/tags/正则表达式"}]},{"title":"B+树(Java实现)","permalink":"http://127.0.0.1:8080/detail/b-plus-tree-java-implementation","content":"\n## 基础概念\n\nB+树是一种多叉的平衡树，特点是数据都存储在叶子节点，且所有叶子节点通过一个双向链表连起来，而中间节点通过x个key分割x+1个子节点，每个子节点的所有值都大于等于左边的key，小于右边的key。对于一个m阶(order)的B+树，中间节点最多可以存储m-1个key，最少存储(m+1)/2-1个，叶子节点最多可以存储m-1个数据key-value对，最少存储(m+1)/2-1个。\n\n## Search\n\n查找是最简单的操作，从根节点往下递归直到叶子节点，通过中间节点的`searchInInternalNode`函数来判断要往哪个儿子节点走。\n\n```java\npublic Tuple search(Tuple key){                       \n    Node curNode=root;                                \n    while(true){                                      \n        if(curNode.isLeaf){                           \n            return curNode.searchInLeafNode(key);     \n        }else{                                        \n            curNode=curNode.searchInInternalNode(key);\n        }                                             \n    }                                                 \n}                                                     \n```\n\n## Insert\n\n插入操作的第一步类似查找，从根节点往下找到叶子节点，然后插入到叶子节点，如果该叶子节点size超过了，就应该进行第二步的分裂，`splitLeafNode(tree)`。\n\n```java\npublic boolean insert(Tuple key,Tuple value){\n    if(search(key)!=null){\n        return false;\n    }\n    Node curNode=root;\n    while(true){\n        if(curNode.isLeaf){\n            curNode.insertLeafNode(this,key,value);\n            break;\n        }else{\n            curNode= curNode.searchInInternalNode(key);\n        }\n    }\n    return true;\n}\npublic void insertLeafNode(BPlusTree tree,Tuple key,Tuple value){\n    insertKeyValue(key,value);\n    if(isOverfull()){\n        splitLeafNode(tree);\n    }                                     \n}               \n```\n\n分裂叶子节点的方法首先是创建两个新的叶子节点，修改链表关系，然后复制所有key-value对，然后比较复杂的一步是修改父亲关系，需要判断当前分裂的叶子节点是否是根节点，若是根节点，需要创建一个新的根节点作为分裂的两个叶子节点的父节点，若不是根节点，需要删除父节点对当前分裂节点的儿子指针，然后加上分裂后两个新节点的儿子指针，然后如果父节点的size也超过了，就应该进行第三步的分裂，`splitInternalNode(tree)`\n\n```java\npublic void splitLeafNode(BPlusTree tree){               \n    Node left=buildLeafNode(tree.order);                 \n    Node right=buildLeafNode(tree.order);                \n    left.prev=prev;                                      \n    if(prev!=null){                                      \n        prev.next=left;                                  \n    }else{                                               \n        tree.head=left;                                  \n    }                                                    \n    right.next=next;                                     \n    if(next!=null){                                      \n        next.prev=right;                                 \n    }                                                    \n    left.next=right;                                     \n    right.prev=left;                                     \n    int siz=keys.size();                                 \n    int leftSize=(siz+1)/2;                              \n    left.keys.addAll(keys.subList(0,leftSize));          \n    left.values.addAll(values.subList(0,leftSize));      \n    right.keys.addAll(keys.subList(leftSize,siz));       \n    right.values.addAll(values.subList(leftSize,siz));   \n    if(isRoot){                                          \n        splitNewRoot(tree,left,right);            \n        tree.root.keys.add(right.keys.get(0));           \n    }else{                                               \n        parent.replaceChildren(this,left,right);     \n        parent.insertInternalNode(right.keys.get(0));    \n        if(parent.isOverfull()){                         \n            parent.splitInternalNode(tree);              \n        }                                                \n    }                                                    \n}          \nprivate void replaceChildren(Node node,Node left,Node right){\n    int index=children.indexOf(node);                        \n    children.remove(index);                                  \n    left.parent=this;                                        \n    right.parent=this;                                       \n    children.add(index,left);                                \n    children.add(index+1,right);                             \n}     \nprivate void splitNewRoot(BPlusTree tree,Node left,Node right){\n    isRoot=false;\n    tree.root= buildRootNode(tree.order);\n    left.parent=tree.root;\n    right.parent=tree.root;\n    tree.root.children.add(left);\n    tree.root.children.add(right);\n}                                                                                                     \n```\n\n类似叶子节点的分裂，首先创建两个中间节点，然后复制key和children，注意中间节点的分裂需要把中间一个key拉上去，然后修改父亲关系，同样是需要判断是否是根节点，若是根节点，同叶子节点，创建一个新的根节点然后把分裂的两个节点加上去，注意和叶子节点不同的是这里根节点加的key是拉上去的key，若不是根节点，同叶子节点进行替换，并插入拉上去的key，再判断父亲节点是否需要继续分裂。\n\n```java\npublic void splitInternalNode(BPlusTree tree){                        \n    Node left=buildInternalNode(tree.order);                          \n    Node right=buildInternalNode(tree.order);                         \n    int siz=keys.size();                                              \n    int leftSize=(siz+1)/2;                                           \n    Tuple keyToParent=keys.get(leftSize);                             \n    left.keys.addAll(keys.subList(0,leftSize));                       \n    children.subList(0,leftSize+1).forEach(child->{                   \n        left.children.add(child);                                     \n        child.parent=left;                                            \n    });                                                               \n    right.keys.addAll(keys.subList(leftSize+1,siz));                  \n    children.subList(leftSize+1,siz+1).forEach(child->{               \n        right.children.add(child);                                    \n        child.parent=right;                                           \n    });                                                               \n    if(isRoot){                                                       \n        splitNewRoot(tree,left,right);                        \n        tree.root.keys.add(keyToParent);                              \n    }else{\n        parent.replaceChildren(this,left,right);                      \n        parent.insertInternalNode(keyToParent);                     \n        if(parent.isOverfull()){                                      \n            parent.splitInternalNode(tree);                           \n        }                                                             \n    }                                                                 \n}                                                                     \n```\n\n## Delete\n\n删除操作细节比较多，整体框架也类似插入，递归找到叶子节点并删除对应key-value对，然后如果size不足，考虑三种情况，从前驱结点借，从后继节点借，或者合并前驱或后继节点，然后合并后父节点会少一个key，如果size不足，就需要继续进行下一步的操作，同时还要处理类似删除最后一个节点，删除第一个key-value对后必须修改祖先节点的key等细节。\n\n```java\npublic boolean delete(Tuple key,Tuple value){\n    if(search(key)==null){\n        return false;\n    }\n    Node curNode=root;\n    while(true){\n        if(curNode.isLeaf){\n            return curNode.deleteLeafNode(this,key,value);\n        }else{\n            curNode= curNode.searchInInternalNode(key);\n        }\n    }\n}\npublic boolean deleteLeafNode(BPlusTree tree,Tuple key,Tuple value){\n    boolean first=keys.indexOf(key)==0;                            \n    Tuple oldKey=keys.get(0);                                      \n    if(!deleteKeyValue(key,value)){                                \n        return false;                                              \n    }                                                              \n    if(isRoot && keys.size()==0){                                  \n        isRoot=false;                                              \n        tree.root=null;                                            \n    }                                                              \n    if(first && keys.size()>0){                                    \n        Tuple newKey=keys.get(0);                                  \n        updateParentKey(oldKey,newKey);                            \n    }                                                              \n    if(isDeficient()){                                             \n        if(isPrevLoanable()){                                      \n            loanFromPrev();                                        \n        }else if(isNextLoanable()){                                \n            loadFromNext();                                        \n        }else{                                                     \n            if (canMergePrev()) {                                  \n                mergePrev();                                       \n            } else if (canMergeNext()) {                           \n                mergeNext();                                       \n            }                                                      \n            if(parent!=null && parent.isDeficient()){              \n                mergeInternalNode(tree,parent);                    \n            }                                                      \n        }                                                          \n    }                                                              \n    return true;                                                   \n}          \nprivate boolean isPrevLoanable(){                                                         \n    return prev!=null && prev.isLoanable() && parent==prev.parent;                        \n}                                                                       \nprivate boolean isNextLoanable(){                                                         \n    return next!=null && next.isLoanable() && parent==next.parent;                        \n}                                                                                         \nprivate void loanFromPrev(){                                                              \n    int siz=prev.keys.size();                                                             \n    Tuple loanKey=prev.keys.remove(siz-1);                                                \n    keys.add(0,loanKey);                                                                  \n    Tuple loanValue=prev.values.remove(siz-1);                                            \n    values.add(0,loanValue);                                                              \n    int index=parent.children.indexOf(this)-1;                                            \n    parent.keys.remove(index);                                                            \n    parent.keys.add(index,loanKey);                                                       \n}                                                                                         \nprivate void loadFromNext(){                                                              \n    Tuple loanKey=next.keys.remove(0);                                                    \n    keys.add(loanKey);                                                                    \n    Tuple loanValue=next.values.remove(0);                                                \n    values.add(loanValue);                                                                \n    int index=parent.children.indexOf(this);                                              \n    parent.keys.remove(index);                                                            \n    parent.keys.add(index,next.keys.get(0));                                              \n}     \nprivate boolean canMerge(Node node) {                                                       \n    return node!=null && keys.size() + node.keys.size() <= maxSize && parent == node.parent;\n}                                                                                           \nprivate boolean canMergePrev(){                                                             \n    return canMerge(prev);                                                                  \n}                                                                                           \nprivate boolean canMergeNext(){                                                             \n    return canMerge(next);                                                                  \n}                                                                                   \nprivate void mergePrev(){                    \n    int siz=keys.size();                     \n    for (int i = 0; i <siz; i++) {           \n        prev.keys.add(keys.get(i));          \n    }                                        \n    int index=parent.children.indexOf(this)-1\n    parent.keys.remove(index);               \n    parent.children.remove(this);            \n    prev.next=next;                          \n    if (next != null) {                      \n        next.prev=prev;                      \n    }                                        \n}                                            \nprivate void mergeNext(){                    \n    int siz=keys.size();                     \n    for (int i = 0; i <siz; i++) {           \n        next.keys.add(0,keys.get(i));        \n    }                                        \n    int index=parent.children.indexOf(this); \n    parent.keys.remove(index);               \n    parent.children.remove(this);            \n    next.prev=prev;                          \n    if (prev != null) {                      \n        prev.next=next;                      \n    }                                        \n}                                                                                                               \n```\n\n第一种是情况是要操作的父节点是根节点，且儿子节点只有一个，那么该儿子节点将作为新的根节点，另一种情况，首先要找出当前操作的中间节点的前驱和后继节点，注意中间节点是没有双向链表相连的，然后类似叶子节点的操作，也是考虑三种情况，从前驱结点借，从后继节点借，或者合并前驱或后继节点，注意从前驱结点借，其实相当于它们中间父节点的key落下来，然后前驱结点这个借出去的key拉上去，后继节点同理，最后同样是要往上考虑父亲节点是否需要继续合并下去。\n\n```java\nprivate void mergeInternalNode(BPlusTree tree,Node curNode){                        \n    if (curNode.isRoot) {                                                           \n        if (curNode.children.size() < 2) {                                          \n            tree.root=tree.root.children.get(0);                                    \n            tree.root.isRoot = true;                                                \n            tree.root.parent = null;                                                \n        }                                                                           \n    } else {                                                                        \n        int curIdx = curNode.parent.children.indexOf(curNode);                      \n        int preIdx = curIdx - 1;                                                    \n        int nextIdx = curIdx + 1;                                                   \n        Node prevNode = null;                                                       \n        Node nextNode = null;                                                       \n        if (preIdx >= 0) {                                                          \n            prevNode = curNode.parent.children.get(preIdx);                         \n        }                                                                           \n        if (nextIdx < curNode.parent.children.size()) {                             \n            nextNode = curNode.parent.children.get(nextIdx);                        \n        }                                                                           \n        if (isInternalNodeLoanable(curNode,prevNode)) {                             \n            borrowMiddleNodePrevious(curNode,prevNode);                             \n        } else if (isInternalNodeLoanable(curNode,nextNode)) {                      \n            borrowMiddleNodeNext(curNode,nextNode);                                 \n        } else {                                                                    \n            if (curNode.canMergeInternalNode(prevNode)) {                           \n                mergeTwiToPreMiddleNode(prevNode, curNode);                            \n                int parentKeyIdx = curNode.parent.children.indexOf(curNode)-1;      \n                curNode.parent.keys.remove(parentKeyIdx);                           \n                curNode.parent.children.remove(parentKeyIdx + 1);                   \n            } else if (curNode.canMergeInternalNode(nextNode)) {                    \n                mergeTwoToPreMiddleNode(curNode, nextNode);                            \n                int parentKeyIdx = curNode.parent.children.indexOf(nextNode)-1;     \n                curNode.parent.keys.remove(parentKeyIdx);                           \n                curNode.parent.children.remove(parentKeyIdx + 1);                   \n            }                                                                       \n            if(curNode.parent.isDeficient()){                                       \n                mergeInternalNode(tree,curNode.parent);                             \n            }                                                                       \n        }                                                                           \n    }                                                                               \n}        \nprivate boolean isInternalNodeLoanable(Node curNode,Node node){           \n    return node!=null && node.isLoanable() && curNode.parent==node.parent;\n}           \nprivate void borrowMiddleNodePrevious(Node curNode,Node preNode) {                                                                                 \n    int parentKeyIdx = curNode.parent.children.indexOf(curNode)-1;             \n    Tuple downKey = curNode.parent.keys.get(parentKeyIdx);                     \n    curNode.keys.add(0, downKey);                                              \n    curNode.parent.keys.remove(parentKeyIdx);                                  \n    int preSize = preNode.keys.size();                                         \n    Tuple upKey = preNode.keys.get(preSize - 1);                               \n    curNode.parent.keys.add(parentKeyIdx, upKey);                              \n    preNode.keys.remove(preSize - 1);                                          \n    int preChildSize = preNode.children.size();                                \n    Node borrowPoint = preNode.children.get(preChildSize - 1);                 \n    curNode.children.add(0 , borrowPoint);                                     \n    preNode.children.remove(preChildSize - 1);                                 \n    borrowPoint.parent = curNode;                                              \n}                                                                              \nprivate void borrowMiddleNodeNext(Node curNode,Node nextNode) {                                                                                     \n    int parentKeyIdx = curNode.parent.children.indexOf(nextNode)-1;            \n    Tuple downKey = curNode.parent.keys.get(parentKeyIdx);                     \n    curNode.keys.add(downKey);                                                 \n    curNode.parent.keys.remove(parentKeyIdx);                                  \n    Tuple upKey = nextNode.keys.get(0);                                        \n    curNode.parent.keys.add(parentKeyIdx, upKey);                              \n    nextNode.keys.remove(0);                                                   \n    Node borrowPoint = nextNode.children.get(0);                               \n    curNode.children.add(borrowPoint);                                         \n    nextNode.children.remove(0);                                               \n}                                                                                                                                                          \nprivate boolean canMergeInternalNode(Node node){\n    if (node != null) {\n        return keys.size() + node.keys.size() <= maxSize && parent == node.parent;\n    }\n    return false;\n}\nprivate void mergeTwoInternalNode(Node first, Node sec) {\n    int parentKeyIdx = sec.parent.children.indexOf(sec)-1;\n    first.keys.add(first.parent.keys.get(parentKeyIdx));\n    first.keys.addAll(sec.keys);\n    for (int i = 0; i < sec.children.size(); i++) {\n        sec.children.get(i).parent = first;\n        first.children.add(sec.children.get(i));\n    }\n}                                                        \n```\n","date":"Dec 20, 2020","category":{"name":"算法学习","permalink":"http://127.0.0.1:8080/category/算法学习"},"tags":[{"name":"B+树","permalink":"http://127.0.0.1:8080/tags/B+树"},{"name":"数据库","permalink":"http://127.0.0.1:8080/tags/数据库"},{"name":"数据结构","permalink":"http://127.0.0.1:8080/tags/数据结构"}]},{"title":"位运算实现加减乘除","permalink":"http://127.0.0.1:8080/detail/add-sub-mul-div-by-bit-operation","content":"\n## 加法\n\n### 思路\n\n思路很简单，首先假设两个数相加，所有位都没进位，那么结果就是所有位直接对应相加即可，比如123+234=357，而如果有一些位有进位的清空，比如456+789，我们先不考虑进位，每一位单独算，得到的结果设为a，这里a=135，再考虑进位，这里每一位都进位了，分别进了十位，百位和千位，所以进位的结果是1110，那很显然，我们可以将456+789的答案转化为**不考虑进位的和**加上**进位的值**，在这里也就是135+1110=1245，如果这个加法也出现了进位，那么很明显就采用递归/迭代的思路继续操作下去，直到两个数的加法不出现进位，即为答案。\n\n上面是10进制的情况，2进制的道理是一样的，而且因为2进制我们可以采用位运算来快速计算上面的**不进位加法**(位异或)和**进位和**(位与再左移1位)，所以就可以采用位运算的方法来计算两个数的和。\n\n### 代码\n\n递归版：\n\n```cpp\nint add(int a,int b){\n    if(b==0){\n        return a;\n    }\n    return add(a^b,(a&b)<<1);\n}\n```\n\n非递归版：\n\n```cpp\nint add(int a,int b){\n    while(b!=0){\n        int c=a^b;\n        b=(a&b)<<1;\n        a=c;\n    }\n    return a;\n}\n```\n\n## 减法\n\n### 思路\n\n减去一个数，就等于加上一个的相反数，所以我们考虑相反数的二进制表示，总所周知，计算机是使用补码来表示一个数，比如我们分别输出-5和5的二进制位如下。\n\n```cpp\nvoid print(int a){\n    for(int i=31;i>=0;i--){\n        printf(\"%d\",(a>>i)&1);\n    }\n    printf(\"\\n\");\n}\n//111111111111111111111111111111011\n//000000000000000000000000000000101\n```\n\n所以相反数的二进制表示就是按位取反再加1，而不用考虑正数还是负数的问题。\n\n### 代码\n\n```cpp\nint neg(int a){\n    return add(~a,1)\n}\nint sub(int a,int b){\n    return add(a,neg(b));\n}\n```\n\n## 乘法\n\n### 思路\n\n乘法的思路是二进制数列竖式计算的思路，首先单独处理符号位(a^b可以判断符号位)，用绝对值来进行计算，对于被乘数乘以乘数，我们从低到高遍历乘数，当乘数当前位为1，答案应该加上被乘数**乘以乘数当前位的权重**，所以这里可以用一个简单的优化，因为随着乘数的遍历，权重一直都是左移一位的关系(1-->10-->100->...)，所以直接让被乘数左移即可，而另外为了方便计算乘数的当前位，我们可以直接让乘数右移，把当前位固定在最后一位，这样通过(b&1)就能快速得到这一位的值。\n\n### 代码\n\n```cpp\nint mul(int a,int b){\n    // 取绝对值\n    int aa=a<0?neg(a):a;\n    int bb=b<0?neg(b):b;\n    int ans=0;\n    while(bb){\n        // 如果被乘数当前(最后一位)为1，答案加上被乘数\n        if((bb&1)==1){\n            ans=add(ans,aa);\n        }\n        // 被乘数左移一位，被乘数左移是因为对应的权重不同\n        aa<<=1;\n        // 乘数右移一位，乘数右移是为了方便定位当前计算的位(一直是最后一位)\n        bb>>=1;\n    }\n    // 判断符号位\n    if((a^b)<0){\n        ans=neg(ans);\n    }\n    return ans;\n}\n```\n\n## 除法\n\n### 思路\n\n除法是一个贪心的做法，从高位到低位(1<<31,1<<30,...)依次试除。\n\n### 代码\n\n```cpp\nint div2(int a,int b){\n    int aa=a<0?neg(a):a;\n    int bb=b<0?neg(b):b;\n    int ans=0;\n    for(int i=31;i>=0;i--){\n        if((aa>>i)>=bb){\n            ans=add(ans,1<<i);\n            aa=sub(aa,bb<<i);\n        }\n    }\n    if((a^b)<0){\n        ans=neg(ans);\n    }\n    return ans;\n}\n```\n\n## 完整代码\n\n以上加减乘除四种做法其实都没有考虑到溢出的情况，<del>又不是做数电作业题</del>，<del>如果遇到这么刁难的面试题就拒了吧</del>。\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\nvoid print(int a){\n    for(int i=31;i>=0;i--){\n        printf(\"%d\",(a>>i)&1);\n    }\n    printf(\"\\n\");\n}\nint add(int a,int b){\n    if(b==0){\n        return a;\n    }\n    return add(a^b,(a&b)<<1);\n}\nint add2(int a,int b){\n    while(b!=0){\n        int c=a^b;\n        b=(a&b)<<1;\n        a=c;\n    }\n    return a;\n}\nint neg(int a){\n    return add(~a,1);\n}\nint sub(int a,int b){\n    return add(a,neg(b));\n}\nint mul(int a,int b){\n    // 取绝对值\n    int aa=a<0?neg(a):a;\n    int bb=b<0?neg(b):b;\n    int ans=0;\n    while(bb){\n        // 如果被乘数当前(最后一位)为1，答案加上被乘数\n        if((bb&1)==1){\n            ans=add(ans,aa);\n        }\n        // 被乘数左移一位 被乘数左移是因为对应的权重不同\n        aa<<=1;\n        // 乘数右移一位 乘数右移是为了方便定位当前计算的位(一直是最后一位)\n        bb>>=1;\n    }\n    // 判断符号位\n    if((a^b)<0){\n        ans=neg(ans);\n    }\n    return ans;\n}\nint div2(int a,int b){\n    int aa=a<0?neg(a):a;\n    int bb=b<0?neg(b):b;\n    int ans=0;\n    for(int i=31;i>=0;i--){\n        if((aa>>i)>=bb){\n            ans=add(ans,1<<i);\n            aa=sub(aa,bb<<i);\n        }\n    }\n    if((a^b)<0){\n        ans=neg(ans);\n    }\n    return ans;\n}\nint main(){\n    printf(\"%d\\n\",add(13,58));\n    printf(\"%d\\n\",add2(13,58));\n    printf(\"%d\\n\",sub(13,58));\n    printf(\"%d\\n\",sub(58,13));\n    printf(\"%d\\n\",mul(58,13));\n    printf(\"%d\\n\",mul(-58,13));\n    printf(\"%d\\n\",mul(-58,-13));\n    printf(\"%d\\n\",div2(58,13));\n    printf(\"%d\\n\",div2(-58,-13));\n    printf(\"%d\\n\",div2(-58,13));\n    printf(\"%d\\n\",div2(13,58));\n    return 0;\n}\n```\n","date":"Dec 2, 2020","category":{"name":"算法学习","permalink":"http://127.0.0.1:8080/category/算法学习"},"tags":[{"name":"位运算","permalink":"http://127.0.0.1:8080/tags/位运算"}]},{"title":"字符串最小表示","permalink":"http://127.0.0.1:8080/detail/string-minimun-representation","content":"\n## 思路\n\n给定一个字符串，最小表示指的是循环的字符串中字典序最小的那个。\n\n做法是定义两个指针i=0和j=1，表示正在比较的两个字符下标，定义k表示当前比较的字符串相等前缀长度，当a[i+k]==a[j+k]，k++，当a[i+k]!=a[j+k]，假设a[i+k]>a[j+k]，直接让i+=k+1，因为a[i+1...i+k-1]这部分开头的前缀显然不可能是最小表示，因为对应的有一个a[j+1...j+k-1]与其相等，而a[i+k]又大于a[j+k]，所以显然不是最优。然后注意字符串的循环结构，所以i+k的下标要取模n；如果i和j移动后相等，需要一个往后移一位。\n\n## 代码\n\n```cpp\nint minStrRepresentation(string s){\n    int n=s.size();\n    int i=0;\n    int j=1;\n    int k=0;\n    while(i<n && j<n && k<n){\n        int cmp=s[(i+k)%n]-s[(j+k)%n];\n        if(cmp==0){\n            k++;\n        }else{\n            if(cmp>0){\n                // 加k+1是因为以s[i+k]开头的前缀肯定以s[j+k]开头的前缀\n                i+=k+1;\n            }else{\n                j+=k+1;\n            }\n            if(i==j){\n                j++;\n            }\n            k=0;\n        }\n    }\n    // 因为字典序大的字符指针往后跳\n    return min(i,j);\n}\n```\n\n","date":"Dec 3, 2020","category":{"name":"算法学习","permalink":"http://127.0.0.1:8080/category/算法学习"},"tags":[{"name":"字符串","permalink":"http://127.0.0.1:8080/tags/字符串"},{"name":"最小表示","permalink":"http://127.0.0.1:8080/tags/最小表示"}]},{"title":"康拓展开","permalink":"http://127.0.0.1:8080/detail/permutation-and-cantor-expansion","content":"\n## 康拓展开(全排列排名)\n\n### 思路\n\n康拓展开是求一个全排列是第k小，思路很简单，我们求出比这个排列小的排列的数量，对于a[i]，显然比a[i]小的数都可以放在这个位置，然后后面的位置随便放，方案数就是一个阶乘，当然，因为是全排列，为了避免重复使用数字，所以求的是在i的后面比a[i]小的数的个数。即$rk=b_n(n-1)!+b_{n-1}(n-2)!+...b_10!$，其中$b_i$就是表示在i的后面比a[i]小的数的个数。\n\n朴素做法是O(N^2)，可以用树状数组/线段树来优化计算$b_i$，复杂度为O(NlogN)。\n\n### 代码\n\n```cpp\nint cantor(vector<int>& perm){\n    int n=perm.size();\n    int ans=0;\n    for(int i=0;i<n;i++){\n        int cnt=0;\n        for(int j=i+1;j<n;j++){\n            if(perm[j]<perm[i]){\n                cnt++;\n            }\n        }\n        ans+=cnt*fac[n-1-i];\n    }\n    return ans+1;\n}\n```\n\n## 逆康拓展开(第k个全排列)\n\n### 思路\n\n逆康拓展开是指求第k个全排列，是康拓展开的逆过程，我们依次确定每一位，通过除以阶乘计算有多少个数比a[i]小，通过取模得到下一位所对应的排名，同样的，为了避免重复使用数字，我们用一个标记数组来维护还没用过的数字。\n\n朴素做法是O(N^2)，可以用树状数组/线段树/主席树/平衡树来维护未使用的数字，复杂度为O(NlogN)。\n\n### 代码\n\n```cpp\nvector<int> revCantor(int n,int k){\n    vector<int> vis(n+1,0);\n    vector<int> ans(n,0);\n    int m=k-1;\n    for(int i=0;i<n;i++){\n        int t=m/fac[n-1-i];\n        m-=t*fac[n-1-i];\n        for(int j=1;j<=n;j++){\n            if(!vis[j]){\n                t--;\n                if(t==-1){\n                    ans[i]=j;\n                    vis[j]=1;\n                    break;\n                }\n            }\n        }\n    }\n    return ans;\n}\n```\n\n## 上/下一个全排列\n\n### 思路\n\n感性理解，肯定是在后面找一个比前面某一个位置大的数，交换，之后后面的部分转为顺序排列，为了使交换后的全排列尽量小，满足\"下一个\"，这个在前面被交换的数应该尽量靠后，权重更小，然后逆向思维，  被交换我们尽量从为了比如1 4 2 5 6 3，\n\n感性理解，下一个全排列肯定是从后往前找到一段满足**递增一个，再不断递减到完的后缀**，要交换的显然是这个递增的这个，记为x，因为递减就是表示字典序最大了，要交换过去的数就是从后面找到一个刚好大于x的数，记为y，swap(x,y)，然后将后面递减部分逆序成为递增，类似于进位，后面要恢复到字典序最小的递增，为什么要从后往前，因为我们尽量让交换发生在权重更小的后面，满足**下一个全排列**的要求。\n\n总结起来就是：\n\nnext_permutation的思路：从后往前找到一个i满足a[i]<a[i+1]，此时a[i+1...n-1]是递减的，然后再次从后往前找到第一个大于a[i]的数，两数交换，此时a[i+1...n-1]仍然保持递减，将这一部分逆序即可。\n\nprev_permutation的思路：累死next_permutation，符号相反，即从后往前找到一个i满足a[i]>a[i+1]，此时a[i+1...n-1]是递增的，然后再次从后往前找到第一个小于a[i]的数，两数交换，此时a[i+1...n-1]仍然保持递增，将这一部分逆序即可。\n\n### 代码\n\nSTL库函数\n\n```\nprev_permutation(a,a+n);\nnext_permutation(a,a+n);\n```\n\n```cpp\nvector<int> next_permutation(vector<int>& perm){\n    int n=perm.size();\n    int idx=-1;\n    for(int i=n-1;i>0;i--){\n        if(perm[i]>perm[i-1]){\n            idx=i-1;\n            break;\n        }\n    }\n    if(idx!=-1){\n        for(int i=n-1;i>idx;i--){\n            if(perm[i]>perm[idx]){\n                swap(perm[i],perm[idx]);\n                break;\n            }\n        }\n    }\n    int l=idx+1;\n    int r=n-1;\n    while(l<r){\n        swap(perm[l++],perm[r--]);\n    }\n    return perm;\n}\nvector<int> prev_permutation(vector<int>& perm){\n    int n=perm.size();\n    int idx=-1;\n    for(int i=n-1;i>0;i--){\n        if(perm[i]<perm[i-1]){\n            idx=i-1;\n            break;\n        }\n    }\n    if(idx!=-1){\n        for(int i=n-1;i>idx;i--){\n            if(perm[i]<perm[idx]){\n                swap(perm[i],perm[idx]);\n                break;\n            }\n        }\n    }\n    int l=idx+1;\n    int r=n-1;\n    while(l<r){\n        swap(perm[l++],perm[r--]);\n    }\n    return perm;\n}\n```","date":"Dec 3, 2020","category":{"name":"算法学习","permalink":"http://127.0.0.1:8080/category/算法学习"},"tags":[{"name":"全排列","permalink":"http://127.0.0.1:8080/tags/全排列"},{"name":"康拓展开","permalink":"http://127.0.0.1:8080/tags/康拓展开"}]},{"title":"数位dp(爆搜)","permalink":"http://127.0.0.1:8080/detail/digital-dp-just-dfs","content":"\n## 解决什么问题\n\n此类题目大多数有两个特点: 区间，具有某些特征的数。\n\n比如不含有4和连续的62(hdu2089)，比如含有13并且可以被13整除(hdu3652)，比如满足该数可以整除其数位和(hdu4389)等等...\n\n## 如何解决\n\n区间的问题显然很好处理，通过前缀和来解决，所以问题就是如何求出[1,n]中满足条件的数的个数(有时候0要单独考虑)。\n\n就想标题所说的，数位dp本质就是记忆化爆搜，算法流程基本都是:\n\n- 拆位\n- 定义dp状态\n- 从高位搜索，根据具体题目逻辑枚举当前位，递归边界也是根据逻辑判断是否返回有效值。\n- 再加上记忆化搜索\n\n## 代码框架\n\n首先定义dp状态，第一维通常是指数字的位数(低i位)，其他维度需要根据不同题目定义状态，可以灵活根据空间/时间大小即将状态定义在dp数组里或者写在dfs参数里。\n\n```cpp\nll dp[50][20];\n```\n\n然后拆位，通常是10进制的数，也有2进制或者16进制。\n\n```cpp\nvoid pre(ll x){\n    // 数字x从低位到高位分别存放在dig[0...k-1]中\n    int k=0;\n    while(x){\n        dig[k++]=x%10;\n        x/=10;\n    }\n}\n```\n\n主体搜索函数，其中各个参数:\n\n- idx表示当前枚举的位\n- sta表示已枚举的前缀状态(二进制)，dif表示前缀不同数个数，这两个变量都属于状态变量，要根据不同题目设计不同状态\n- lead表示当前枚举的是否是一串前导零，因为比如当数字上限是4位数，那么对于小于4位数的数来说，在搜索时实际上枚举到的数是有前导零的，比如0001，0082,0123\n- 当dp计算的状态是跟数字结构无关的，比如计算数位总和，那么完全可以不考虑这个前导零，因为0001和1是一样的，但当状态跟结构或者是0的个数有关时，就需要考虑前导零的情况，比如计算不同数位的个数，那么前导零的0是不能算的。\n- 递归时要根据lead && i==0来计算下一位是否还有前导零限制。\n- limit表示这一位的枚举是否有限制，比如题目给的上限是185，那么当最高位枚举0，显然第二位是没有上限的，可以枚举到9，当最高位枚举1，第二位就只能枚举到8\n- 递归时要根据limit && i==up来计算下一位是否有上限\n\n```cpp\nll dfs(int idx,int sta,int dif,bool lead,bool limit){\n    if(idx==-1){\n        // 一个合法数字(这题要排除0)，有时候要单独考虑0的情况\n        // 其实我觉得前导零的命名有点误导性，其实lead为true就是表示现在搜索的前缀是一串0\n        if(dif==k && !lead){\n            return 1;\n        }else{\n            return 0;\n        }\n    }\n    // 记忆化的都是无上限的\n    if(!limit && dp[idx][dif]!=-1){\n        return dp[idx][dif];\n    }\n    // 这一位的枚举上限\n    int up=limit?dig[idx]:15;\n    ll ans=0;\n    for(int i=0;i<=up;i++){\n        // 具体枚举逻辑，前导零不计入不同数字个数\n        if(lead && i==0 || (sta>>i)&1){\n            ans+=dfs(idx-1,sta,dif,lead && i==0,limit && i==up);\n        }else{\n            ans+=dfs(idx-1,sta|(1<<i),dif+1,lead && i==0,limit && i==up);\n        }\n        ans%=mod;\n    }\n    if(!limit){\n        dp[idx][dif]=ans%mod;\n    }\n    return ans%mod;\n}\n```\n\n主函数调用，通常都是通过前缀和的方法来计算区间的值。\n\n```cpp\nll solve(ll x){\npre(x);\nreturn dfs(k-1,0,0,true,true);\n}\nint main(){\n    // 通常dp状态只需要初始化一次\n    memset(dp,-1,sizeof(dp));\n    ll ans=solve(r)-solve(l-1);\n    return 0;\n}\n```\n\n","date":"Mar 28, 2021","category":{"name":"算法学习","permalink":"http://127.0.0.1:8080/category/算法学习"},"tags":[{"name":"dp","permalink":"http://127.0.0.1:8080/tags/dp"},{"name":"数位dp","permalink":"http://127.0.0.1:8080/tags/数位dp"}]},{"title":"约瑟夫环问题","permalink":"http://127.0.0.1:8080/detail/josephur-ring","content":"\n## 思路\n\n约瑟夫环问题是：n个人围成一个环，编号从1到n(或者从0到n-1)，从1开始数(从几开始数不重要，重要的是数到第几个出队)，第k个人出队，然后从下一个人开始重新数，直到剩最后一个为赢家。\n\n我们先把环展开，n个人时，队列为1 2 3 ... n-1 n(n个人，队列1)，淘汰一个人后，下一轮应该从k+1开始报数，也就是k+1 k+1 ... k-1(n-1个人，队列2)，对于这两个队列，胜者的编号应该是一样的，但在两个队列中的位置发生了变化，而对于下面这个队列1 2 3 ... n-2 n-1(n-1个人，队列3)，因为和队列2都是n-1个人，所以显然胜者的位置是一样的，而我们的递归函数f(n,k)其实算的就是队列1(n个人)和队列3(n-1个人)，而当我们计算了队列3，需要转化为队列2之后，再返回给上层函数，这两者的关系就是(x+k)%n，n是当前人数，在迭代做法中应该是i。\n\n注意最后一步，当队列编号从0开始时，可以直接(x+k)%n，当队伍编号从1开始时，应该是(x+k-1)%n+1。\n\n## 代码\n\n### 递归\n\n```cpp\nint josephus(int n,int k){\n    // 递归边界，注意编号是0或者1\n    if(n==1){\n        return 0;\n    }\n    // 注意如果编号从1开始，则是(josephus(n-1,k)+k-1)%n+1\n    return (josephus(n-1,k)+k)%n;\n}\n```\n\n### 非递归\n\n```cpp\nint josephus2(int n,int k){\n    int x=0;\n    for(int i=2;i<=n;i++){\n        x=(x+k)%i;\n    }\n    return x;\n}\n```\n","date":"Dec 2, 2020","category":{"name":"算法学习","permalink":"http://127.0.0.1:8080/category/算法学习"},"tags":[{"name":"约瑟夫环","permalink":"http://127.0.0.1:8080/tags/约瑟夫环"},{"name":"递归","permalink":"http://127.0.0.1:8080/tags/递归"}]},{"title":"这次一定要学会KMP","permalink":"http://127.0.0.1:8080/detail/i-learned-kmp","content":"\n## 概述\n\nKMP是一种快速的字符串匹配算法，字符串匹配的暴力做法中，当匹配到某一个字符时失配，模式串需要重新回到第一个字符串，对应的，主串需要在上一个起点的基础上前进**一个**字符，而KMP解决的问题就在这里，通过预处理，在这里可以让模式串跳**多个**字符，从而加快匹配速度，最终时间复杂度是O(n+m)，n是主串长度，m是模式串长度，其中预处理复杂度是O(m)，匹配复杂度是O(n)。\n\n首先计算一个nex数组(也有叫做fail函数，意思相同，指匹配失败之后的处理)，简单来说，nex[i]表示模式串前缀pat[0...i]的最长相同前后缀长度(可重叠，但不可以是整个子串)，这里也有另一种表示法，nex[0]为-1，然后用nex[i]表示前i的字符，也就是模式串前缀pat[0...i-1]的最长相同前后缀长度。\n\n\n## 匹配\n\n### 思路\n\n先不考虑nex数组的计算，直接考虑匹配，假设采用的是第二种表示方法，也就是nex[i]表示模式串前i个字符的最长相同前缀后缀长度，那么，现在假设匹配到第8个字符串(i=7)失配了，按照上一节提到的，原串不动，模式串跳**多个字符**，那么跳多少呢，就得看此时的nex值，nex[7]表示此时模式串前7个字符的最长相同前缀后缀长度，**注意这7个字符是和原串匹配的**，所以如果nex[7]=3，大概就是`abcdabc`这种情况，那么原串的前7个字符也是`abcdabc`，这时候，很显然，我们把模式串跳到原串的第5个字符，也就是第二段`abc`开始匹配，就可以节省很多没必要的移动了，这里跳的数量就是`i-nex[i]`。\n\n### 代码\n\n```cpp\n// kmp基本查找有几种：查找出现位置(第一次/多次)，查找数量(可重叠/不可重叠)\nint kmp(char* s,int n,char* p,int m){\n    kmp_pre(p,m);\n    // i，j分别是主串和模式串比较字符下标\n    int i=0;\n    int j=0;\n    while(i<n && j<m){\n        printf(\"%d %d\\n\",i,j);\n        // 这里j=-1表示没有相同前后缀了，所以实际上应该是i++;j=0;只是这里合并了分支\n        if(j==-1 || s[i]==p[j]){\n            i++;\n            j++;\n        }else{\n            j=nex[j];\n        }\n        // 匹配个数\n        // if(j==m){\n           // 统计数量，多次出现的位置同理\n           // cnt++;\n           // 可重叠\n           // j=nex[j];\n           // 不可重叠\n           // j=0;\n        // }\n    }\n    // 匹配\n    if(j==m){\n        // 返回第一个出现位置\n        return i-j;\n    }\n    return -1;\n}\n```\n\n## 预处理\n\n## 思路\n\n解决了匹配，剩下的就是如何求出nex数组，有两种情况。\n\n第一种，比如abaabaaba**a**ba**a**，我们已经求出了nex[12]=9，要求nex[13]，就需要比较pat[12]和pat[9]，这里两个字符相等，都是a，刚好可以接上，所以nex[13]=nex[12]+1。\n\n第二种，比如bacababac**a**ba**c**，我们已经求出了nex[12]=6，要求nex[13]，同样需要比较pat[12]和pat[6]，这里字符c和b是不相等的，那说明我们无法直接接上nex[12]对应的前缀pat[0...5]，所以我们尝试把前缀缩短，缩短有个重要的前提，我们必须保证我们的字符pat[12]能够接上后缀，因此我们并不是把前缀从pat[0...5]缩短到pat[0...4]，**我们需要保证缩短后的前缀pat[0...k]和pat[12]之前的某一段子串是相同的**，所以我们直接跳到nex[6]即可(<del>文字解释非常无力</del>)，比如bacababacabac，一开始我们尝试pat[12]和后缀pat[6...11]拼接，拼接失败，因为pat[12]不等于pat[6]，所以前缀pat[0...5]不能延伸作为nex[13]，我们考虑缩短到，根据nex[6]=2，我们将前缀缩短到pat[0..1]，此时pat[12]要拼接的后缀对应的就是pat[10...11]，拼接成功，所以nex[13]=2+1=3，总之就是抓住nex数组的定义，**前缀和后缀相同，那么前缀的前缀(特指nex数组对应的最大相同前缀，后缀同理)和后缀的后缀也相同**，不断尝试拼接后缀，直到可以拼接上或者nex值为-1。\n\n一个<del>类似于并查集路径压缩的</del>小优化：比如模式串是abab，主串是abacababc，当匹配到s[3]和p[3]，失配，模式串向右移动两位，然后比较s[3]和p[1]，但是实际上这两个肯定失配，因为从模式串abab来看，当某一个字符p[i]失配，下一个要匹配的字符肯定是p[nex[i]]，而对于这个模式串的p[3]，p[3]=p[nex[3]]，所以显然失配，为了避免这些无谓的比较，采用一种类似压缩路径的方法，当计算nex[3]时，也就是即将nex[++i]=++j，i是当前比较字符，j是前缀长度，我们继续看下一个字符，也就是p[3]，如果p[3]=p[j]，那么我们就要避免nex[++i]=nex[++j]的出现，而是使用nex[++i]=nex[++j]，这里可以这么写是因为nex[++j]已经在之前迭代求出来，所以这相当于一个非递归的路径压缩，这样子当失配的时候，因为nex[3]是-1了，所以模式串不再是右移到p[1]。\n\n### 代码\n\n代码乍一看和思路不太一样，其实是经过了一些写法上的优化。\n\n```cpp\nvoid kmp_pre(char* p,int m){\n    // i初始化为0表示要比较的字符，也就是要求nex[i+1]\n    int i=0;\n    // j表示尝试拼接的后缀长度(也等于前缀长度)，所以也是要比较的字符下标\n    // 初始化为-1，因为一开始前缀为空\n    int j=-1;\n    nex[0]=-1;\n    while(i<m){\n        // j==-1只有可能是前缀为p[0]，此时nex值为0，合并分支\n        if(j==-1 || p[i]==p[j]){\n            // 因为要求的是nex[i+1]，之后求下一个，所以是++i\n            // j是前缀长度，拼接上所以是j+1\n            if(p[i+1]==p[j+1]){\n                // 优化\n                nex[++i]=nex[++j];\n            }else{\n                nex[++i]=++j;\n            }\n        }else{\n            j=nex[j];\n        }\n    }\n}\n```\n## 拓展\n\n### 周期和border\n\n来源金策的字符串ppt，省去其中看起来比较复杂的定义。\n\nborder其实就是字符串的相同前后缀长度，所以nex数组其实求的就是前缀的最长border。\n\n周期是子串的长度，且该可以通过重复构成原串(可能不是完整的)。\n\n比如abaaaba，周期是4(abaa)，6(abaaab)，7(abaaaba)，border是aba，a和空串。\n","date":"Dec 2, 2020","category":{"name":"算法学习","permalink":"http://127.0.0.1:8080/category/算法学习"},"tags":[{"name":"KMP","permalink":"http://127.0.0.1:8080/tags/KMP"},{"name":"字符串","permalink":"http://127.0.0.1:8080/tags/字符串"}]},{"title":"MySQL是怎样运行的：从根儿上理解MySQL","permalink":"http://127.0.0.1:8080/detail/understanding-mysql-from-root","content":"\n## 0\n\n1-3章是MySQL一些基础知识和相关操作，**4-7章是索引相关**，8-9章数据目录和表空间先大致了解即可，**10-11章单表访问和多表连接，涉及到很多索引的内容**，12-14章知道MySQL会利用统计信息进行基于成本和基于规则的查询优化即可，15-17章关于Explain命令和optimizer trace表的应用，在实际使用中可能会比较重要，18章是Buffer Pool，简单了解原理，**19-23章是日志和故障恢复**，**24-25章是事务，MVCC，并发控制，锁等面试常考点**。\n\n## 1 MySQL概述\n\n- MySQL是C/S架构，客户端将查询语句发送给服务端，服务端处理一个请求大致需要经过三部分：连接管理，解析优化，存储引擎。\n- 客户端和服务端的连接本质上也是进程通信，最常用的方法是TCP/IP套接字通信，而对于Linux本地连接，还可以使用Unix域套接字(可以简单理解为网络和本地的socket)。\n- 解析优化包括对SQL语句进行词法分析，语法分析，语义分析之后生成查询树(查询计划)，之后对查询计划进行查询优化处理，得到实际的物理查询计划，再通过存储引擎查询数据。\n- 存储引擎的作用是存储和管理实际数据，对外提供存取数据的接口，MySQL支持多个存储引擎，默认是InnoDB，此外比较常用的还有MyISAM和MEMORY，可以通过`SHOW ENGINEs;`查看支持的存储引擎，可以给不同数据库和不同表设置使用不同的存储引擎。\n\n## 2 启动选项和系统变量\n\n- 客户端最常用启动选项：-u(用户名)，-p(密码)，-h(主机名)，-P(端口)，注意短形式的选项后面可以不加空格(-p除外，必须加空格)，长形式的选项大多后面要加等号，且之间不能有空格。\n- 将选项写入到配置文件，MySQL会按照一些路径顺序查找配置文件，通常(docker安装的MySQL)默认配置文件放在`/etc/mysql/my.cnf`里。\n- 配置文件里可以分为不同的组，例如[server]，[client]，可以将配置应用于不同的MySQL程序。\n- 多个配置文件的同一配置，以顺序最后的配置文件为准；同一配置文件多个组的同一配置，以最后出现的组为准；既出现在配置文件里又出现在启动选项里的配置，以启动选项为准；在启动时可以指定`default-file`设置只读取某个配置文件。\n- 系统变量包括例如`max_connections`表示同时允许连接的最大客户端数量，例如`default_storage_engine`表示默认存储引擎，大多数系统变量可以通过启动选项设定，也可以在运行时修改。\n- 大多数系统变量都具有GLOBAL和SESSION不同的作用域，修改系统变量时默认作用域是SESSION。\n- 状态变量是描述数据库程序运行状态的变量，比如`Threads_connected`表示当前的客户端连接数，状态变量不允许自行设置。\n\n## 3 字符集和比较规则\n\n- MySQL支持很多字符集，最常用的是utf8(阉割版utf8，只用1-3个字节表示一个字符)和utf8mb4，可以通过`SHOW CHARSET [LIKE xxx]`查看字符集，一个字符集包括多种比较规则，可以通过`SHOW COLLATION [LIKE xxx]`查看比较规则。\n- MySQL支持服务器，客户端，表，列四个级别的字符集和比较规则的设置。\n- 在客户端和服务端通信的过程中，会进行多次字符集的转化，所以为了方便和避免乱码的产生，通常把`character_set_client`，`character_set_connection`和`character_set_results`这三个系统变量设置为同一个字符集，可以使用一条命令`SET NAME 字符集名`来设置。\n\n## 4 InnoDB记录结构\n\n- InnoDB以页作为磁盘和内存交互的基本单位，页大小为16KB，一页包括多个行记录。\n- InnoDB有4种行记录格式，目前默认和通用的是COMPACT行格式。\n- 一个InnoDB行包括额外数据和真实数据，额外数据包括变长字段的长度列表，NULL值列表和记录头信息。\n- 变长字段的长度只保存非NULL(实际数据)的变长字段，逆序存放(因为遍历链表的指针会定位在额外数据和真实数据之间，逆序存放方便更快找到字段长度)，字段长度1(不超过127)或2个字节。\n- NULL值列表二进制位压缩保存允许NULL值的字段，同样是逆序存放。\n- 记录头信息固定5个字节，包括删除标记，下一条记录(按主键排序而不是插入顺序)相对位置，记录类型(普通记录，页虚拟最大最小记录，B+树非叶子节点记录)，是否是非叶子节点的最小记录标记，记录所在分组的记录数，记录在堆中的位置编号。\n- 记录的真实数据中除了用户定义的列，InnoDB还会自动添加transaction_id，roll_pointer和row_id(如果没有定义主键)三个列。\n- 当一个列(VARCHAR，TEXT，BLOB等)存储了大量数据时，可能会发生行溢出(具体情况再分析)的现象。\n\n## 5 InnoDB数据页结构\n\n- InnoDB设计了多种不同的页，通常所说的数据页其实是索引(INDEX)页。\n- 一个数据页包括7部分的内容，分别是File Header和File Trailer(不同类型的页面都有)，Page Header和Page Directory(数据页专有信息)，Infimum和Supremum(最小和最大虚拟记录)，User Records(用户实际记录)和Free Space(空闲空间)。\n- File Header38字节，包括页面校验和，页号，逻辑上下一个页号，页类型等，所属表空间以及LSN相关的属性。\n- File Trailer8个字节，包括页面校验和和LSN，当页面刷回磁盘中断导致页面不完整，因为File Header肯定是先写入磁盘，所以可以根据两者的校验和来判断同步是否出错。\n- Page Header56个字节，包括Page Directory的slot数量，Free Space的起始地址，记录总数(包含和不包含已删除和虚拟记录)，删除链表首地址，插入方向(主键增加为右)，一个方向连续插入数量，B+树索引的相关信息等。\n- Page Directory的每个slot记录记录分组(规定最小记录单独一个组，最大记录分组记录数1-8，其他分组记录数4-8)之后，每个组的最大记录地址偏移量，因此可以通过二分法定位到记录所在分组，再通过记录链表遍历查找。\n- User Records中将记录按主键大小排序，通过每个记录的next_record属性组织成一个单链表，并且维护了一个已删除的记录链表。\n\n## 6 B+树索引\n\n- **简单来说**，索引通常指B+树索引，就是将页面组织成B+树的形式，其中**非叶子节点页面**类似Page Directory，但是指向的是页面而不是记录(即页面中的记录不是实际数据记录而是**保存了索引列，主键列和对应页号**)，而叶子节点页面是存放实际数据，当定位到某个**叶子节点**，通过**二分查找Page Directory**定位到某个记录组，再**顺序遍历记录组**中的记录链表，查找到具体数据。\n- 聚簇索引是指记录数据按照主键大小顺序，完整地保存在B+树叶子节点中，InnoDB会自动创建聚簇索引，且该索引其实也是正式InnoDB数据的存储组织方式，也就是所谓的\"索引即数据，数据即索引\"。\n- 二级索引是指叶子节点只保存记录的主键，因此还需要一次\"回表\"，通过聚簇索引得到实际数据。\n- 联合索引是指按多个列建立索引，按顺序先按第一个列排序，相同的值再按第二个列排序，以此类推。\n- B+树索引的根节点自诞生起便不会再移动位置。\n\n## 7 B+树索引的使用\n\n- 索引不易过多，使用索引在时间(维护B+树的平衡，有序，双向链表的维护，页面分裂和回收等)和空间(一个页面16KB，一棵B+树索引由很多个页面组成)上都需要代价。\n- 全值匹配是指搜索条件都是等于，且所有列都是索引列，顺序不影响是否使用索引(查询优化器会处理)。\n- 对于联合索引，搜索条件各个列必须是索引中从左连续的列，因为联合索引是按顺序一列一列排序的。\n- 对于字符串类型的列，搜索前缀也可以用到索引快速定位。\n- 对于范围的搜索，只有当搜索条件列是索引最左列才有效(其他索引列是在左边索引列值相同的情况下才排序)，或者左边的列全值匹配，再范围搜索右边的列(这些列必须是连续的，注意联合索引的定义)。\n- 索引本身就是一个排序结构，所以可以用于ORDER BY子句，这里顺序会影响是否使用索引。\n- 当ASC，DESC混用时不会使用索引。\n- 带有非索引列的搜索加上ORDER BY，不会使用索引(因为必须先WHERE搜索后才能排序)。\n- 排序列如果使用了函数或者其他表达式，也不会使用索引。\n- 类似于排序，索引也能用于分组。\n- 使用二级索引+回表，有时候效率反而不如全表扫描，例如当要回表的数据特别多，就需要在访问聚簇索引时使用大量随机IO。\n- 综上，通常只对需要搜索，排序，分组的列建立索引，且要考虑列的值域，值域太小建立索引的效果也不好，同时列的类型也要尽量小，如果可以的话，让数据库自动生成自增主键，避免叶子节点过多的分裂和移位造成性能损耗。\n\n## 8 数据目录\n\n- 可以通过`SHOW VARIABLES LIKE 'datadir';`查看数据目录。\n- 每一个数据库都在数据目录中对应了一个文件夹，在MySQL8.0之前，每个表会有对应一个`.frm`文件描述表结构，MySQL8.0之后，表结构和数据统一放在`.ibd`文件里(具体细节参考MySQL8.0的新版本特性)。\n- 表空间是一个逻辑上的概念，一个表空间包含多个页，在物理层面上可能包括一个或多个文件。\n- 系统表空间默认对应一个名为`ibdata1`，大小为12M的文件，包括了一些系统表数据，例如数据字典，日志，事务等相关信息。\n- 每个表对应一个独立表空间，也就是数据库目录下的一个`.ibd`文件。\n- MySQL四个主要的系统数据库，mysql存储了MySQL的用户账户，权限信息，存储过程定义，帮助信息和时区信息等；information_schema存储了所有其他数据库的元数据，比如有哪些表、哪些视图、哪些触发器、哪些列、哪些索引等；performance_schema存储了MySQL运行过程中的一些状态信息，比如统计最近执行了哪些语句，在执行过程的每个阶段都花费了多长时间，内存的使用情况等；sys通过视图的形式将information_schema和performance_schema结合起来，更方便了解MySQL服务器的一些性能信息。 \n\n## 9 InnoDB表空间\n\nTODO\n\n这一章实在无能为力，暂时只粗略看了一遍。\n\n- 一个区(extent)就是物理上连续的64个页，按区为单位分配，虽然会出现比较多的碎片造成空间浪费(MySQL通过其他方法尽量解决)，但可以消除很多随机IO。\n- 一个段(segment)是逻辑上区的集合，比如一个索引会生成两个段，叶子段和非叶子段。\n\n## 10 单表访问方法\n\n- 访问方法/访问类型是指执行查询语句的方式，包括const，ref，ref_or_null，range，index，all等。\n- const: 主键列或者Unique二级索引列和常量(非NULL，NULL不是唯一值)进行等值比较。\n- ref: 普通二级索引列和常量(包括NULL)进行等值比较，当索引是联合索引，不需要所有索引列，只要最左连续索引列是与常量比较就可以。\n- ref_or_null: 二级索引列和常量进行等值比较或者判断是否NULL。\n- range: 利用索引进行范围匹配，当搜索条件中出现`>`，`<`，`BETWEEN`，`IN`(看成多个单点区间)，`IS NULL`等。\n- index: 遍历二级索引，但是不需要回表，直接将索引列的值加入结果集。\n- all: 全表扫描，对于InnoDB也就是直接扫描聚簇索引(数据)。\n- 考虑一种情况，假设对key2列建立二级索引，对于查询`SELECT * FROM single_table WHERE key2 > 100 AND common_field = 'abc';`，先不考虑common_field，而是根据key2的索引，range查询得到一个数据范围，再遍历查找第二个条件；而对于`SELECT * FROM single_table WHERE key2 > 100 OR common_field = 'abc';`，无法使用索引(因为不满足索引列key2的条件的数据仍可能满足第二个条件)，只能遍历所有数据。\n- **人肉**判断使用哪个索引好的一个方法：先找出所有可能使用的索引，对于每个索引，将复杂的查询条件简化，也就是将无关的条件替换为True或者False(除了矛盾条件，其他都为True)，化简得到的查询条件就是使用该二级索引第一步能查到的数据，如果为True，说明需要扫描整个索引。\n- 在某些特殊情况下会对多个二级索引进行合并，比如多个条件(二级索引等值匹配(只有等值匹配才能让主键集合有序)或者主键索引范围匹配)AND或者OR，且条件列都可以是二级索引列，可以分别查出主键，求交集/并集之后再统一回表查找聚簇索引。\n- 对于二级索引不是等值匹配的情况(此时查到的主键集合无序的)，采用Sort-Union的索引合并方法，将查出来的主键集合排序，再合并回表。\n\n## 11 表连接\n\n- 两个表的连接普通做法: 根据单表条件查找得到第一个表的部分数据作为驱动表(外表)，另一个表作为被驱动表(内表)，对于驱动表的每个记录，都在被驱动表中找到符合多表条件的记录，加入结果集。\n- 连接根据\"驱动表的记录在被驱动表中找不到匹配记录时是否加入结果集\"分为外连接和内连接，在MySQL中，外连接又根据\"以左侧或者右侧的表作为驱动表\"分为左外连接和右外连接。\n- 外连接中使用`ON 条件`作为计算得到中间表的条件，使用`WHERE 条件`作为最后对中间表的过滤条件，在内连接中`ON`和`WHERE`等同。\n- 外连接语法: `SELECT * FROM t1 LEFT/RIGHT [OUTER] JOIN t2 ON 连接条件 [WHERE 过滤条件];`。\n- 内连接语法: `SELECT * FROM t1(可由逗号分割多个表) [INNER/CROSS] JOIN t2 ON/WHERE(可同时使用，含义相同) 连接条件;`。\n- 嵌套循环连接: 最简单粗暴的连接方式，遍历驱动表，然后多次在被驱动表中查找匹配的记录。\n- 可以使用索引加快对被驱动表的查找。\n- 基于块的循环嵌套连接: 将驱动表按块读取到内存(join buffer)，之后遍历被驱动表的每一条记录，都和块中的所有驱动表记录匹配，最好的情况是join buffer可以容纳整个驱动表，那么只需要读取一次驱动表，遍历一次被驱动表即可。\n- MySQL8.0之后才支持Hash Join，所以本书中没有提到，Hash Join简单来说就是先对外表简历HashMap，然后只要遍历一次内表，就能从HashMap中找到匹配的数据。\n\n## 12 基于成本的查询优化\n\n## 13 InnoDB统计数据\n\n## 14 基于规则的查询优化\n\n- 条件化简: 包括移除不必要的括号，常量传递，等值传递，移除永真或永假的条件，表达式计算，合并HAVING和WHERE子句等。\n- 外连接消除: 如果查询中对被驱动表进行空值拒绝(NULL-reject)，这样外连接就能和内连接互相转换，查询优化器就可以评估各种不同连接顺序的成本，选择最优的连接顺序。简单来说，就是把一些可以转化为内连接的外连接转化为内连接。\n- 子查询优化: TODO\n\n## 15 16 Explain命令\n\n## 17 optimizer_trace表\n\n## 18 Buffer Pool\n\n- Buffer Pool默认大小128M，可以在启动时通过`innodb_buffer_pool_size`选项设置。\n- Buffer Pool对于每个缓存页，都会有对应的一个控制块保存页对应的一些控制信息，控制块按顺序放在Buffer Pool的前边，缓存页放在后边。\n- 控制块构成一个free链表，维护空闲的缓存页。\n- 维护一个哈希表，key为表空间+页号，value为缓存页，可以快速判断页面是否在Buffer Pool中。\n- 当修改了缓存页的数据，就产生了脏页(dirty page)，出于性能考虑无法立即同步，所以类似的维护一个控制块的flush链表表示脏页，再定义刷回磁盘。\n- Buffer Pool的空间是有限的，当已经没有空闲的缓存页时，就需要把其中某些页从Buffer Pool中移除，把新的页放进去，通常采用LRU算法维护一个LRU缓存链表。\n- 简单的LRU链表原理: 当访问某一个缓存页面，就把该页对应的控制块移到LRU链表头部(可能是添加或者移动)，这样当空闲缓存页不足时，就从LRU链表尾部淘汰即可。\n- 上面的LRU链表存在两个问题: InnoDB会对周围的内存页进行预读(read ahead)，可能出现大量的预读页面会把LRU链表的其他热门页面挤到尾部；偶尔一次全表扫描的情况会导致Buffer Pool的页面全换了一遍。\n- InnoDB将LRU链表分为热区(young)和冷区(old)，old区所占比例可以通过系统变量`innodb_old_blocks_pct`来设置。然后规定，页面初次加载到Buffer Pool时，控制块是加到链表old区的头部，而不影响young区的热点数据；而对于权标扫描，通过设置系统变量`innodb_old_blocks_time`(默认1000ms)，对于一个在old区的页面，当后续访问时间和第一次访问时间相差不超过这个系统变量时间时，该页面就不会被移动到young区，因为全表扫描的次数不多，而在一次全表扫描中，通常对同一个页面第一次和最后一次访问相差时间也不会超过1000ms。\n- LRU还有很多很多其他的优化策略，但无论如何优化，最终目的都是要优化Buffer Pool的缓存命中率。\n- 后台线程脏页刷回磁盘主要有两种方式，一种是从old区尾部开始扫描，遇到脏页就刷回磁盘，一种是直接遍历flush链表。如果后台线程刷新脏页速度较慢，就会导致需要用户线程同步去刷新，这会大大降低性能。\n- 可以通过设置`innodb_buffer_pool_instances`选项来设置多个Buffer Pool，各个Buffer Pool之间相互独立，独立管理各种链表，多线程访问时不冲突。\n- 通过`SHOW ENGINE INNODB STATUS;`可以查看Buffer Pool状态信息。\n\n## 19 事务\n\n- 事务指能满足ACID特性的一个或多个数据库操作。\n- 事务的状态包括active(还在执行)，partially committed(事务所有操作完成，还没刷回磁盘)，failed(当处于以上两种情况，遇到某些错误或者人为停止)，aborted(failed状态下执行回滚，恢复到事务执行之前的状态)，committed(所有修改数据都同步到磁盘)。\n- MySQL有一个系统变量`autocommit`默认为ON，表示如果不显式地开启事务，那么每条语句都会当做一个事务并自动提交。\n- 在事务中，虽然没有输入COMMIT，当如果输入了某些特殊语句也会导致隐式提交，包括：修改数据库对象的DDL，事务控制或者锁表解锁的语句等。\n- 在事务中定义保存点(save point)，在ROLLBACK回滚时可以指定回滚到哪一个保存点，相关语句分别是`SAVEPOINT xx`，`ROLLBACK TO xx`和`RELEASE SAVEPOINT xx`。\n- 事务只有在第一次真正修改记录(INSERT，UPDATE，DELETE)时才会分配一个事务id，而不是事务开启，并且初始化为0。\n\n## 20 21 redo日志\n\n- **redo(重做)日志是为了保证事务的持久性**，记录下对数据进行的修改，及时系统发生了故障导致Buffer Pool的数据修改没能刷回磁盘，也能够根据redo日志重新修改数据。注意，这些redo日志就一定要及时刷回磁盘，比起将数据修改的页面刷回磁盘，这样做的好处一是redo日志占用空间小，二是redo日志是顺序写入磁盘。\n- redo日志的基本结构是类型，表空间ID，页号和具体内容。\n- 简单的redo日志: 例如MLOG_1BYTE(type为1)表示在某个页面的某个偏移量写入(无论是插入还是更新，本质都是二进制数据的写入)1字节，在上面提到的基本结构中的具体内容里就需要保存偏移量和写入的数据；MLOG_2BYTE(type为2)同理，MLOG_WRITE_STRING(type为30)表示在某个页面的某个偏移量写入某个长度的数据等等。这种日志也叫物理日志，保存的是物理上实际的数据，恢复时直接写入即可。\n- 其他redo日志: 有时候一个语句会影响到很多个页面的修改，比如插入一条记录，可能会导致相关的索引页发生大量的修改，这时候采用另一种类型的redo日志，例如MLOG_REC_INSERT(type为9)表示插入一条非紧凑行格式的记录，MLOG_COMP_REC_INSERT(type为10)表示插入一条紧凑行格式的记录等等。这类日志称为逻辑日志，保存的是一个逻辑的动作，恢复时要根据其中保存的一些信息执行一些对应的函数才能恢复。\n- Mini-Transaction: 有些操作(比如插入一条记录)可能会产生多个redo日志，为了保证原子性，必须将这些redo日志作为一个组，恢复时要不全执行，要不全不执行，这就是一个Mini-Transaction(mtr)，具体实现上，对于单个redo日志的Mini-Transaction，为了节省空间，使用了日志记录中的一个比特位来表示，对于多个redo日志，在最后额外添加一个MLOG_MULTI_REC_END的特殊日志。\n- redo日志存放在专门的页里，又称为block，一个block512字节，包括了16字节(12+4)的一些元数据和校验和。\n- redo日志有一块专门的缓冲区redo log buffer，分为多个block，可以通过选项`innodb_log_buffer_size`来指定buffer大小，默认为16MB(MySQL5.7)。\n- 同一个mtr的redo日志组会先暂存在一个地方，当mtr结束，才将整个日志组插入到buffer中。\n- log buffer刷回磁盘的时机大概包括log buffer空间不足时，事务提交时(数据页修改可以不立即刷回磁盘，但日志要马上刷回磁盘)，后台线程定时刷，关闭服务器时，做checkpoint时。\n- 磁盘上的redo日志包括一个文件组(ib_logfile[数字])，循环写，当最后一个文件写满，又会重新转到第一个文件写，通过checkpoint的方式保证日志不会被覆盖。\n- 日志序列号(Log Sequence Number，lsn)是一个全局变量，初始值为8704。每次写入一组redo日志，lsn的增长要考虑log buffer block的结构，比如第一组redo日志写入，lsn就增加一个block的头部字节数(12)，再加上日志的字节数，如果日志大小超过一个block，还需要增加适当的block尾部字节数。\n- 数据库维护了两个全局变量，buf_next_to_write(write_lsn)表示目前刷回磁盘的lsn，而flushed_to_disk_lsn表示完成fsync，真正写入到文件的lsn。\n- flush链表中，每个控制块会维护页面第一次修改时的LSN和最新一次修改的LSN，然后根据old_lsn从最近到最远排序(修改已经在flush链表中的页面不会再次移动，只会更新new_lsn)。\n- redo日志是为了保证系统崩溃会能将脏页刷回磁盘，如果脏页已经刷回磁盘了，那么对应的redo日志就可以不用了。\n- 维护一个checkpoint_lsn表示可以覆盖的redo日志lsn，初始值也是8704，当一个脏页被刷回磁盘，就需要重新计算checkpoint_lsn，该值也等于当前flush链表里最早修改的脏页(表头)对应的old_lsn(在此之前的所有redo日志都可以覆盖了，因此直接让checkpoint_lsn等于该值)。同时每做一次checkpoint，还要将checkpoint_lsn，对应redo日志文件组(ib_logfile)的偏移量和checkpoint编号写入到日志文件的管理信息里(ib_logfile前4个block)。\n- 各个lsn之间的简单关系: checkpoint_lsn之前是脏页已经刷回磁盘的，这部分log file可以被覆盖，checkpoint_lsn到flushed_to_disk_lsn之间是日志已经刷回磁盘log file的，flushed_to_disk_lsn和lsn之间是log buffer还没刷回log file的日志。\n- 崩溃时如何恢复: 首先确定恢复的起点就是最近的checkpoint_lsn，终点就是最后一个没写满的block(通过元数据确定)，恢复就是按照日志顺序执行redo操作，可以有一些优化包括把对同一个页面的修改日志先用一个哈希表存起来，再一起做，减少很多页面随机IO，还有就是对于一个页面，如果File Header中的最近一次被修改的LSN(FIL_PAGE_LSN)大于checkpoint_lsn(注意这里说的页面都是指磁盘页面，而不是Buffer Pool，都崩溃了，肯定只能根据已经在磁盘的数据来恢复)，说明是在checkpoint之后被写入磁盘的，那么相当于就可以把checkpoint_lsn更新到FIL_PAGE_LSN了，在此之前的redo日志不需要再次redo。\n\n## 22 23 undo日志\n\n- **undo(撤销)日志是为了保证事务的原子性**，当事务执行到一半出现错误或者手动rollback时，可以根据undo日志执行逆操作进行回滚。\n- (基本上)每个事务都会分配一个事务id，对应改动的记录的隐藏列transaction_id，生成算法类似row_id，可以保证递增，另外一个隐藏列roll_pointer则是指向对应undo日志(保存了该记录的旧版本)的一个指针。\n- INSERT操作对应的undo日志: 日志类型为TRX_UNDO_INSERT_REC；其中最主要是保存了该记录主键每个列的存储空间大小和实际值，有了主键信息，回滚时直接删除该记录即可，此时记录roll_pointer指向该日志记录。\n- DELETE操作对应的undo日志: 日志类型为TRX_UNDO_DEL_MARK_REC；记录删除包括两个阶段，第一个阶段是将页面中记录链表的记录打上一个删除标记，第二阶段是当事务提交之后，会有专门的线程将记录真正删除(移到删除链表里)。所以删除操作只需要考虑第一阶段对应的undo日志(事务提交后就肯定不需要回滚了，如果出故障没写入硬盘，那是redo日志的事了)。其中主要包括old_trx_id和old_roll_pointer表示旧记录的trx_id和roll_pointer(方便找到旧记录对应的undo日志，比如该记录是先插入，再删除，那么这个old_roll_pointer就指向了插入的undo日志，这样子这一系列的undo日志就构成了一个版本链)，主键各列信息和索引各列信息(用于第二阶段对记录真正的删除)。\n- UPDATE操作对应的undo日志: 对于不更新主键的情况，日志类型为TRX_UNDO_UPD_EXIST_REC，类似删除的undo日志，多了一个属性记录被更新列更新前的值(用于undo，而对于上面DELETE操作，只需要根据主键在删除链表中找到即可undo)。对于更新主键的情况，等同于做一次删除(TRX_UNDO_DEL_MARK_REC)和一次插入(TRX_UNDO_INSERT_REC)。\n- undo日志基本上有两个作用: 回滚和MVCC(多版本并发控制)。\n\n## 24 事务隔离级别与MVCC\n\n- 事务的并发执行如果不加以控制，就会出现一些问题: 脏写(Dirty Write，一个未提交事务修改的数据被另一个事务的修改所覆盖)，脏读(Dirty Read，读了另一个未提交事务修改的数据，然后该事务回滚了，读到的就是脏数据)，不可重复读(Non-Repeatable Read，在一次事务中多次读了一个数据，且该数据被其他事务修改并提交，导致多次读取到不同的值)，幻读(Phantom，类似不可重复读，不同在于其他事务对数据进行了插入(强调是插入，如果是删除或者修改都属于不可重复读)而不是修改)。\n- 对应的，SQL标准里定义了四种隔离级别，不同的隔离级别可以防止不同的事务并发所导致的问题发生。为了方便记忆，我将问题和隔离级别放在一起: **脏写 | (未提交读) | 脏读 | (已提交读) | 不可重复读 | (可重复读) | 幻读 | (可串行化)**，括号内是隔离级别，左边表示该级别下不会发生的问题，右边表示会发生的问题。在满足ACID的RDBMS中，脏写是绝对不允许发生的，无论是什么隔离级别。\n- 实际上不同RDBMS对隔离级别的支持是不同的，并且一些隔离级别的实现并不同于SQL标准，比如MySQL的默认隔离级别是可重复读，并且是可以防止幻读的发生的。\n- 在一条记录经过多个事务的多次更新后，产生了一个版本链，头结点就是当前记录，然后roll_pointer依次指向前一个版本的undo日志，记住每个undo日志里也保存了生成该日志的事务id。\n- 对于已提交读和可重复读，必须保证只能读到已提交的事务修改的数据，而不一定是数据的最新版本，因此核心问题是要能够判断版本链中哪个版本是当前事务可见的。\n- ReadView: 当一个事务生成一个ReadView，对应维护了4个值，包括m_ids表示生成时刻活跃(未提交)的事务id，min_trx_id表示m_ids的最小值，max_trx_id表示下一个系统分配的事务id(全局维护的值)，creator_trx_id表示生成该ReadView的事务id(注意区分事务开启的时间和生成ReadView的时间，可能出现事务id是1(如果该事物还没进行修改操作，为0)，下一个分配事务id是100)。\n- 这样在访问某一条记录时，只需要根据以下规则判断记录的某个版本是否可见，而如果不可见，就根据版本链回到上一个版本继续检查: 如果该版本事务id等于creator_trx_i，说明是访问当前事务自身修改过的记录，可以访问；如果该版本事务id小于min_trx_id，说明在生成ReadView前已提交，可以访问；如果该版本事务id大于等于max_trx_id，说明该版本事务在生成ReadView之后才开启，不可以访问；如果该版本事务id大于等于min_trx_id而小于max_trx_id，需要判断是否在m_ids中，如果是，说明创建ReadView时该事务还处于活跃状态，不能访问，否则已提交可以访问。\n- 在MySQL中，已提交读和可重复读两种不同隔离级别的的一个非常大的区别就是它们生成ReadView的时机不同。已提交读是在每一次查询数据前都独立生成一个ReadView(显然不可重复读，两个ReadView状态可能不同)，可重复读是在第一次读取数据前生成一个ReadView。\n- MVCC处理的只是事务并发的读-写问题，可以避免加锁，提升系统性能。\n- insert undo在事务提交之后就可以删除了(即insert undo只在事务中的回滚起作用，因为insert undo对应的是删掉这个记录，而MVCC要的是找到记录的旧版本，而insert undo对应的就已经是最旧版本了，指向的undo日志已经不是记录了)，而update undo和delete undo还需要支持MVCC，不能立即删除。\n\n## 25 锁\n\n- 事务并发执行可能带来各种问题，可以分为三类: 读-读，读-写和写-写。\n- 读-读: 同时读不会有影响，允许这种操作。\n- 写-写: 会发生脏写的问题，所以需要通过锁来实现让多个事务顺序执行，锁是一个内存结构，简单来说，当一个事务想修改一条记录，先查看内存中是否有和该记录关联的锁结构，如果没有，生成一个锁(比如包括事务信息和is_waiting表示是否等待中)与记录关联，加锁成功，如果此时另一个事务也要修改该记录，查看发现已经有锁，也生成一个锁与记录关联，但is_waiting属性为true，加锁失败，只能等待。\n- 读-写: 不同的隔离级别可能会产生脏读，不可重复读，幻读的问题，有两种解决方案: 一种是读操作利用MVCC读旧版本，写操作利用加锁写最新版本，这种方案性能更高；另一种操作当业务需求必须读最新版本数据，那么读和写都要采用加锁的方式。\n- MySQL中的一致性读: 也叫快照读，或者一致性无锁读，采用MVCC进行读操作，无需加锁。所有普通的SELECT操作在已提交读和可重复的的隔离级别下都是采用一致性读。\n- MySQL中的锁定读: 包括共享锁(S锁)和独占锁(X锁)，S和S是兼容的，其他都不兼容，通过在SELECT语句后加`LOCK IN SHARE MODE`或`FOR UPDATE`可以显式S锁和X锁。\n- 多粒度锁: 对记录加的锁称为行(级)锁，也可以对表加锁，称为表(级)锁。表锁和行锁也是满足锁的兼容要求的，例如给一个表加了读锁，就不能给表或表的某些行加读锁，给一个表加了写锁，就不能给表或表的某些行加任何的锁。表锁粒度大，实现简单，占用资源少，行锁粒度小，可以实现更精准的并发控制，但会影响性能。\n- 意向锁: 包括意向共享锁(IS锁)和意向排他锁(IX锁)，当给某一行加S锁时，就要给对应的表加一个IS锁，IS锁和X锁不兼容，因此只要看到表有IS锁，就不能给这个表加X锁，而不需要遍历表的每一行；同理IX锁和S锁X锁都不兼容，那么如果给某一行加了X锁，就会给对应的表加IX锁，这样就不能再给这个表加S锁或者X锁。\n- MySQL中的表级锁: 在对某个表执行增删改查操作时，InnoDB并不会为这个表添加表锁，手动获取表锁的语句是`LOCK TABLES t READ/WRITE;`，对应的释放锁是`UNLOCK TABLES;`，一般不用。对于自增列，如何保证生成的数据严格自增不重复，系统有两种选择，一种是使用AUTO-INC锁，在插入语句开始时获取一个表锁，语句结束后释放(不是事务结束后)，使用于不确定插入记录数量的情况；一种是使用更轻量的锁，在生成自增id前加锁，生成后就释放锁，适用于确定要插入记录数量的情况，可以避免锁定表，提升插入性能。\n- MySQL中的行级锁: InnoDB支持行级锁，且有多种行锁类型，包括Record Lock(LOCK_REC_NOT_GAP)，即普通的记录锁，也有读锁和写锁之分；Gap Locks(LOCK_GAP)，用来在可重复读隔离级别下解决幻读问题的一种方案(另一种是采用MVCC，如果事务中两次读到的都是同一个ReadView，那就肯定不会出现幻读)，对某个记录加Gap锁，就相当于禁止在该记录和前一个记录之间(聚簇索引排序)插入新的记录，这样通过适当地加Gap锁，就可以避免幻读问题的发生；Next-Key Locks(LOCK_ORDINARY)，本质上一个普通记录锁和一个Gap锁的结合，即保护了该记录，又禁止在该记录之前插入新记录；Insert Intention Locks(LOCK_INSERT_INTENTION): 插入意向锁，当要插入一条新纪录而发现已经存在Gap锁，需要插入这个锁结构进行等待，类似于获取锁失败；隐式锁: 通常一个事务插入一条新纪录是不加锁的，这时候另一个事务如果对该记录进行读取或修改就可能产生脏读或脏写的问题，为了解决这个问题，采用隐式锁的方式，即当前插入的记录不加锁，当别的事务想要对该记录加S锁或者X锁时，先查该记录的trx_id是否是当前活跃事务，如果是，帮助该事务给该记录插入一个X锁，然后自己再插入一个等待的锁结构。\n- 内存中的锁结构包括事务信息，索引信息(行锁)，表锁/行锁信息(例如对哪个表加的锁，记录所在表空间，页号)，锁类型等。且在同一个事务中，可能会根据情况，对锁进行复用，并不一定是每一次加锁都创建一个新的锁结构。\n\n\n// TODO 做一些实验?","date":"Mar 13, 2021","category":{"name":"读书笔记","permalink":"http://127.0.0.1:8080/category/读书笔记"},"tags":[{"name":"数据库","permalink":"http://127.0.0.1:8080/tags/数据库"},{"name":"MySQL","permalink":"http://127.0.0.1:8080/tags/MySQL"},{"name":"TODO","permalink":"http://127.0.0.1:8080/tags/TODO"}]},{"title":"AtCoder-ABC-178(A-F)","permalink":"http://127.0.0.1:8080/detail/atcoder-abc-178","content":"\n## A\n\n输出1-x\n\n## B\n\n给定区间[a,b]和区间[c,d]，要从区间各取一个数，使得乘积最大。\n\n一开始想复杂了，其实最大值肯定是区间边缘的值相乘得到的，如果包含0，再跟0取个max即可。\n\n## C\n\n求长度为n，至少含有一个0和一个9的不同数字序列个数。\n\n利用容斥的思想，不加限制，长度为n的不同数字序列个数为$10^n$，再减去**可以**含有0，但是不含有9的数字序列个数$9^n$，再同样减去**可以**含有9，但是不含有0的数字序列个数$9^n$，然后要加上被删除两次的同时不含0和9的序列个数。\n\n## D\n\n求每一项大于等于3，且和为给定S的序列个数。\n\n有序的凑硬币问题，基础dp。比如[3,4]和[4,3]是属于不同序列，如果题目要求是属于同一个序列的话，就要将第一重循环改为枚举硬币面额。\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\ntypedef long long ll;\nconst int N=2005;\nconst ll MOD=1e9+7;\nll dp[N];\nint s;\nint main(){\n    scanf(\"%d\",&s);\n    dp[0]=1;\n    for(int i=1;i<=s;i++){\n        for(int j=3;j<=i;j++){\n            if(i-j>=0){\n                dp[i]=(dp[i]+dp[i-j])%MOD;\n            }\n        }\n//        printf(\"%d %lld\\n\",i,dp[i]);\n    }\n    printf(\"%lld\\n\",dp[s]%MOD);\n    return 0;\n}\n```\n\n## E\n\n给定n个点，求两点间最大的曼哈顿距离。\n\n经典题目，做法是将曼哈顿距离公式的绝对值符号拆开，对二维点来说会有4种情况，然后将同一个点的x，y放一起，发现两个点的形式其实是一样的，比如都是$(x_i+y_i)-(x_j+y_j)$，或者是$(-x_i+y_i)-(-x_j+y_j)$，所以直接枚举符号状态，然后遍历一遍求最大最小值。\n\n这个算法也可以改成动态的(支持查询，插入，删除)，只需要对每个二进制状态分别维护一个数据结构即可。原题hdu4666\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\ntypedef long long ll;\nconst int N=2e5+50;\nint n;\nll x[N],y[N];\nint sta[4][2]={{1,1},{1,-1},{-1,1},{-1,-1}};\nint main(){\n    scanf(\"%d\",&n);\n    for(int i=1;i<=n;i++){\n        scanf(\"%lld%lld\",&x[i],&y[i]);\n    }\n    ll ans=0;\n    for(int i=0;i<4;i++){\n        ll mx=-1e18;\n        ll mn=1e18;\n        for(int j=1;j<=n;j++){\n            ll t=sta[i][0]*x[j]+sta[i][1]*y[j];\n            mx=max(mx,t);\n            mn=min(mn,t);\n        }\n        ans=max(ans,mx-mn);\n    }\n    printf(\"%lld\\n\",ans);\n    return 0;\n}\n```\n\n## F\n\n给定a和b长度相同的两个升序数组，要求重排数组b，使得a和b对应位置的数字不同。\n\na和b都已经升序，所以首先将b逆序，这时候中间就可能存在一段a和b对应位置数字相同，且**这段区间都是同一个数字**，然后考虑将b数组这些位置的数字交换到其他位置，这里维护一个可以交换的位置下标即可，这样复杂度是O(N)的。\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\ntypedef long long ll;\nconst int N=2e5+50;\nint n,a[N],b[N];\nint main(){\n    scanf(\"%d\",&n);\n    for(int i=1;i<=n;i++){\n        scanf(\"%d\",&a[i]);\n    }\n    for(int i=1;i<=n;i++){\n        scanf(\"%d\",&b[n+1-i]);\n    }\n    int idx=1;\n    bool flag=true;\n    for(int i=1;i<=n;i++){\n        if(a[i]==b[i]){\n            while(idx<=n && (a[i]==b[idx] || a[idx]==b[i])){\n                idx++;\n            }\n            if(idx>n){\n                flag=false;\n                break;\n            }\n            swap(b[idx],b[i]);\n        }\n    }\n    if(flag){\n        printf(\"Yes\\n\");\n        for(int i=1;i<=n;i++){\n            printf(\"%d%c\",b[i],i==n?'\\n':' ');\n        }\n    }else{\n        printf(\"No\\n\");\n    }\n    return 0;\n}\n```\n\n","date":"Nov 25, 2020","category":{"name":"题解","permalink":"http://127.0.0.1:8080/category/题解"},"tags":[{"name":"思维","permalink":"http://127.0.0.1:8080/tags/思维"},{"name":"快速幂","permalink":"http://127.0.0.1:8080/tags/快速幂"},{"name":"容斥","permalink":"http://127.0.0.1:8080/tags/容斥"},{"name":"dp","permalink":"http://127.0.0.1:8080/tags/dp"},{"name":"二进制枚举","permalink":"http://127.0.0.1:8080/tags/二进制枚举"},{"name":"最大曼哈顿距离","permalink":"http://127.0.0.1:8080/tags/最大曼哈顿距离"},{"name":"贪心","permalink":"http://127.0.0.1:8080/tags/贪心"}]},{"title":"AtCoder-ABC-179(A-F)","permalink":"http://127.0.0.1:8080/detail/atcoder-abc-179","content":"\n## A\n\n输出字符串，以s结尾就加es，否则加s。\n\n## B\n\n判断是否有连续三对相同的数。\n\n## C\n\n给定N，求多少个正数三元组(A,B,C)满足AB+C=N。预处理因子个数然后暴力枚举C。\n\n## D\n\n给定一些区间，从数轴的1开始，每次可以选择任意一个区间的任意一个数，向右走这么多步，问走到N的方案数。\n\n简单的方案数DP，不同的是从前面的某一段区间转移而来，所以枚举区间，然后用前缀和优化即可，再优化也可以先把区间进行合并。\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\nconst int N=2e5+50;\nconst long long MOD=998244353;\nint n,k;\nlong long dp[N],pre[N];\nstruct node{\n    int l,r;\n    bool operator<(const node& rhs)const{\n        if(l!=rhs.l){\n            return l<rhs.l;\n        }else{\n            return r<rhs.r;\n        }\n    }\n}a[15];\nint main(){\n    scanf(\"%d%d\",&n,&k);\n    for(int i=1;i<=k;i++){\n        scanf(\"%d%d\",&a[i].l,&a[i].r);\n    }\n    sort(a+1,a+1+k);\n    vector<node> b;\n    int lastl=-1;\n    int lastr=-1;\n    int cur=1;\n    while(cur<=k){\n        if(a[cur].l<=lastr){\n            lastr=a[cur].r;\n        }else{\n            if(lastr!=-1){\n                b.push_back(node{lastl,lastr});\n            }\n            lastl=a[cur].l;\n            lastr=a[cur].r;\n        }\n        cur++;\n    }\n    if(lastr!=-1){\n        b.push_back(node{lastl,lastr});\n    }\n    dp[1]=1LL;\n    int siz=b.size();\n    for(int i=1;i<=n;i++){\n        for(int j=0;j<siz;j++){\n            int l=max(0,i-b[j].l);\n            int r=max(0,i-b[j].r-1);\n            dp[i]=(dp[i]+pre[l]-pre[r]+MOD)%MOD;\n        }\n        pre[i]=pre[i-1]+dp[i];\n    }\n    printf(\"%lld\\n\",dp[n]);\n    return 0;\n}\n```\n\n## E\n\n给定首项x和递推式$a_n=(a_{n-1}^2)\\%m$，求前n项和。\n\n注意到m的范围是1e5，所以很明显每一项都不会超过1e5，所以肯定会存在循环节，所以暴力找出循环节然后用前缀和计算一下剩下的小部分即可。\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\ntypedef long long ll;\nconst int N=2e6+50;\nll n,x,m;\nll a[N],p[N];\nint loc[N];\nint main(){\n//    freopen(\"in.txt\",\"r\",stdin);\n    scanf(\"%lld%lld%lld\",&n,&x,&m);\n    a[1]=x;\n    p[1]=x;\n    ll ans=x;\n    memset(loc,-1,sizeof(loc));\n    loc[x]=1;\n    for(int i=2;i<=n;i++){\n        a[i]=(a[i-1]*a[i-1])%m;\n        p[i]=p[i-1]+a[i];\n        ans+=a[i];\n        if(loc[a[i]]!=-1){\n            ll cy=p[i]-p[loc[a[i]]];\n            ll num=(n-loc[a[i]])/(i-loc[a[i]]);\n            ll rem=n-loc[a[i]]-num*(i-loc[a[i]]);\n            ans+=cy*(num-1);\n            ans+=p[loc[a[i]]+rem]-p[loc[a[i]]];\n            break;\n        }\n        loc[a[i]]=i;\n    }\n    printf(\"%lld\\n\",ans);\n    return 0;\n}\n```\n\n## F\n\n有个n行n列的网格图，初始中间n-2的方阵是黑色，最下边的行和最右边的列是白色，有两种操作，选择一列或者一行变成白色直到第一个原先已经白色的格子，给定一些操作，问最后黑色格子数量。\n\n最重要的一点是要从全局来看这个网格图，每一次操作其实都是对中间n-2行n-2列的黑色方阵的压缩，比如对第x列操作，那么对于x列右边的列，横向行的操作就不会再影响到了。\n\n具体做法如下，行和列分别维护：从上到下第一个被操作(全部变成白色)的行，这是因为上一段提到的，相当于一个收缩隔离的作用，在这个行以下的，状态不会再被改变，即黑色格子数量是一定的，可以通过第二个维护的量来记录；每一行第一个白色格子位置，显然这个白色格子就是列操作产生的，这样子当对这一行进行操作，就可以直接算出消失的黑色格子数量(**注意比如[黑 白 黑]的清空，行的操作也只会影响第一个黑，但是这不会影响答案，第二个黑会在列的操作时被影响**)。列同理。\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\ntypedef long long ll;\nconst int N=2e5+50;\nint n,q,o,x;\n// a[i]表示第i列第一个白色，b[i]表示第i行第一个白色\nint a[N],b[N];\nint main(){\n    scanf(\"%d%d\",&n,&q);\n    for(int i=1;i<=n;i++){\n        a[i]=n;\n        b[i]=n;\n    }\n    // r表示从左到右第一个被竖切全白的列\n    // d表示从上到下第一个被横切全白的行\n    int r=n;\n    int d=n;\n    long long ans=(long long)(n-2)*(n-2);\n    while(q--){\n        scanf(\"%d%d\",&o,&x);\n        if(o==1){\n            if(x<r){\n                for(int i=x;i<r;i++){\n                    a[i]=d;\n                }\n                // 更新从左到右第一个全白列\n                r=x;\n            }\n            // a[x]是x列从上到下第一个白色，所以前面a-1个除了第一行(没有颜色)，其他(a-2)个都是黑色，将变成白色\n            ans-=(long long)(a[x]-2);\n        }else{\n            if(x<d){\n                for(int i=x;i<d;i++){\n                    b[i]=r;\n                }\n                d=x;\n            }\n            ans-=(long long)(b[x]-2);\n        }\n    }\n    printf(\"%lld\\n\",ans);\n    return 0;\n}\n```\n","date":"Nov 11, 2020","category":{"name":"题解","permalink":"http://127.0.0.1:8080/category/题解"},"tags":[{"name":"因子","permalink":"http://127.0.0.1:8080/tags/因子"},{"name":"dp","permalink":"http://127.0.0.1:8080/tags/dp"},{"name":"前缀和","permalink":"http://127.0.0.1:8080/tags/前缀和"},{"name":"区间合并","permalink":"http://127.0.0.1:8080/tags/区间合并"},{"name":"循环节","permalink":"http://127.0.0.1:8080/tags/循环节"},{"name":"思维","permalink":"http://127.0.0.1:8080/tags/思维"}]},{"title":"AtCoder-ABC-180(A-F)","permalink":"http://127.0.0.1:8080/detail/atcoder-abc-180","content":"\n## A\n\n输出N-A+B。\n\n## B\n\n输出一个点到原点的曼哈顿距离，欧几里得距离，切比雪夫距离。\n\n## C\n\n按顺序输出一个数的所有因子。\n\n## D\n\n初始值X，每次操作可以乘A或者加B，在X保持小于Y的前提下要求尽量多的操作次数。\n\n有个边界，当某一次乘法操作后大于加法操作，说明后面都需要是加法操作。注意long long溢出。\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\ntypedef long long ll;\nll x,y,a,b;\nint main(){\n    scanf(\"%lld%lld%lld%lld\",&x,&y,&a,&b);\n    // ans要long long\n    long long ans=0;\n    // a*x会溢出\n    while(__int128(a)*x<__int128(y) && a*x<=b+x){\n        x*=a;\n        ans++;\n    }\n    ans+=(y-1-x)/b;\n    printf(\"%lld\\n\",ans);\n    return 0;\n}\n```\n\n## E\n\n给n个点，定义之间的距离，然后要求从点1出发走完所有点(至少走一次)回到点1，求最短路。\n\n同hdu5418，先floyd跑出任意两点之间的最短路，然后状压dp。\n\n**注意：** 至少经过一次这个条件其实在跑floyd的时候就完成了，之后任意两点之间都可通且是最短路，所以肯定不存在走过一个点之后再重复走一次，但是实际上在走点对点的最短路时可能会重复走过某个点。\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\nconst int N=17;\nconst int INF=0x3f3f3f3f;\nint n;\nint x[N],y[N],z[N];\nint dis[N][N];\nint dp[(1<<17)+5][17];\nint main(){\n//    freopen(\"in.txt\",\"r\",stdin);\n    scanf(\"%d\",&n);\n    for(int i=0;i<n;i++){\n        scanf(\"%d%d%d\",&x[i],&y[i],&z[i]);\n    }\n    for(int i=0;i<n;i++){\n        for(int j=0;j<n;j++){\n            dis[i][j]=abs(x[j]-x[i])+abs(y[j]-y[i])+max(0,z[j]-z[i]);\n        }\n    }\n    // floyd求所有点之间最短路\n    for(int i=0;i<n;i++){\n        for(int j=0;j<n;j++){\n            for(int k=0;k<n;k++){\n                dis[j][k]=min(dis[j][k],dis[j][i]+dis[i][k]);\n            }\n        }\n    }\n    memset(dp,INF,sizeof(dp));\n    // 起点是点0，所以\n    dp[1][0]=0;\n    // 经过一个点至少一次，但是跑了floyd最短路之后，就要保证只走一次\n    // 枚举前一个状态i\n    for(int i=1;i<(1<<n);i++){\n        // 枚举要到达的点\n        for(int j=0;j<n;j++){\n            // 状态i必须没经过j\n            if(((i>>j)&1)==0){\n                // 枚举状态i的最后点\n                for(int k=0;k<n;k++){\n                    // 状态i必须经过k\n                    if(((i>>k)&1)==1){\n                        dp[i|(1<<j)][j]=min(dp[i|(1<<j)][j],dp[i][k]+dis[k][j]);\n                    }\n                }\n            }\n        }\n    }\n    int ans=INF;\n    for(int i=1;i<n;i++){\n        ans=min(ans,dp[(1<<n)-1][i]+dis[i][0]);\n    }\n    printf(\"%d\\n\",ans);\n    return 0;\n}\n```\n\n## F\n\n比较难的dp，等有时间再学一学\n","date":"Nov 7, 2020","category":{"name":"题解","permalink":"http://127.0.0.1:8080/category/题解"},"tags":[{"name":"AtCoder","permalink":"http://127.0.0.1:8080/tags/AtCoder"},{"name":"贪心","permalink":"http://127.0.0.1:8080/tags/贪心"},{"name":"最短路","permalink":"http://127.0.0.1:8080/tags/最短路"},{"name":"floyd","permalink":"http://127.0.0.1:8080/tags/floyd"},{"name":"dp","permalink":"http://127.0.0.1:8080/tags/dp"},{"name":"状压dp","permalink":"http://127.0.0.1:8080/tags/状压dp"},{"name":"ToBeAC","permalink":"http://127.0.0.1:8080/tags/ToBeAC"}]},{"title":"AtCoder-ABC-181(A-F)","permalink":"http://127.0.0.1:8080/detail/atcoder-abc-181","content":"\n## A\n\n奇数输出Black，偶数输出White。\n\n## B\n\n求多个连续整数区间和。\n\n## C\n\n判断点C是否在直线AB上，可以通过判断AB向量和AC向量的叉积是否为0，叉积公式为$x_1y_2-x_2y_1$，大于0说明AC向量在AB向量顺时针方向，小于0是逆时针方向，等于0则共线。\n\n## D\n\n1e5的数字串，问能否重新调整位置使得这个数可以整除8。分类讨论，因为1000肯定可以被8整除，所以只要枚举后三位能被8整除的数，判断原来的数能否组成这个三位数。\n\n**注意：** 枚举要从三位数开始枚举，也就是104，而不是8，枚举8的话另外两位不一定是0，所以如果要枚举8的话，其实要枚举的是008，所以原数字里需要两个0。如果从104开始枚举，例如1008，1016这种数字也会被被check\n到，因为会枚举到800，800可以，008显然也可以，同理也会枚举到160，所以016也可以。\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\nconst int N=2e5+50;\nchar s[N];\nint dig[10];\nbool check(int x){\n    int temp[10]={0};\n    int cnt=3;\n    while(x){\n        cnt--;\n        temp[x%10]++;\n        x/=10;\n    }\n    temp[0]+=cnt;\n    for(int i=0;i<10;i++){\n        if(dig[i]<temp[i]){\n            return false;\n        }\n    }\n    return true;\n}\nint main(){\n    scanf(\"%s\",s);\n    int n=strlen(s);\n    if(n==1){\n        if(s[0]=='8'){\n            printf(\"Yes\\n\");\n        }else{\n            printf(\"No\\n\");\n        }\n    }else if(n==2){\n        if(((s[0]-'0')*10+(s[1]-'0'))%8==0 || ((s[1]-'0')*10+(s[0]-'0'))%8==0){\n            printf(\"Yes\\n\");\n        }else{\n            printf(\"No\\n\");\n        }\n    }else{\n        for(int i=0;i<n;i++){\n            dig[s[i]-'0']++;\n        }\n        for(int i=8;i<1000;i+=8){\n            if(check(i)){\n                printf(\"Yes\\n\");\n                return 0;\n            }\n        }\n        printf(\"No\\n\");\n    }\n}\n```\n\n## E\n\n从m个数中选择一个插入到另外n个数(n为奇数)，然后两两配对，使得差的和最小。\n\n首先n个数排序，然后枚举m个数中的每一个，二分查找插入的位置，插入之后会影响小范围的答案统计，而大范围的不变，所以可以先正着和倒着分别求一次差的前缀和和后缀和，然后分类讨论插入的位置，更新答案即可。\n\n一开始可以先把所有边界情况详细分类讨论，然后再考虑合并。\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\nconst int N=2e5+50;\nint n,m,h[N],w[N];\nint l[N],r[N];\nint main(){\n    scanf(\"%d%d\",&n,&m);\n    for(int i=1;i<=n;i++){\n        scanf(\"%d\",&h[i]);\n    }\n    for(int i=1;i<=m;i++){\n        scanf(\"%d\",&w[i]);\n    }\n    sort(h+1,h+1+n);\n    for(int i=1;i<=n;i++){\n        l[i]=l[i-1];\n        if(i%2==0){\n            l[i]+=h[i]-h[i-1];\n        }\n    }\n    for(int i=n;i>=1;i--){\n        r[i]=r[i+1];\n        if((n-i)%2==1){\n            r[i]+=h[i+1]-h[i];\n        }\n    }\n    int ans=INT_MAX;\n    for(int i=1;i<=m;i++){\n        int k=lower_bound(h+1,h+1+n,w[i])-h;\n        if(k%2==1){\n            ans=min(ans,l[k-1]+h[k]-w[i]+r[k+1]);\n        }else{\n            ans=min(ans,l[k-2]+w[i]-h[k-1]+r[k]);\n        }\n    }\n    printf(\"%d\\n\",ans);\n    return 0;\n}\n```\n\n## F\n\ny=100和y=-100两条直线之间有一些钉子，求一个最大的环能够从左边走到右边而不会碰到钉子和直线。\n\n因为答案具有单调性，所以可以二分答案，然后考虑一个问题：两个钉子如果距离小于2r，说明环不可能从这两个钉子之间通过，所以可以把这两个钉子看成一个整体，所以用到了并查集，对于每个半径，枚举所有钉子对，用并查集维护，也包括了上下两条直线。\n\n最后判断如果上下两条直线已经属于同一个并查集，则这个半径的环无法通过。\n\n**注意：** 一开始写成当并查集数量等于1时，无法通过，其实不止这种情况，有可能上下直线连成一起，但是还有其他的钉子组成了其他的并查集。\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\nconst double eps=1e-8;\nint n;\ndouble x[105],y[105];\nint p[105];\nvoid init(){\n    for(int i=1;i<=n+2;i++){\n        p[i]=i;\n    }\n}\nint find(int x){\n    return x==p[x]?p[x]:find(p[x]);\n}\nvoid unit(int i,int j){\n    int fa=find(i);\n    int fb=find(j);\n    p[fa]=fb;\n}\ndouble dis2(int i,int j){\n    return (x[i]-x[j])*(x[i]-x[j])+(y[i]-y[j])*(y[i]-y[j]);\n}\nbool check(double a){\n    init();\n    for(int i=1;i<=n;i++){\n        for(int j=1;j<=n;j++){\n            if(i==j){\n                continue;\n            }\n            if(dis2(i,j)-4*a*a<=eps){\n                unit(i,j);\n            }\n        }\n    }\n    for(int i=1;i<=n;i++){\n        if(100-y[i]-2*a<=eps){\n            unit(i,n+1);\n        }\n        if(y[i]+100-2*a<=eps){\n            unit(i,n+2);\n        }\n    }\n    return find(n+1)!=find(n+2);\n}\nint main(){\n    scanf(\"%d\",&n);\n    for(int i=1;i<=n;i++){\n        scanf(\"%lf%lf\",&x[i],&y[i]);\n    }\n    double l=0,r=100;\n    double ans;\n    while(l+eps<=r){\n        double mid=(l+r)/2;\n        if(check(mid)){\n            ans=mid;\n            l=mid;\n        }else{\n            r=mid;\n        }\n    }\n    printf(\"%lf\\n\",ans);\n    return 0;\n}\n```\n","date":"Nov 5, 2020","category":{"name":"题解","permalink":"http://127.0.0.1:8080/category/题解"},"tags":[{"name":"AtCoder","permalink":"http://127.0.0.1:8080/tags/AtCoder"},{"name":"计算几何","permalink":"http://127.0.0.1:8080/tags/计算几何"},{"name":"思维","permalink":"http://127.0.0.1:8080/tags/思维"},{"name":"二分查找","permalink":"http://127.0.0.1:8080/tags/二分查找"},{"name":"分类讨论","permalink":"http://127.0.0.1:8080/tags/分类讨论"},{"name":"前缀和","permalink":"http://127.0.0.1:8080/tags/前缀和"},{"name":"二分答案","permalink":"http://127.0.0.1:8080/tags/二分答案"},{"name":"并查集","permalink":"http://127.0.0.1:8080/tags/并查集"}]},{"title":"AtCoder-ABC-190(A-F)","permalink":"http://127.0.0.1:8080/detail/atcoder-abc-190","content":"\n## A\n\n输入A，B，C，C决定先后手，A和B每次轮流减1，先减为0的输。\n\n根据先后手决定判断是小于还是小于等于。\n\n## B\n\n有一些法术，施法需要Xi秒，攻击力Yi，怪兽可以无视施法大于等于S秒或者是攻击力小于等于D的法术，问能否使怪兽造成伤害。\n\n遍历判断，必须两个条件都不符合。\n\n## C\n\n有N个盘子，M个条件，要求某两个盘子上都要有球，有K个操作，可以给两个盘子的其中一个放上一个球，问怎么放能满足最多条件。\n\nK<=16，直接爆搜，然后枚举所有条件进行判断。\n\n## D\n\n求出和为N的公差为1的整数等差数列的数量。\n\n根据定义，该数列首尾相加乘以长度就是2N，是一个偶数，所以直接枚举2N的因子，再排除掉两个因子都是偶数(**如果数列长度是偶数，而公差又是1，则首尾相加不可能是偶数**)的情况，剩下的就是所有可能的数列。\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\ntypedef long long ll;\nll n;\nint main(){\n    scanf(\"%lld\",&n);\n    n*=2LL;\n    ll ans=0;\n    for(ll i=1LL;i*i<=n;i++){\n        if(n%i==0){\n            if(i%2==1 || (n/i)%2==1){\n                ans+=2;\n            }\n        }\n    }\n    printf(\"%lld\\n\",ans);\n    return 0;\n}\n```\n\n## E\n\n问题抽象出来就是给一个1e5的无向图，边权都为1，指定K个特殊点，要求找到最短的路径满足每个特殊点至少经过一次。\n\n特殊点和特殊点之间肯定是走最短路径，所以可以先枚举特殊点，通过bfs求出任意两个特殊点的最短距离，这样就可以对图进行简化，其他的点都不需要，只需要关注这K个特殊点和它们之间的距离。\n\n然后就是转化为带权的最短哈密顿通路的问题，因为K<=17，所以可以用状压dp来解决这个问题。\n\n枚举起点，定义dp[i][j]表示已经过的点状态为i，最后经过的点是j的最短距离，枚举状态，再枚举上一个经过的点，状态转移方程为`dp[j][l]=min(dp[j][l],dp[(j^(1<<(l-1)))][o]+cost[o][l])`\n\n注意状压dp中的位运算。\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\nconst int N=1e5+50;\nconst int INF=0x3f3f3f3f;\nint n,m,a,b,k;\nint c[20];\nvector<int> g[N];\nint vis[N];\nint cost[20][20],dp[1<<19][20];\nint main(){\n    scanf(\"%d%d\",&n,&m);\n    for(int i=1;i<=m;i++){\n        scanf(\"%d%d\",&a,&b);\n        g[a].push_back(b);\n        g[b].push_back(a);\n    }\n    scanf(\"%d\",&k);\n    for(int i=1;i<=k;i++){\n        scanf(\"%d\",&c[i]);\n    }\n    if(k==1){\n        printf(\"%d\\n\",1);\n        return 0;\n    }\n    for(int i=1;i<=k;i++){\n        for(int j=1;j<=k;j++){\n            if(i==j){\n                cost[i][i]=0;\n                continue;\n            }else{\n                cost[i][j]=INF;\n            }\n            int st=c[i];\n            int ed=c[j];\n            queue<pair<int,int>> q;\n            queue<int> vq;\n            q.push({st,0});\n            vis[st]=1;\n            vq.push(st);\n            while(!q.empty()){\n                auto t=q.front();\n                q.pop();\n                int u=t.first;\n                int cs=t.second;\n                if(u==ed){\n                    cost[i][j]=cs-1;\n                    break;\n                }\n                int siz=g[u].size();\n                for(int l=0;l<siz;l++){\n                    int v=g[u][l];\n                    if(!vis[v]){\n                        q.push({v,cs+1});\n                        vis[v]=1;\n                        vq.push(v);\n                    }\n                }\n            }\n            while(!vq.empty()){\n                vis[vq.front()]=0;\n                vq.pop();\n            }\n        }\n    }\n    int ans=INF;\n    for(int i=1;i<=k;i++) {\n        memset(dp,INF,sizeof(dp));\n        // 起点\n        dp[1<<(i-1)][i]=0;\n        for(int j=1;j<(1<<k);j++){\n            for(int l=1;l<=k;l++){\n                if((j>>(l-1))&1){\n                    // 枚举上一个经过的点\n                    for(int o=1;o<=k;o++){\n                        if(((j^(1<<(l-1)))>>(o-1))&1){\n                            dp[j][l]=min(dp[j][l],dp[(j^(1<<(l-1)))][o]+cost[o][l]);\n                        }\n                    }\n                }\n            }\n        }\n        for(int j=1;j<=k;j++){\n            if(j!=i){\n                ans=min(ans,dp[(1<<k)-1][j]);\n            }\n        }\n    }\n    printf(\"%d\\n\",ans>=INF?-1:ans+k);\n    return 0;\n}\n```\n\n## F\n\n给定一个全排列，计算分别循环左移0到n-1位时候序列的逆序数。\n\n因为是全排列，所以当一个数从首位左移到末位，显然逆序数的变化就是减去后面比它小的数的个数，再加上后面比它大的数的个数。\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\nconst int N=3e5+50;\ntypedef long long ll;\nint n,a[N];\nint c[N];\nint lowbit(int x){\n    return x&(-x);\n}\nvoid add(int i,int x){\n    while(i<=n){\n        c[i]+=x;\n        i+=lowbit(i);\n    }\n}\nint sum(int i){\n    int ans=0;\n    while(i){\n        ans+=c[i];\n        i-=lowbit(i);\n    }\n    return ans;\n}\nint main(){\n    scanf(\"%d\",&n);\n    for(int i=1;i<=n;i++){\n        scanf(\"%d\",&a[i]);\n    }\n    ll ans=0;\n    for(int i=n;i>=1;i--){\n        ans+=1LL*sum(a[i]+1);\n        add(a[i]+1,1);\n    }\n    printf(\"%lld\\n\",ans);\n    for(int i=1;i<n;i++){\n        ans=ans+n-1-2LL*a[i];\n        printf(\"%lld\\n\",ans);\n    }\n    return 0;\n}\n```","date":"Jan 30, 2021","category":{"name":"题解","permalink":"http://127.0.0.1:8080/category/题解"},"tags":[{"name":"思维","permalink":"http://127.0.0.1:8080/tags/思维"},{"name":"枚举","permalink":"http://127.0.0.1:8080/tags/枚举"},{"name":"暴搜","permalink":"http://127.0.0.1:8080/tags/暴搜"},{"name":"因子","permalink":"http://127.0.0.1:8080/tags/因子"},{"name":"bfs","permalink":"http://127.0.0.1:8080/tags/bfs"},{"name":"最短路","permalink":"http://127.0.0.1:8080/tags/最短路"},{"name":"状压dp","permalink":"http://127.0.0.1:8080/tags/状压dp"},{"name":"树状数组","permalink":"http://127.0.0.1:8080/tags/树状数组"},{"name":"逆序数","permalink":"http://127.0.0.1:8080/tags/逆序数"}]},{"title":"AtCoder-ABC-194(A-F)","permalink":"http://127.0.0.1:8080/detail/atcoder-abc-194","content":"\n## A\n\n略\n\n## B\n\nN个员工，每个员工做A，B两种工作的时间都不同，如果让一个员工做A和B，花费时间就是两者之和，如果让两个员工做，花费时间就是两者最大值，问所花费最小时间。\n\nN为1000，直接暴力枚举。\n\n## C\n\n求$\\displaystyle \\sum_{i = 2}^{N} \\sum_{j = 1}^{i - 1} (A_i - A_j)^2$。\n\n展开一下，预处理前缀和和平方前缀和即可。\n\n## D\n\n有N个点，一开始在点1，每次等概率随机选择一个点(包括所在的点)，将这两点连边并走到该点，问使得图连通的操作次数。\n\n有个很重要的思路，走的顺序是不重要的，**重要的只有一点，就是走过了(连通)了多少个点**。就比如从1走到2，再从2走到1，和在2一直原地踏步是一样的，所以将图抽象为一条线段，比如3个点`1---2---3`，先算从1走到2的期望，这里的2并不仅仅指图上的点2，只是代表第二个走过的点，所以这里有三种可能性，原地踏步1种，另外2种可能性都可以走到第二个点，所以从1走到2的期望是3/2，同理再算从2走到3的期望是3/1，因此可以总结得到规律，答案为$\\sum_{i-1}^{n-1}\\frac{n}{i}$。\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\nconst int N=1e5+50;\nint n;\nint main(){\n    scanf(\"%d\",&n);\n    double ans=0.0;\n    for(int i=1;i<n;i++){\n        ans+=n*1.0/i;\n    }\n    printf(\"%.8lf\\n\",ans);\n    return 0;\n}\n```\n\n再仔细思考，其实题目本质就是N个点，每次随机给一个点染色，问所有点全部染色的次数期望。\n\n考虑期望dp，dp[i]表示已经染色了i个点，要染色n个点所需要的次数期望，状态转移方程为$dp[i]=\\frac{i}{n}dp[i]+\\frac{n-i}{n}dp[i+1]+1$，其中第一部分是指随机选到的点是已染色的，第二部分是指选到未染色的点，移项化简，得到$dp[i]=dp[i+1]+\\frac{n}{n-i}$，初始状态为dp[n]=0。\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\nconst int N=1e5+50;\nint n;\ndouble dp[N];\nint main(){\n    scanf(\"%d\",&n);\n    for(int i=n-1;i>=1;i--){\n        dp[i]=dp[i+1]+n*1.0/(n-i);\n    }\n    printf(\"%.8lf\\n\",dp[1]);\n    return 0;\n}\n```\n\n## E\n\n给定长度为N的数组，对于每个长度为M的子数组，求出Mex，然后再求出所有Mex的最小值。\n\n最简单的做法就是滑窗维护mex，但有个重要的问题，**只需要考虑删除数字之后mex的变化，而不用考虑添加数字之后mex的变化**，因为要求的是最小mex，而添加数字只会让mex变大。\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\nconst int N=2e6+50;\nint n,m;\nint a[N],b[N];\nint mex;\nvoid add(int i){\n    b[a[i]]++;\n}\nvoid del(int i){\n    b[a[i]]--;\n    if(b[a[i]]==0 && a[i]<mex){\n        mex=a[i];\n    }\n}\nint main(){\n    scanf(\"%d%d\",&n,&m);\n    for(int i=1;i<=n;i++){\n        scanf(\"%d\",&a[i]);\n    }\n    for(int i=1;i<=m;i++){\n        add(i);\n    }\n    for(int i=0;i<=n;i++){\n        if(b[i]==0){\n            mex=i;\n            break;\n        }\n    }\n    for(int i=m+1;i<=n;i++){\n        add(i);\n        del(i-m);\n    }\n    printf(\"%d\\n\",mex);\n    return 0;\n}\n```\n\n## F\n\n求出1到N(不含前导零的16进制数)中多少个数字正好含有K的不同的数字(包括A-F)。\n\n16进制的数位dp基础题。基本思路和注意点都在注释里了，后面可能会整理一个数位dp学习专题。\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\ntypedef long long ll;\nconst int N=2e5+50;\nconst int mod=1e9+7;\nchar s[N];\nint k;\nint dig[N];\n// 低i位满足有k个不同数位的数字个数\n// 注意这里定义的dp状态其实就蕴含了一种记忆化搜索时\"空间换时间\"的一种思想\n// 实际上这里的状态也可以只定义dp[]一维，那表示的就是满足条件的数字个数，这种方法粒度大，使用空间小，但显然花费时间多，因为大多数搜索状态都无法记忆化，只能重复搜\n// 同理状态也可以定义为dp[][][]三维，再加上一维状态压缩，这种做法就是粒度小，但是太占空间\nll dp[N][22];\nvoid print(int x){\n    if(x){\n        print(x/2);\n    }\n    printf(\"%d\",x%2);\n}\nll dfs(int idx,int sta,int dif,bool lead,bool limit){\n    if(idx==-1){\n        // 一个合法数字(这题要排除含前导零的数)，有时候要单独考虑0的情况\n        // 其实我觉得前导零的命名有点误导性，其实lead为true就是表示现在搜索的前缀是一串0\n        if(dif==k && !lead){\n            return 1;\n        }else{\n            return 0;\n        }\n    }\n    // 记忆化的都是无上限的\n    if(!limit && dp[idx][dif]!=-1){\n        return dp[idx][dif];\n    }\n    // 这一位的枚举上限\n    int up=limit?dig[idx]:15;\n    ll ans=0;\n    for(int i=0;i<=up;i++){\n        if(lead && i==0 || (sta>>i)&1){\n            ans+=dfs(idx-1,sta,dif,lead && i==0,limit && i==up);\n        }else{\n            ans+=dfs(idx-1,sta|(1<<i),dif+1,lead && i==0,limit && i==up);\n        }\n        ans%=mod;\n    }\n    if(!limit){\n        dp[idx][dif]=ans%mod;\n    }\n    return ans%mod;\n}\nint main(){\n    scanf(\"%s %d\",s,&k);\n    int n=strlen(s);\n    for(int i=n-1;i>=0;i--){\n        if(s[i]>='A'){\n            dig[n-1-i]=s[i]-'A'+10;\n        }else{\n            dig[n-1-i]=s[i]-'0';\n        }\n    }\n    memset(dp,-1,sizeof(dp));\n    // 从高位搜索下去\n    ll ans=dfs(n-1,0,0,true,true);\n    printf(\"%lld\\n\",ans%mod);\n    return 0;\n}\n```\n","date":"Mar 17, 2021","category":{"name":"题解","permalink":"http://127.0.0.1:8080/category/题解"},"tags":[{"name":"暴力","permalink":"http://127.0.0.1:8080/tags/暴力"},{"name":"预处理","permalink":"http://127.0.0.1:8080/tags/预处理"},{"name":"前缀和","permalink":"http://127.0.0.1:8080/tags/前缀和"},{"name":"思维","permalink":"http://127.0.0.1:8080/tags/思维"},{"name":"dp","permalink":"http://127.0.0.1:8080/tags/dp"},{"name":"期望dp","permalink":"http://127.0.0.1:8080/tags/期望dp"},{"name":"数位dp","permalink":"http://127.0.0.1:8080/tags/数位dp"}]},{"title":"CodeForces-Div2-678(A-D)","permalink":"http://127.0.0.1:8080/detail/codeforces-div2-678","content":"\n## 比赛情况\n\n<del>退役复健之CF</del>，第一场VP，果然和想象中一样，勉强过了C，D赛后看了一下题解思路不太对，整体来说还是很符合rating 1300的水平的。\n\n## 题解\n\n### A\n\n题意的复杂公式转化后就是判断给定的数组和是否等于m。\n\n### B\n\n给定n(n<=100)，构造一个数的方阵满足每个数都不是素数，且每一行每一列的和都为素数。\n\n我的做法是先筛素数，然后暴力构造，从n=3的全1矩阵开始，然后**行和列对称**，维护每一行的和，然后直接暴力枚举每个数判断新的一行的和是否为素数，最后，**所有新添加的数的和也就是新的一行的和**，通过这个和，就就构造最后的一个a[n-1][n-1]。\n\n......突然发现题意看错，矩阵的数可以是0，所以按照题解的做法，直接根据n的奇偶性，如果n是偶数，则主副对角线都为1，其他为0，如果n是奇数，在偶数的基础上再补充两个1即可。\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\nconst int N=1e5+50;\nint t,n;\nbool isPrime[N];\nint ans[105][105],sum[105];\nvoid init(){\n    isPrime[2]=true;\n    for(int i=3;i<N;i++){\n        isPrime[i]=true;\n        for(int j=2;j*j<=i;j++){\n            if(i%j==0){\n                isPrime[i]=false;\n                break;\n            }\n        }\n    }\n}\nint main(){\n    init();\n    scanf(\"%d\",&t);\n    while(t--){\n        scanf(\"%d\",&n);\n        if(n==1){\n            printf(\"1\\n\");\n        }else if(n==2){\n            printf(\"1 1\\n1 1\\n\");\n        }else if(n==3){\n            printf(\"1 1 1\\n1 1 1\\n1 1 1\\n\");\n        }else{\n            for(int i=0;i<3;i++){\n                for(int j=0;j<3;j++){\n                    ans[i][j]=1;\n                }\n                sum[i]=3;\n            }\n            for(int i=3;i<n;i++){\n                int shu=0;\n                for(int j=0;j<i;j++){\n                    int nw=1;\n                    while(!isPrime[nw+sum[j]]){\n                        nw++;\n                        while(isPrime[nw]){\n                            nw++;\n                        }\n                    }\n                    ans[i][j]=ans[j][i]=nw;\n                    shu+=nw;\n                    sum[j]+=nw;\n                }\n                int nw=1;\n                while(!isPrime[nw+shu]){\n                    nw++;\n                    while(isPrime[nw]){\n                        nw++;\n                    }\n                }\n                ans[i][i]=nw;\n                sum[i]=shu+nw;\n            }\n            for(int i=0;i<n;i++){\n                for(int j=0;j<n;j++){\n                    printf(\"%d%c\",ans[i][j],j==n-1?'\\n':' ');\n                }\n            }\n        }\n    }\n    return 0;\n}\n```\n\n### C\n\n给定n，x和p，求有多少个长为n的全排列满足p位置是x，且可以通过给定的二分查找算法查找到x。\n\n按照给定的算法二分查找，因为二分的路径是一定的，所以可以确定每次二分定位的那个数要大于x还是小于x，还是等于x，所以最后可以得到的信息，就是这个全排列里，有多少个位置必须放比x大的数，多少个位置必须放比x小的数，剩下就随便放，排列数乘一下就可以了。\n\n比赛时想得很乱，写完完全没底，结果直接秒过？？？\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\ntypedef long long ll;\nconst int N=1e5+50;\nconst int MOD=1e9+7;\nint n,x,p;\nll Pow(ll x,ll n,ll mod){\n    ll ans=1LL;\n    while(n){\n        if(n%2){\n            ans=ans*x%mod;\n        }\n        x=x*x%mod;\n        n/=2;\n    }\n    return ans;\n}\nll Inv(ll x,ll mod){\n    return Pow(x,mod-2,mod);\n}\nll fac[1005];\nvoid init(){\n    fac[0]=1;\n    fac[1]=1;\n    for(ll i=2;i<1005;i++){\n        fac[i]=(fac[i-1]*i)%MOD;\n    }\n}\nll A(int a,int b){\n    return (fac[a]*Inv(fac[a-b],MOD))%MOD;\n}\nint main(){\n//    freopen(\"in.txt\",\"r\",stdin);\n    init();\n    scanf(\"%d%d%d\",&n,&x,&p);\n    int l=0,r=n;\n    int xiao=0;\n    int da=0;\n    while(l<r){\n        int mid=(l+r)/2;\n        if(mid<p){\n            l=mid+1;\n            xiao++;\n        }else if(mid==p){\n            l=mid+1;\n        }else{\n            r=mid;\n            da++;\n        }\n    }#include <bits/stdc++.h>\nusing namespace std;\nconst int N=1e5+50;\nint t,n;\nbool isPrime[N];\nint ans[105][105],sum[105];\nvoid init(){\n    isPrime[2]=true;\n    for(int i=3;i<N;i++){\n        isPrime[i]=true;\n        for(int j=2;j*j<=i;j++){\n            if(i%j==0){\n                isPrime[i]=false;\n                break;\n            }\n        }\n    }\n}\nint main(){\n    init();\n    scanf(\"%d\",&t);\n    while(t--){\n        scanf(\"%d\",&n);\n        if(n==1){\n            printf(\"1\\n\");\n        }else if(n==2){\n            printf(\"1 1\\n1 1\\n\");\n        }else if(n==3){\n            printf(\"1 1 1\\n1 1 1\\n1 1 1\\n\");\n        }else{\n            for(int i=0;i<3;i++){\n                for(int j=0;j<3;j++){\n                    ans[i][j]=1;\n                }\n                sum[i]=3;\n            }\n            for(int i=3;i<n;i++){\n                int shu=0;\n                for(int j=0;j<i;j++){\n                    int nw=1;\n                    while(!isPrime[nw+sum[j]]){\n                        nw++;\n                        while(isPrime[nw]){\n                            nw++;\n                        }\n                    }\n                    ans[i][j]=ans[j][i]=nw;\n                    shu+=nw;\n                    sum[j]+=nw;\n                }\n                int nw=1;\n                while(!isPrime[nw+shu]){\n                    nw++;\n                    while(isPrime[nw]){\n                        nw++;\n                    }\n                }\n                ans[i][i]=nw;\n                sum[i]=shu+nw;\n            }\n            for(int i=0;i<n;i++){\n                for(int j=0;j<n;j++){\n                    printf(\"%d%c\",ans[i][j],j==n-1?'\\n':' ');\n                }\n            }\n        }\n    }\n    return 0;\n}\n```\n\n### D\n\n一个只含单向边的有根树，根为1，每个节点有ai个居民，一开始绑匪在根节点，居民可以分散跑，可以在某个节点合并，居民先跑1步，然后绑匪再跑1步，两者都用最优策略，直到叶子节点居民无法再跑，问绑匪能抓到多少个居民。\n\n比赛时想复杂了，其实对于任意一个子树，居民肯定要跑得越分散越好，所以就是sum[u]/leaf[u]向上取整即可，sum[u]表示子树u所有居民总数，leaf[u]表示子树u叶子总数。\n\n**注意：** 假如子树的sum并不能全部分散到叶子，比如所有居民在一个分支，而无法向上走到另一个分支，这会导致对于这个子树的答案出错，但并不会导致总体的答案出错，因为最终dfs会走到这个分支对应的子树，而总体的答案是不断取max更新。\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\ntypedef long long ll;\nconst int N=2e5+05;\nvector<int> g[N];\nint n,f;\nll a[N];\nll sum[N],leaf[N];\nll ans=0;\nvoid dfs(int u){\n    int siz=g[u].size();\n    sum[u]=a[u];\n    leaf[u]=siz==0;\n    for(int i=0;i<siz;i++){\n        int v=g[u][i];\n        dfs(v);\n        sum[u]+=sum[v];\n        leaf[u]+=leaf[v];\n    }\n    ans=max(ans,(sum[u]+leaf[u]-1)/leaf[u]);\n}\nint main(){\n//    freopen(\"in.txt\",\"r\",stdin);\n    scanf(\"%d\",&n);\n    for(int i=2;i<=n;i++){\n        scanf(\"%d\",&f);\n        g[f].push_back(i);\n    }\n    for(int i=1;i<=n;i++){\n        scanf(\"%lld\",&a[i]);\n    }\n    dfs(1);\n    printf(\"%lld\\n\",ans);\n    return 0;\n}\n```\n","date":"Nov 13, 2020","category":{"name":"题解","permalink":"http://127.0.0.1:8080/category/题解"},"tags":[{"name":"CodeForces","permalink":"http://127.0.0.1:8080/tags/CodeForces"},{"name":"构造","permalink":"http://127.0.0.1:8080/tags/构造"},{"name":"素数","permalink":"http://127.0.0.1:8080/tags/素数"},{"name":"二分查找","permalink":"http://127.0.0.1:8080/tags/二分查找"},{"name":"dfs","permalink":"http://127.0.0.1:8080/tags/dfs"},{"name":"贪心","permalink":"http://127.0.0.1:8080/tags/贪心"}]},{"title":"CodeForces1447D","permalink":"http://127.0.0.1:8080/detail/codeforces-1447D","content":"\n## 题意\n\n给定两个长度为5000的字符串s和t，分别从中找到一个子串ss和tt，使得4\\*lcs(ss,tt)-len(ss)-len(tt)的值最大。\n\n## 分析\n\n5000! lcs! <del>很容易</del>想到最长公共子序列O(N^2)的dp求法，类似的，我们定义dp[i][j]表示以s[i]结尾的子串和以t[j]结尾的子串的最大计算值，注意一定是以这两个字符结尾，这样统计才不重不漏，而子串的左端点其实无需考虑，只需要通过取max更新答案。\n\n当两个字符相等，显然lcs加1，两个子串长度也分别加1，总的值加2，当两个字符不等，按照lcs的做法，这里的一个子串长度加1，另一个不变。\n\n## 代码\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\nconst int N=5050;\nchar s[N],t[N];\nint n,m;\n// 不用管子串的开头，dp[i][j]表示以s[i]结尾的子串和以t[j]结尾的子串的最大计算值\nint dp[N][N];\nint main(){\n    scanf(\"%d%d\",&n,&m);\n    scanf(\"%s\",s+1);\n    scanf(\"%s\",t+1);\n    int ans=0;\n    for(int i=1;i<=n;i++){\n        for(int j=1;j<=m;j++){\n            if(s[i]==t[j]){\n                dp[i][j]=dp[i-1][j-1]+2;\n            }else{\n                dp[i][j]=max(0,max(dp[i][j-1],dp[i-1][j])-1);\n            }\n            ans=max(ans,dp[i][j]);\n        }\n    }\n    printf(\"%d\\n\",ans);\n    return 0;\n}\n```","date":"Dec 3, 2020","category":{"name":"题解","permalink":"http://127.0.0.1:8080/category/题解"},"tags":[{"name":"字符串","permalink":"http://127.0.0.1:8080/tags/字符串"},{"name":"LCS","permalink":"http://127.0.0.1:8080/tags/LCS"},{"name":"dp","permalink":"http://127.0.0.1:8080/tags/dp"}]},{"title":"CodeForces1485C","permalink":"http://127.0.0.1:8080/detail/codeforces-1485C","content":"\n## 题意\n\n求满足$a \\in [1,x]$和$b \\in [1,y]$且$\\left \\lfloor \\frac{a}{b} \\right \\rfloor\t=a\\%b$的数对$(a,b)$数量。\n\n## 分析\n\n设余数为$k$，当$k=0$，不存在这样的数对，当$k>0$，则$a=kb+k=k(b+1)$，根据余数的定义，$k<b$，因此$k^2<kb<kb+k=a<x$，所以$k<=\\sqrt x$。\n\n所以可以枚举$k$，而对于给定的$k$，如何求出满足条件的数对数量：$b$的最小值是$k+1$，$b$每递增1，$a$递增$k$，因此$b$上界为$min(x/k-1,y)$。\n\n另一种思路：\n\n很容易发现，对于上一段提到的这个$k$，会存在一个分割点，若我们从1到$y$枚举$b$，当$b \\in [1,\\sqrt x]$时，$b<=\\sqrt x$，**因此**$a\\%b < \\sqrt x$，而因为$a<=x$，**所以**$\\lfloor \\frac{a}{b} \\rfloor>=\\sqrt x$，因此对于这个$b$，贡献的数对数量应该由小的来决定，也就是$a\\%b$，即$b-1$。所以这一部分的数对总个数为$\\sum_{b=1}^{\\sqrt x} b-1=\\frac{\\sqrt x(\\sqrt x-1)}{2}$。（注意$y<\\sqrt x$的情况）\n\n同理，当$b \\in [\\sqrt x+1]$时，因为$a=k(b+1)$，所以数对的数量($b$已知，不同的$k$对应不同的$a$)应为$\\sum_{b=\\sqrt x+1}^{y} \\lfloor \\frac{x}{b+1} \\rfloor$，这一部分用整除分块来做。(通过$c=b+1$的代换，可以更好理解)\n\n## 代码\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\ntypedef long long ll;\nint T;\nll x,y;\nint main(){\n    scanf(\"%d\",&T);\n    while(T--){\n        scanf(\"%lld%lld\",&x,&y);\n        ll ans=0;\n        ll sq=sqrt(x);\n        if(y<=sq){\n            // 枚举b从1到y求sum(b-1)=>等差数列求和\n            ans=y*(y-1)/2;\n        }else{\n            // 等差数列求第一部分\n            ans+=sq*(sq-1)/2;\n            // 整除分块求第二部分\n            for(ll l=sq+2,r;l<=y+1;l=r+1){\n                if(x/l==0){\n                    break;\n                }\n                r=min(y+1,x/(x/l));\n                // printf(\"%d %d %d %d\\n\",l,r,r-l+1,x/l);\n                ans+=(r-l+1)*(x/l);\n            }\n        }\n        printf(\"%lld\\n\",ans);\n    }\n    return 0;\n}\n```\n","date":"Feb 14, 2021","category":{"name":"题解","permalink":"http://127.0.0.1:8080/category/题解"},"tags":[{"name":"数论","permalink":"http://127.0.0.1:8080/tags/数论"},{"name":"思维","permalink":"http://127.0.0.1:8080/tags/思维"},{"name":"整除分块","permalink":"http://127.0.0.1:8080/tags/整除分块"},{"name":"枚举","permalink":"http://127.0.0.1:8080/tags/枚举"}]},{"title":"Google-CodeJam2020-Qulification-Round","permalink":"http://127.0.0.1:8080/detail/google-code-jam-2020-qualification-round","content":"\n## A Vestigium\n\n求方阵的主对角线和，判断有多少个行和多少个列有重复数字。\n\n## B Nesting Depth\n\n给一个数字序列，要求添加最少的括号，使得最后的字符串**括号是平衡的**，并且**每个数字x都正好被x对括号所包括**。\n\n先考虑`xxx0xxx`的串，很显然，0不需要括号，而两边的两部分至少都是1，所以可以用一对括号将xxx包括起来，得到`(xxx)0(xxx)`，然后再处理每部分的xxx，此时`xxx`外层已经有一个括号，因此xxx中的1不需要括号，所以需要找到`xxx1xxx`的结构进行迭代处理。\n\n迭代因为每次字符串都会变化，不太好写，因此采用递归的方法，基于上面的思路，可以得到算法如下：\n\n- 构造一个dfs函数，进行类似split的操作，split的分隔符从'0'到字符串的最大数字字符，该函数返回传入字符串加括号后的结果。\n- dfs函数中，利用分隔符将字符串分为多个部分，每个部分再递归处理，然后将每个部分加括号的结果再加上外层的一对括号，而分割符号不需要加(即使分隔符号不是0)，再将结果返回。\n- 考虑一些边界情况：比如遍历后没有分割，不能直接返回，需要递归找下一个字符的分割；比如分割时注意最后不要漏了最后一个部分。\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\nint T;\nchar s[105];\nvector<char> dfs(vector<char> &arr,char cp,char mx){\n    if(cp==mx){\n        return arr;\n    }\n    int siz=arr.size();\n    vector<char> ans;\n    vector<char> t;\n    for(int i=0;i<siz;i++){\n        if(arr[i]==cp){\n            if(t.size()>0){\n                t=dfs(t,cp+1,mx);\n                ans.push_back('(');\n                ans.insert(ans.end(),t.begin(),t.end());\n                ans.push_back(')');\n                t.clear();\n            }\n            ans.push_back(cp);\n        }else{\n            t.push_back(arr[i]);\n        }\n    }\n    if(t.size()>0){\n        t=dfs(t,cp+1,mx);\n        if(t.size()>0){\n            ans.push_back('(');\n            ans.insert(ans.end(),t.begin(),t.end());\n            ans.push_back(')');\n        }\n    }\n    return ans;\n}\nint main(){\n    scanf(\"%d\",&T);\n    for(int t=1;t<=T;t++){\n        scanf(\"%s\",s);\n        int n=strlen(s);\n        vector<char> vs(s,s+n);\n        char mx='0';\n        for(int i=0;i<n;i++){\n            mx=max(mx,s[i]);\n        }\n        vector<char> ans=dfs(vs,'0',mx);\n        printf(\"Case #%d: \",t);\n        int siz=ans.size();\n        for(int i=0;i<siz;i++){\n            printf(\"%c\",ans[i]);\n        }\n        printf(\"\\n\");\n    }\n    return 0;\n}\n```\n\n## C Parenting Partnering Returns\n\n将n个时间段分配给两个人，要求不重叠，输出任意一个方案或者无解。\n\n按照开始时间，结束时间的顺序排序，然后遍历贪心取，如果两个人都不能拿，就无解。\n\n## D ESAb ATAd\n\n第一次写交互题，参考了[这个博客](http://zory.ink/posts/6ad41bdb.html)。\n\n题意是有一个100位的01序列，你每次可以询问某个位置的值，且每10次询问后(注意题目表达)，序列会等概率随机发生4种变化的一种，包括1)取反 2)反转 3)取反+反转 4)无变化，要求不超过150次询问得到该序列。\n\n首先要找到变化的规律，一对01对有四种形式(00 01 10 11)进行这四种操作的结果只有16种，经过观察，可以发现01对可以抽象为两种，相同和不同(00和01)，而对应四种变化得到的也是两种不同的01对，所以，如果能找到一对相同的01对和一对不同的01对，通过观察变化前后的不同，就可以确定这次变化，这样子，需要2次询问，剩下的8次询问可以用来确定其他位，然后在变化的时候，将这些所有位都进行变化。\n\n为了方便处理，采用头尾双指针的方法来确定序列，这样每次都是两次询问，刚好可以保证每十次询问就变化。\n\n如果没同时找到相同和不同的01对，比如一直只有相同的01对，也没关系，此时只需要一次询问就可以确定序列的变化。\n\n```cpp\n#include <bits/stdc++.h>\nusing namespace std;\nint T,n;\nint ans[105];\nint ask(int p){\n    printf(\"%d\\n\",p);\n    fflush(stdout);\n    int ans;\n    scanf(\"%d\",&ans);\n    return ans;\n}\nvoid complement(){\n    for(int i=1;i<=n;i++){\n        ans[i]=1-ans[i];\n    }\n}\nvoid reverse(){\n    for(int i=1;i<=n/2;i++){\n        swap(ans[i],ans[n+1-i]);\n    }\n}\nint main(){\n    scanf(\"%d%d\",&T,&n);\n    for(int t=1;t<=T;t++){\n        // 第几次询问\n        int qs=1;\n        // 已确定数组边界\n        int l=1,r=n;\n        // 相同和不同的数对位置\n        int sa=0,di=0;\n        while(l<=r){\n            if(qs>10 && qs%10==1){\n                // 变化是发生在第x1次询问之后，回复之前\n                if(sa&& di){\n                    int ta=ask(sa);\n                    int tb=ask(di);\n                    if(ta==ans[sa]){\n                        if(tb==ans[di]){\n                            // 4 没变化\n                        }else{\n                            // 2 reverse\n                            reverse();\n                        }\n                    }else{\n                        if(tb==ans[di]){\n                            // 3 取反+reverse\n                            complement();\n                            reverse();\n                        }else{\n                            // 1 取反\n                            complement();\n                        }\n                    }\n                }else if(sa){\n                    int t=ask(sa);\n                    if(t!=ans[sa]){\n                        // 1或3 取反\n                        complement();\n                    }\n                    ask(sa);\n                }else{\n                    int t=ask(di);\n                    if(t!=ans[di]){\n                        // 1或2 reverse\n                        reverse();\n                    }\n                    ask(di);\n                }\n            }else{\n                ans[l]=ask(l);\n                ans[r]=ask(r);\n                if(ans[l]==ans[r]){\n                    sa=l;\n                }else{\n                    di=l;\n                }\n                l++;\n                r--;\n            }\n            // 题目设计每十次询问才会变化，所以每次都询问两个，方便处理\n            qs+=2;\n        }\n        for(int i=1;i<=n;i++){\n            printf(\"%d\",ans[i]);\n        }\n        printf(\"\\n\");\n        fflush(stdout);\n        // 返回结果Y或N，但一般没办法再怎么处理，但要记得读\n        char res[10];\n        scanf(\"%s\",res);\n    }\n    return 0;\n}\n```\n\n## E Indicium\n\n构造一个N\\*N的矩阵，满足每一行每一列都是不重复的1到N，且对角线的和为K。\n\n// TODO 暂时不会做","date":"Feb 10, 2021","category":{"name":"题解","permalink":"http://127.0.0.1:8080/category/题解"},"tags":[{"name":"贪心","permalink":"http://127.0.0.1:8080/tags/贪心"},{"name":"递归","permalink":"http://127.0.0.1:8080/tags/递归"},{"name":"字符串","permalink":"http://127.0.0.1:8080/tags/字符串"},{"name":"排序","permalink":"http://127.0.0.1:8080/tags/排序"},{"name":"区间覆盖","permalink":"http://127.0.0.1:8080/tags/区间覆盖"},{"name":"交互题","permalink":"http://127.0.0.1:8080/tags/交互题"},{"name":"01序列","permalink":"http://127.0.0.1:8080/tags/01序列"},{"name":"TODO","permalink":"http://127.0.0.1:8080/tags/TODO"}]}]